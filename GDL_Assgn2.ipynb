{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rudradeep22/GDL_Project/blob/main/GDL_Assgn2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpmDn-176k7i"
      },
      "source": [
        "***\n",
        "***\n",
        "\n",
        "# Assignment 2 - Extend My Playlist - 50 points\n",
        "\n",
        "- This assignment will see you use a dataset of Spotify playlists, and build a GNN to build a playlist-track RecSys (Recommendation System). We'll use a type of layer called LightGCN for this task.\n",
        "- You will almost certainly need a GPU for this assignment. The free tier GPUs of colab should suffice. So write your code with that in mind. While I was able to train it pretty well even on a non-GPU system, do note that it might take a lot longer, and the analysis part would require you to run the model multiple times\n",
        "- The dataset you have been shared is a small part of a much larger dataset so the free tier colab GPU can handle it. Do **NOT** share the dataset with other people. We'll release details about the dataset after the assignment is over if you would like access to the full dataset.\n",
        "\n",
        "\n",
        "## Marking Breakup\n",
        "1. Code (40 points)\n",
        "2. Model Results on Hidden Test set (10 points)\n",
        "3. You will receive a bonus 10 points for being able to provide a visualisation of the embeddings your model learned. Hints for this are at the end of the assignment.\n",
        "4. You will receive a bonus 5 points for well-documented code. So use your comments well, and remember to follow PEP guidelines.\n",
        "\n",
        "\n",
        "### Details\n",
        "- You may attempt this assignment **alone, or in teams of 2 or 3**. This assignment is heavier, yes, but should still be doable alone. The deadline is EOD 24/06/23.\n",
        "- We have given a weak boilerplate code for you  with instructions. Following the instructions will be the easiest way to get the most grades.\n",
        "- This time, plagiarism across groups will mean that the assignment submission becomes null and void. No marks will be given to either group.\n",
        "- We are not testing your literature review skils here. The dataset, modules to use, layers you'll need are all given below. Don't waste time hunting for better ones, the combination we have picked out is already very powerful.\n",
        "- *30 points* are for the straight code work on your model (Section 1 and 2). The last 10 points are for the codework in training visualisation (Section 3).\n",
        "- *10 points* are for the results of your model on a hidden test that is not being shared with you. You do not need to adjust your code in anyway, we will simply change the test set you have in your code to a test set we have.\n",
        "- There's *10 points* for the visualisation of the training and analysis with different hyperparameters. This is weighted lesser than the main code work, but is far easier to implement. However, no marks will be given for this section if your model doesn't actually train at all.\n",
        "- There's a *bonus 10 points* for embedding visualisation. This involves plotting the learnings of your model over epoch using UMAP Dimensionality Reduction and the Animation Submodule of Matplotlib. We stop here, but if you manage to get this done, you could go one step further to actually see what happens to datapoints in your model. Details of this section are given in the section itself.\n",
        "- You get *bonus 5 points* if I find your codework to be very neat and well-documented. Naturally you need to atleast finish sections 1-3 to be eligible for these bonus points. Wouldn't be very helpful if you just emptied the notebook, printed 'Hello World!' with exquisite documentation and then asked me for 5 bonus points, would it?\n",
        "- You will have to submit the notebook (or a link to it) in a Google Form. **Rename your notebook to `<Your_Name>_A2.ipynb` before submitting.**\n",
        "\n",
        "### Boilerplate -\n",
        "0. [Setting up - Installation and Data Loading](#section-0---installation-and-data-loading-0-points)\n",
        "1. [Data preprocessing (15 points)](#section-1---data-preprocessing-graded-sections-start-here)\n",
        "2. [Training and Testing (15 points)](#section-2-training-and-testing)\n",
        "3. [Training Visualisation (10 points)](#section-3---training-visualisation)\n",
        "4. [(Bonus) Embedding Visualisation](#section-4---bonus-embedding-visualisation)\n",
        "\n",
        "### What score to expect?\n",
        "- Model that was not trained (15 points)\n",
        "- Model that has been trained (30 points)\n",
        "- Visualising the training of the model with various hyperparameters (40 points)\n",
        "- Top-tier performance on our test set (50 points)\n",
        "- Top-tier performance with embedding visualisation and documentation (65 points)\n",
        "\n",
        "## Tentative Pass Criteria\n",
        "**30 points** are needed to pass this assignment. Which means your model needs to atleast train.\n",
        "\n",
        "Realistically, good performance on your test set should also translate to pretty decent performance on our test set, so I expect most people to cross from the 30s to the 45s pretty easily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcqDayPV6k7k"
      },
      "source": [
        "***\n",
        "\n",
        "# Code Starts Here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrZwCAFu6k7k"
      },
      "source": [
        "\n",
        "## Section 0 - Installation and Data Loading (0 points)\n",
        "\n",
        "This section is already done for you and you do not need to change anything here. Go through it to understand what kind of data is available and what modules are pre-installed for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8fqcu1B6k7k"
      },
      "source": [
        "### Installation of Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvuYWu7O6k7k",
        "outputId": "3935d646-9b82-4be7-ef7d-240ee233ffb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-1.13.1+cu116.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_scatter-2.1.1%2Bpt113cu116-cp310-cp310-linux_x86_64.whl (9.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.1+pt113cu116\n",
            "Looking in links: https://data.pyg.org/whl/torch-1.13.1+cu116.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.13.0%2Bcu116/torch_sparse-0.6.17%2Bpt113cu116-cp310-cp310-linux_x86_64.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-sparse) (1.10.1)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scipy->torch-sparse) (1.22.4)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.17+pt113cu116\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (4.65.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.22.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch-geometric) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch-geometric) (3.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910459 sha256=b8fa982f1aeab5b0c20db665e0f93b3021951cedc01f73fdcf1422e3f8724927\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.3.1\n",
            "Collecting umap-learn\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (1.10.1)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.56.4)\n",
            "Collecting pynndescent>=0.5 (from umap-learn)\n",
            "  Downloading pynndescent-0.5.10.tar.gz (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.65.0)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from numba>=0.49->umap-learn) (67.7.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->umap-learn) (3.1.0)\n",
            "Building wheels for collected packages: umap-learn, pynndescent\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.5.3-py3-none-any.whl size=82816 sha256=cfb45eb6ccca110ecbaeffece9cfd19e06478513c0c98ba9144892996b6ee0d2\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/e8/c6/a37ea663620bd5200ea1ba0907ab3c217042c1d035ef606acc\n",
            "  Building wheel for pynndescent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynndescent: filename=pynndescent-0.5.10-py3-none-any.whl size=55622 sha256=277d5f39b1c1aad4cd908b69918562f7a1963ede4393cd43add02ddc66eaec8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/4a/38/5d/f60a40a66a9512b7e5e83517ebc2d1b42d857be97d135f1096\n",
            "Successfully built umap-learn pynndescent\n",
            "Installing collected packages: pynndescent, umap-learn\n",
            "Successfully installed pynndescent-0.5.10 umap-learn-0.5.3\n"
          ]
        }
      ],
      "source": [
        "# # If you are running this code locally, remember to comment this section after you have installed the code.\n",
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-1.13.1+cu116.html\n",
        "!pip install torch-sparse -f https://data.pyg.org/whl/torch-1.13.1+cu116.html\n",
        "!pip install torch-geometric\n",
        "!pip install umap-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5y82OSU6k7l"
      },
      "source": [
        "### General imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXi8ZAAM6k7l"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pprint\n",
        "from pathlib import Path as Data_Path\n",
        "import os\n",
        "from os.path import isfile, join\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXhiN7qn6k7l"
      },
      "source": [
        "### ML imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGIWWHCS6k7l",
        "outputId": "f4b9cbc4-b28d-4341-8a10-38e9c31a47c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:31: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_scatter/_scatter_cuda.so: undefined symbol: _ZN2at4_ops6narrow4callERKNS_6TensorElll\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:42: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /usr/local/lib/python3.10/dist-packages/torch_sparse/_diag_cuda.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowEllb\n",
            "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.0.1+cu118; Torch-cuda version: 11.8;         Torch Geometric version: 2.3.1.\n"
          ]
        }
      ],
      "source": [
        "# Import relevant ML libraries\n",
        "from typing import Optional, Union\n",
        "\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn as nn\n",
        "from torch.nn import Embedding, ModuleList, Linear\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric\n",
        "import torch_geometric.nn as pyg_nn\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "from torch.nn.modules.loss import _Loss\n",
        "\n",
        "from torch_geometric.nn.conv import LGConv\n",
        "from torch_geometric.typing import Adj, OptTensor, SparseTensor\n",
        "\n",
        "print(f\"Torch version: {torch.__version__}; Torch-cuda version: {torch.version.cuda};\\\n",
        "         Torch Geometric version: {torch_geometric.__version__}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYEo-GV16k7m"
      },
      "source": [
        "### Seeding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAj0SOHL6k7m"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sY8I-DtG6k7m"
      },
      "source": [
        "### Useful Classes\n",
        "I am creating some classes that you will find useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7vijpza6k7m"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "The original data was stored in JSON files, which contain playlists, which themselves contain tracks.\n",
        "Thus, we define three classes:\n",
        "  Track    : contains information for a specific track (its id, name, etc.)\n",
        "  Playlist : contains information for a specific playlist (its id, name, etc. as well as a list of Tracks)\n",
        "  JSONFile : contains the loaded json file and stores a dictionary of all of the Playlists\n",
        "\n",
        "Note: If you want to use Artist info, you may want to create an Artist class\n",
        "\"\"\"\n",
        "\n",
        "class Track:\n",
        "  \"\"\"\n",
        "  Simple class for a track, containing its attributes:\n",
        "    1. URI (a unique id)\n",
        "    2. Name\n",
        "    3. Artist info (URI and name)\n",
        "    4. Parent playlist\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, track_dict, playlist):\n",
        "    self.uri = track_dict[\"track_uri\"]\n",
        "    self.name = track_dict[\"track_name\"]\n",
        "    self.artist_uri = track_dict[\"artist_uri\"]\n",
        "    self.artist_name = track_dict[\"artist_name\"]\n",
        "    self.playlist = playlist\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"Track {self.uri} called {self.name} by {self.artist_uri} ({self.artist_name}) in playlist {self.playlist}.\"\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"Track {self.uri}\"\n",
        "\n",
        "class Playlist:\n",
        "  \"\"\"\n",
        "  Simple class for a playlist, containing its attributes:\n",
        "    1. Name (playlist and its associated index (the data is a subset, so every index hasn't been shared with you))\n",
        "    2. Title (playlist title in the Spotify dataset)\n",
        "    3. Loaded dictionary from the raw json for the playlist\n",
        "    4. Dictionary of tracks (track_uri : Track), populated by .load_tracks()\n",
        "    5. List of artists uris\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, json_data, index):\n",
        "\n",
        "    self.name = f\"playlist_{index}\"\n",
        "    self.title = json_data[\"name\"]\n",
        "    self.data = json_data\n",
        "\n",
        "    self.tracks = {}\n",
        "    self.artists = []\n",
        "\n",
        "  def load_tracks(self):\n",
        "    \"\"\" Call this function to load all of the tracks in the json data for the playlist.\"\"\"\n",
        "\n",
        "    tracks_list = self.data[\"tracks\"]\n",
        "    self.tracks = {x[\"track_uri\"] : Track(x, self.name) for x in tracks_list}\n",
        "    self.artists = [x[\"artist_uri\"] for x in tracks_list]\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"Playlist {self.name} with {len(self.tracks)} tracks loaded.\"\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f\"Playlist {self.name}\"\n",
        "\n",
        "class JSONFile:\n",
        "  \"\"\"\n",
        "  Simple class for a JSON file, containing its attributes:\n",
        "    1. File Name\n",
        "    2. Index to begin numbering playlists at\n",
        "    3. Loaded dictionary from the raw json for the full file\n",
        "    4. Dictionary of playlists (name : Playlist), populated by .process_file()\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, data_path, file_name, start_index):\n",
        "\n",
        "    self.file_name = file_name\n",
        "    self.start_index = start_index\n",
        "\n",
        "    with open(join(data_path, file_name)) as json_file:\n",
        "      json_data = json.load(json_file)\n",
        "    self.data = json_data\n",
        "\n",
        "    self.playlists = {}\n",
        "\n",
        "  def process_file(self):\n",
        "    \"\"\" Call this function to load all of the playlists in the json data.\"\"\"\n",
        "\n",
        "    for i, playlist_json in enumerate(self.data[\"playlists\"]):\n",
        "      playlist = Playlist(playlist_json, self.start_index + i)\n",
        "      playlist.load_tracks()\n",
        "      self.playlists[playlist.name] = playlist\n",
        "\n",
        "  def __str__(self):\n",
        "    return f\"JSON {self.file_name} has {len(self.playlists)} playlists loaded.\"\n",
        "\n",
        "  def __repr__(self):\n",
        "    return self.file_name\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkyZHH5_6k7m"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uhQZJ7h6k7m"
      },
      "source": [
        "- `playlist_dat.pt` is a file with a PyG Data object. This has a graph with all the data you'll need.\n",
        "- A simple `torch.load()` command will put this in your memory, as shown below. Feel free to rename this variable.\n",
        "- `playlists_idx.dat` and `tracks_idx.dat` are just pickle objects with list of node ids which are playlists and tracks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ-1441C6k7m"
      },
      "source": [
        "This is what (a small subset of) your graph looks like, blue represents playlists, and red represents tracks\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVcef-m_6k7m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1bb923d-9448-4602-9fe6-3772fa334b1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch_geometric.data.data.Data'>\n"
          ]
        }
      ],
      "source": [
        "graph_data = torch.load('playlist_data.pt')\n",
        "print(type(graph_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGqFifTZ6k7m"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open(\"playlists_idx.dat\",\"rb\") as f:\n",
        "    playlists_idx = pickle.load(f)\n",
        "\n",
        "with open(\"tracks_idx.dat\",\"rb\") as f:\n",
        "    tracks_idx = pickle.load(f)\n",
        "n_playlists = len(playlists_idx)\n",
        "n_tracks = len(tracks_idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18y8Bp_o6k7n"
      },
      "source": [
        "### Try out some EDA\n",
        "\n",
        "What does this graph look like? What are the number of nodes? What kind of information is available to you here? Do some EDA and get comfortable. This part won't be graded, but I doubt you will be able to progress much without understanding your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0jK1jnBc6k7n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e837ab5-ff32-45d8-9620-03223fed9350"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(edge_index=[2, 628743], num_nodes=15720)\n",
            "No. of nodes are: 15720\n",
            "No. of edges are: 628743\n",
            "Average degree is: 39.996374045801524\n",
            "Any isolated nodes ? False\n",
            "Any self loops? False\n",
            "Is it directed? True\n"
          ]
        }
      ],
      "source": [
        "print(graph_data)\n",
        "print(f'No. of nodes are: {graph_data.num_nodes}')\n",
        "print(f'No. of edges are: {graph_data.num_edges}')\n",
        "print(f'Average degree is: {graph_data.num_edges/graph_data.num_nodes}')\n",
        "print(f'Any isolated nodes ? {graph_data.has_isolated_nodes()}')\n",
        "print(f'Any self loops? {graph_data.has_self_loops()}')\n",
        "print(f'Is it directed? {graph_data.is_directed()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVIRL_MN6k7n"
      },
      "source": [
        "## Section 1 - Data Preprocessing (Graded Sections start here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOW7lsC26k7n"
      },
      "source": [
        "Split the data into train-test-split sets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = RandomLinkSplit(is_undirected=False)\n",
        "train_split, val_split, test_split = transform(graph_data)"
      ],
      "metadata": {
        "id": "acTTfO7QsMqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwFIBT0r6k7n"
      },
      "source": [
        "The dataset has data in `float32` format, but training will require you to use it in `torch.int64` (Just trust me bro.)\n",
        "Just a reminder -\n",
        "1. Edge Index - Message Passing Edges\n",
        "2. Edge Label Index - Supervision Edges\n",
        "\n",
        "And if you don't understand what I'm saying, you should go back and see some lectures on link prediction"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######################################################################\n",
        "# Convert the edge_indexes and edge_label_indexes to integer type data.\n",
        "train_split.edge_index = train_split.edge_index.to(torch.int64)\n",
        "val_split.edge_index = val_split.edge_index.to(torch.int64)\n",
        "test_split.edge_index = test_split.edge_index.to(torch.int64)\n",
        "#######################################################################"
      ],
      "metadata": {
        "id": "tr9g3M_u6k7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIrUCeZ86k7n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36350da7-fa13-437c-e347-52e96c9cfc97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set has 880242 positives supervision edges\n",
            "Validation set has 125748 positive supervision edges\n",
            "Test set has 251496 positive supervision edges\n",
            "Train set has 440121 message passing edges\n",
            "Validation set has 440121 message passing edges\n",
            "Test set has 502995 message passing edges\n"
          ]
        }
      ],
      "source": [
        "print(f\"Train set has {train_split.edge_label_index.shape[1]} positives supervision edges\")\n",
        "print(f\"Validation set has {val_split.edge_label_index.shape[1]} positive supervision edges\")\n",
        "print(f\"Test set has {test_split.edge_label_index.shape[1]} positive supervision edges\")\n",
        "\n",
        "print(f\"Train set has {train_split.edge_index.shape[1]} message passing edges\")\n",
        "print(f\"Validation set has {val_split.edge_index.shape[1]} message passing edges\")\n",
        "print(f\"Test set has {test_split.edge_index.shape[1]} message passing edges\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sef2OM-U6k7n"
      },
      "source": [
        "### The BPR Loss\n",
        "\n",
        "You'll need a new kind of Loss function for this task, so here's a class for implementing BPR, Bayesian Personalised Ranking Loss. We've implemented this for you. You can change function names and stuff, however you are forbidden from changing the mathematical formula -\n",
        "\n",
        "\\begin{equation*}\n",
        "    \\text{BPR Loss}(i) = \\frac{1}{|\\mathcal{E}(i)|} \\underset{{(i, j_{+}) \\in \\mathcal{E}(i)}}{\\sum} \\log \\sigma \\left( \\text{score}(i, j_+) - \\text{score}(i, j_-) \\right)\n",
        "\\end{equation*}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMzd2H146k7n"
      },
      "outputs": [],
      "source": [
        "class BPRLoss(_Loss):\n",
        "    r\"\"\"The Bayesian Personalized Ranking (BPR) loss.\n",
        "\n",
        "    The BPR loss is a pairwise loss that encourages the prediction of an\n",
        "    observed entry to be higher than its unobserved counterparts\n",
        "    (see `here <https://arxiv.org/abs/2002.02126>`__).\n",
        "\n",
        "    .. math::\n",
        "        L_{\\text{BPR}} = - \\sum_{u=1}^{M} \\sum_{i \\in \\mathcal{N}_u}\n",
        "        \\sum_{j \\not\\in \\mathcal{N}_u} \\ln \\sigma(\\hat{y}_{ui} - \\hat{y}_{uj})\n",
        "        + \\lambda \\vert\\vert \\textbf{x}^{(0)} \\vert\\vert^2\n",
        "\n",
        "    where :math:`lambda` controls the :math:`L_2` regularization strength.\n",
        "    We compute the mean BPR loss for simplicity.\n",
        "\n",
        "    Args:\n",
        "        lambda_reg (float, optional): The :math:`L_2` regularization strength\n",
        "            (default: 0).\n",
        "        **kwargs (optional): Additional arguments of the underlying\n",
        "            :class:`torch.nn.modules.loss._Loss` class.\n",
        "    \"\"\"\n",
        "    __constants__ = ['lambda_reg']\n",
        "    lambda_reg: float\n",
        "\n",
        "    def __init__(self, lambda_reg: float = 0, **kwargs):\n",
        "        super().__init__(None, None, \"sum\", **kwargs)\n",
        "        self.lambda_reg = lambda_reg\n",
        "\n",
        "    def forward(self, positives: Tensor, negatives: Tensor,\n",
        "                parameters: Tensor = None) -> Tensor:\n",
        "        r\"\"\"Compute the mean Bayesian Personalized Ranking (BPR) loss.\n",
        "\n",
        "        .. note::\n",
        "\n",
        "            The i-th entry in the :obj:`positives` vector and i-th entry\n",
        "            in the :obj:`negatives` entry should correspond to the same\n",
        "            entity (*.e.g*, user), as the BPR is a personalized ranking loss.\n",
        "\n",
        "        Args:\n",
        "            positives (Tensor): The vector of positive-pair rankings.\n",
        "            negatives (Tensor): The vector of negative-pair rankings.\n",
        "            parameters (Tensor, optional): The tensor of parameters which\n",
        "                should be used for :math:`L_2` regularization\n",
        "                (default: :obj:`None`).\n",
        "        \"\"\"\n",
        "        n_pairs = positives.size(0)\n",
        "        log_prob = F.logsigmoid(positives - negatives).sum()\n",
        "        regularization = 0\n",
        "\n",
        "        if self.lambda_reg != 0:\n",
        "            regularization = self.lambda_reg * parameters.norm(p=2).pow(2)\n",
        "\n",
        "        return (-log_prob + regularization) / n_pairs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7Nasmqs6k7n"
      },
      "source": [
        "### The GCN class\n",
        "As mentioned, the primary model type we need here is a GCN. Use this boilerplate code to define your GCN class. This is merely a template, you can change it up if you think something else would be more intuitive. Some of the functions are defined to make this a little easier\n",
        "\n",
        "As for the type of layer you need to use - It's the [LGConv](https://arxiv.org/abs/2002.02126) layer. PyG already offers this type of layer. [see here](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.LGConv.html#torch_geometric.nn.conv.LGConv).\n",
        "\n",
        "There's other types of layers available, and this boilerplate code here should make it pretty easy to call upon many different PyG layers. This however is not necessary to have a successful model or for your score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odjpC40m6k7n"
      },
      "outputs": [],
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    \"\"\"\n",
        "      Here we adapt the LightGCN model from Torch Geometric for our purposes.\n",
        "      We'll be using the LGCN\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_nodes: int,\n",
        "        embedding_dim: int,\n",
        "        num_layers: int,\n",
        "        alpha: Optional[Union[float, Tensor]] = None,\n",
        "        alpha_learnable = False,\n",
        "        conv_layer = \"LGC\",\n",
        "        name = None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        alpha_string = \"alpha\" if alpha_learnable else \"\"\n",
        "        self.name = f\"LGCN_{conv_layer}_{num_layers}_e{embedding_dim}_nodes{num_nodes}_{alpha_string}\"\n",
        "        self.num_nodes = num_nodes\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        if alpha_learnable == True:\n",
        "          alpha_vals = torch.rand(num_layers+1)\n",
        "          alpha = nn.Parameter(alpha_vals/torch.sum(alpha_vals))\n",
        "          print(f\"Alpha learnable, initialized to: {alpha.softmax(dim=-1)}\")\n",
        "        else:\n",
        "          if alpha is None:\n",
        "              alpha = 1 / (num_layers + 1)\n",
        "\n",
        "          if isinstance(alpha, Tensor):\n",
        "              assert alpha.size(0) == num_layers + 1\n",
        "          else:\n",
        "              alpha = torch.tensor([alpha] * (num_layers + 1))\n",
        "\n",
        "        self.register_buffer('alpha', alpha)\n",
        "\n",
        "        self.embedding = Embedding(num_nodes, embedding_dim)\n",
        "\n",
        "        # initialize convolutional layers\n",
        "        self.conv_layer = conv_layer\n",
        "        if conv_layer == \"LGC\":\n",
        "          self.convs = ModuleList([LGConv(**kwargs) for _ in range(num_layers)])\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        # Reset the parameters of all your layers\n",
        "        for conv in self.convs:\n",
        "          conv.reset_parameters()\n",
        "        nn.init.normal_(self.embedding.weight)\n",
        "\n",
        "    def get_embedding(self, edge_index: Adj) -> Tensor:\n",
        "        # Self explanatory\n",
        "        embed = self.embedding.weight\n",
        "        final_embed = embed * self.alpha[0]\n",
        "        edge_index = edge_index.to(torch.int64)\n",
        "        for layer in range(self.num_layers):\n",
        "          embed = self.convs[layer](embed, edge_index)\n",
        "          final_embed += embed * self.alpha[layer+1]\n",
        "        return final_embed\n",
        "\n",
        "    def initialize_embeddings(self, data):\n",
        "        # initialize with the data node features\n",
        "        node_features = data.node_features\n",
        "        self.embedding.weight.data.copy_(node_features)\n",
        "\n",
        "\n",
        "    def forward(self, edge_index: Adj,\n",
        "                edge_label_index: OptTensor = None) -> Tensor:\n",
        "        # Implement the forward pass\n",
        "        embedding = self.get_embedding(edge_index)\n",
        "        source_nodes = edge_label_index[0]\n",
        "        final_nodes  = edge_label_index[1]\n",
        "        source_nodes = source_nodes.to(torch.int64)\n",
        "        final_nodes = final_nodes.to(torch.int64)\n",
        "        source_embedding = embedding[source_nodes]\n",
        "        final_embedding  = embedding[final_nodes]\n",
        "        out = (source_embedding * final_embedding).sum(dim=-1)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def predict_link(self, edge_index: Adj, edge_label_index: OptTensor = None,\n",
        "                     prob: bool = False) -> Tensor:\n",
        "        # Make a prediction. I hope you remember we want to deal with integers not floats\n",
        "        prediction = self(edge_index, edge_label_index)\n",
        "        prediction_prob = prediction.sigmoid()\n",
        "        if bool:\n",
        "          return prediction_prob\n",
        "        else:\n",
        "          return prediction_prob.round().int()\n",
        "\n",
        "    def predict_link_embedding(self, embed: Adj, edge_label_index: Adj) -> Tensor:\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "    def recommend(self, edge_index: Adj, src_index: OptTensor = None,\n",
        "                  dst_index: OptTensor = None, k: int = 1) -> Tensor:\n",
        "        # This is what you'll use for inference\n",
        "        src_embedding = dst_embedding = self.get_embedding(edge_index)\n",
        "        if src_index != None:\n",
        "          src_embedding = src_embedding[src_index] #filtering nodes according to src_index\n",
        "        if dst_index != None:\n",
        "          dst_embedding = dst_embedding[dst_index]\n",
        "        pred = src_embedding @ dst_embedding.t()\n",
        "        top_indexes = pred.topk(k, dim=-1).indices\n",
        "\n",
        "        if dst_index is not None:\n",
        "            top_indexes = dst_index[top_indexes.view(-1)].view(*top_indexes.size())\n",
        "        return top_indexes\n",
        "\n",
        "\n",
        "    def link_pred_loss(self, pred: Tensor, edge_label: Tensor,\n",
        "                       **kwargs) -> Tensor:\n",
        "        loss_fn = torch.nn.BCEWithLogitsLoss(**kwargs)\n",
        "        return loss_fn(pred, edge_label.to(pred.dtype))\n",
        "\n",
        "\n",
        "    def recommendation_loss(self, pos_edge_rank: Tensor, neg_edge_rank: Tensor,\n",
        "                            lambda_reg: float = 1e-4, **kwargs) -> Tensor:\n",
        "        r\"\"\"Computes the model loss for a ranking objective via the Bayesian\n",
        "        Personalized Ranking (BPR) loss.\"\"\"\n",
        "        # The BPRLoss is defined for you in the next cell.\n",
        "        loss_fn = BPRLoss(lambda_reg, **kwargs)\n",
        "        return loss_fn(pos_edge_rank, neg_edge_rank, self.embedding.weight)\n",
        "\n",
        "    def bpr_loss(self, pos_scores, neg_scores):\n",
        "      return - torch.log(torch.sigmoid(pos_scores - neg_scores)).mean()\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        # Used to get a summary of your model features\n",
        "        return (f'{self.__class__.__name__}({self.num_nodes}, '\n",
        "                f'{self.embedding_dim}, num_layers={self.num_layers})')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRf1oAA-6k7o"
      },
      "source": [
        "### Negative Sampling\n",
        "\n",
        "If you only trained the model to minimise loss however, your model would perform pretty poorly. Why so? Assign every node the same embedding, and you would technically be minimizing the loss. The solution to that usually involves negative sampling, paying attention to the nodes that do not have an edge between them and hence must not be similar.\n",
        "Here are some negative sampling approaches for you to try out. We've done the first one, you do the rest (yes this is graded)-\n",
        "1. Random, no positive check: for each positive edge coming from a playlist $p_i$, randomly draw a track $t_j$ from the full set of track nodes such that ($p_i$, $t_j$) is the negative edge. For computational efficiency, we don't check if ($p_i$, $t_j$) is actually a negative edge, though probabilistically it is very likely.\n",
        "2. Random, positive check: for each positive edge coming from a playlist $p_i$, randomly draw a track $t_j$ from the full set of track nodes such that ($p_i$, $t_j$) is the negative edge. We ensure that ($p_i$, $t_j$) is not a positive edge.\n",
        "3. Hard: for each positive edge coming from a playlist $p_i$, randomly draw a track $t_j$ from the top $k$ proportion of tracks, ranked by dot product similarity to $p_i$. For epoch 0, $k = 1$ and we lower it at each subsequent iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oe82nD4L6k7o"
      },
      "outputs": [],
      "source": [
        "def sample_negative_edges_nocheck(data, num_playlists, num_tracks, device = None):\n",
        "  # note computationally inefficient to check that these are indeed negative edges\n",
        "  playlists = data.edge_label_index[0, :]\n",
        "  tracks = torch.randint(num_playlists, num_playlists + num_tracks - 1, size = data.edge_label_index[1, :].size())\n",
        "\n",
        "  if playlists.get_device() != -1: # on gpu\n",
        "    tracks = tracks.to(device)\n",
        "\n",
        "  neg_edge_index = torch.stack((playlists, tracks), dim = 0)\n",
        "  neg_edge_label = torch.zeros(neg_edge_index.shape[1])\n",
        "\n",
        "  if neg_edge_index.get_device() != -1: # on gpu\n",
        "    neg_edge_label = neg_edge_label.to(device)\n",
        "\n",
        "  return neg_edge_index, neg_edge_label\n",
        "\n",
        "def sample_negative_edges(data, num_playlists, num_tracks, device=None):\n",
        "    playlists = data.edge_label_index[0, :]\n",
        "    tracks = torch.randint(num_playlists, num_playlists + num_tracks - 1, size = data.edge_label_index[1, :].size())\n",
        "    #need to remove negative tracks already marked as positive\n",
        "    isNegative = torch.ones(num_playlists + num_tracks - 1, dtype=torch.bool)\n",
        "    isNegative[data.edge_label_index[1, :]] = False\n",
        "    tracks = tracks[isNegative[tracks]]\n",
        "\n",
        "    if playlists.get_device() != -1: # on gpu\n",
        "      tracks = tracks.to(device)\n",
        "\n",
        "    neg_edge_index = torch.stack((playlists, tracks), dim = 0)\n",
        "    neg_edge_label = torch.zeros(neg_edge_index.shape[1])\n",
        "\n",
        "    if neg_edge_index.get_device() != -1: # on gpu\n",
        "      neg_edge_label = neg_edge_label.to(device)\n",
        "\n",
        "    return neg_edge_index, neg_edge_label\n",
        "\n",
        "def sample_hard_negative_edges(data, model, num_playlists, num_tracks, device=None, batch_size=500, frac_sample = 1):\n",
        "    neg_edges = None\n",
        "    neg_edge_label = None\n",
        "\n",
        "    return neg_edges, neg_edge_label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odT58Ajc6k7o"
      },
      "source": [
        "### Recall@K\n",
        "\n",
        "Using only a loss seems like a bad idea however. Most of the time it can be quite hard to interpret loss alone. So let's just also keep an eye on the Recall@K, a popular one for RecSys.\n",
        "For a playlist $i$, $P^k_i$ represents the set of the top $k$ predicted tracks for $i$ and $T_i$ the ground truth of connected tracks to playlist $i$, then we calculate\n",
        "$$\n",
        "\\text{recall}^k_i = \\frac{| P^k_i \\cap T_i | }{|T_i|}.\n",
        "$$\n",
        "If $T_i = 0$, then we assign this value to 1 (We don't really want to run into ZeroDivisionError). Note, if $T_i \\subset P_i^k$, then the recall is equal to 1. Hence, our choice of $k$ matters a lot.\n",
        "\n",
        "You'll probably want to try out multiple values of k but maybe you can start in the ballpark of 2-3% of the total tracks\n",
        "\n",
        "Note: When you evaluate this metric on the val or test set, make sure to filter the message passing edges from consideration, as the model can directly observe these.\n",
        "\n",
        "I'm implementing this for you as a freebie :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7E_Js-S6k7o"
      },
      "outputs": [],
      "source": [
        "def recall_at_k(data, model, k = 10000, batch_size = 64, device = None):\n",
        "    with torch.no_grad():\n",
        "        embeddings = model.get_embedding(data.edge_index)\n",
        "        playlists_embeddings = embeddings[:n_playlists]\n",
        "        tracks_embeddings = embeddings[n_playlists:]\n",
        "\n",
        "    hits_list = []\n",
        "    relevant_counts_list = []\n",
        "\n",
        "    for batch_start in range(0, n_playlists, batch_size):\n",
        "        batch_end = min(batch_start + batch_size, n_playlists)\n",
        "        batch_playlists_embeddings = playlists_embeddings[batch_start:batch_end]\n",
        "\n",
        "        # Calculate scores for all possible item pairs\n",
        "        scores = torch.matmul(batch_playlists_embeddings, tracks_embeddings.t())\n",
        "\n",
        "        # Set the scores of message passing edges to negative infinity\n",
        "        mp_indices = ((data.edge_index[0] >= batch_start) & (data.edge_index[0] < batch_end)).nonzero(as_tuple=True)[0]\n",
        "        scores[data.edge_index[0, mp_indices] - batch_start, data.edge_index[1, mp_indices] - n_playlists] = -float(\"inf\")\n",
        "\n",
        "        # Find the top k highest scoring items for each playlist in the batch\n",
        "        _, top_k_indices = torch.topk(scores, k, dim=1)\n",
        "\n",
        "        # Ground truth supervision edges\n",
        "        ground_truth_edges = data.edge_label_index\n",
        "\n",
        "        # Create a mask to indicate if the top k items are in the ground truth supervision edges\n",
        "        mask = torch.zeros(scores.shape, device=device, dtype=torch.bool)\n",
        "        gt_indices = ((ground_truth_edges[0] >= batch_start) & (ground_truth_edges[0] < batch_end)).nonzero(as_tuple=True)[0]\n",
        "        mask[ground_truth_edges[0, gt_indices] - batch_start, ground_truth_edges[1, gt_indices] - n_playlists] = True\n",
        "\n",
        "        # Check how many of the top k items are in the ground truth supervision edges\n",
        "        hits = mask.gather(1, top_k_indices).sum(dim=1)\n",
        "        hits_list.append(hits)\n",
        "\n",
        "        # Calculate the total number of relevant items for each playlist in the batch\n",
        "        relevant_counts = torch.bincount(ground_truth_edges[0, gt_indices] - batch_start, minlength=batch_end - batch_start)\n",
        "        relevant_counts_list.append(relevant_counts)\n",
        "\n",
        "    # Compute recall@k\n",
        "    hits_tensor = torch.cat(hits_list, dim=0)\n",
        "    relevant_counts_tensor = torch.cat(relevant_counts_list, dim=0)\n",
        "    # Handle division by zero case\n",
        "    recall_at_k = torch.where(\n",
        "        relevant_counts_tensor != 0,\n",
        "        hits_tensor.true_divide(relevant_counts_tensor),\n",
        "        torch.ones_like(hits_tensor)\n",
        "    )\n",
        "    # take average\n",
        "    recall_at_k = torch.mean(recall_at_k)\n",
        "\n",
        "    if recall_at_k.numel() == 1:\n",
        "        return recall_at_k.item()\n",
        "    else:\n",
        "        raise ValueError(\"recall_at_k contains more than one item.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNny52Sk6k7o"
      },
      "source": [
        "### ROC AUC Score\n",
        "\n",
        "Finally, let's calculate the ROC AUC score for the binary predictions, which provides a measure of the efficiency of our model at distinguishing `true' track-playlist edges from non-existing ones. The higher this score, the better (a perfect score is achieved when ROC AUC = 1). This is done for you here -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r7Lx7AOM6k7o"
      },
      "outputs": [],
      "source": [
        "def metrics(labels, preds):\n",
        "  roc = roc_auc_score(labels.flatten().cpu().numpy(), preds.flatten().data.cpu().numpy())\n",
        "  return roc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H6kwQT-6k7o"
      },
      "source": [
        "## Section 2 Training and Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs8h30pc6k7x"
      },
      "source": [
        "Finally. It's time to write your train and test functions for training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw5j6Qmi6k7x"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "def train(datasets, model, optimizer, loss_fn, args, neg_samp = \"random\"):\n",
        "  print(f\"Beginning training for {model.name}\")\n",
        "\n",
        "  train_data = datasets['train']\n",
        "  val_data = datasets['val']\n",
        "\n",
        "  stats = {\n",
        "      'train': {\n",
        "        'loss': [],\n",
        "        'roc' : []\n",
        "      },\n",
        "      'val': {\n",
        "        'loss': [],\n",
        "        'recall': [],\n",
        "        'roc' : []\n",
        "      }\n",
        "\n",
        "  }\n",
        "  val_neg_edge, val_neg_label = sample_negative_edges_nocheck(val_data, n_playlists, n_tracks, device=torch.device(\"cuda\"))\n",
        "  for epoch in range(args[\"epochs\"]): # loop over each epoch\n",
        "    # Is your model ready?\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Negative sample the data - different methods may need different code\n",
        "    neg_edge_index, neg_edge_label = sample_negative_edges_nocheck(train_data, n_playlists, n_tracks, device=torch.device(\"cuda\"))\n",
        "\n",
        "\n",
        "    # calculate embedding\n",
        "    embedding = model.get_embedding(train_data.edge_index)\n",
        "    # calculate pos, negative scores using embedding\n",
        "    pos_scores = model(train_data.edge_index, train_data.edge_label_index)\n",
        "    neg_scores = model(neg_edge_index, neg_edge_label)\n",
        "    # concatenate pos, neg scores together and evaluate loss\n",
        "    pos_scores = pos_scores.view(-1)\n",
        "    neg_scores = neg_scores.view(-1)\n",
        "    scores = torch.cat([pos_scores, neg_scores])\n",
        "    labels = torch.cat([torch.ones_like(pos_scores), torch.zeros_like(neg_scores)])\n",
        "\n",
        "    # calculate loss function\n",
        "    if loss_fn == \"BCE\":\n",
        "      loss = nn.BCELoss(scores, labels)\n",
        "    elif loss_fn == \"BPR\":\n",
        "      loss = model.bpr_loss(pos_scores, neg_scores)\n",
        "\n",
        "    train_roc = metrics(labels, scores)\n",
        "\n",
        "    # Learn from your mistakes\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # Test on the validation set\n",
        "    val_loss, val_roc, val_neg_edge, val_neg_label = test(model, val_data, loss_fn, \"random\", 10, neg_edge_index, neg_edge_label)\n",
        "\n",
        "    # I mean it'd be useless if you didn't even save the results\n",
        "    stats['train']['loss'].append(loss)\n",
        "    stats['train']['roc'].append(train_roc)\n",
        "    stats['val']['loss'].append(val_loss)\n",
        "    stats['val']['roc'].append(val_roc)\n",
        "\n",
        "    print(f\"Epoch {epoch}; Train loss {loss}; Val loss {val_loss}; Train ROC {train_roc}; Val ROC {val_roc}\")\n",
        "\n",
        "    # if epoch % 10 == 0:\n",
        "      # calculate recall @ K - !ADJUST K!\n",
        "      # val_recall = recall_at_k(val_data, model, k = int(0.02*n_tracks), device = args[\"device\"])\n",
        "      # print(f\"Val recall {val_recall}\")\n",
        "      # stats['val']['recall'].append(val_recall)\n",
        "\n",
        "  pickle.dump(stats, open(f\"model_stats/{model.name}_{loss_fn}_{neg_samp}.pkl\", \"wb\"))\n",
        "  return stats\n",
        "\n",
        "def test(model, data, loss_fn, neg_samp, epoch = 0, neg_edge_index = None, neg_edge_label = None):\n",
        "\n",
        "  # I mean pretty similar now\n",
        "  # Is your model ready to evaluate?\n",
        "\n",
        "\n",
        "  with torch.no_grad(): # want to save RAM\n",
        "\n",
        "    # conduct negative sampling - different modes may need different code too\n",
        "    if neg_samp == \"random\":\n",
        "      neg_edge_index, neg_edge_label = sample_negative_edges_nocheck(data, n_playlists, n_tracks, device=torch.device(\"cuda\"))\n",
        "    # obtain model embedding\n",
        "    embed = model.get_embedding(data.edge_index)\n",
        "    # calculate pos, neg scores using embedding\n",
        "    pos_scores = model(data.edge_index, data.edge_label_index)\n",
        "    neg_scores = model(neg_edge_index, neg_edge_label)\n",
        "    # concatenate pos, neg scores together and evaluate loss\n",
        "    pos_scores = pos_scores.view(-1)\n",
        "    neg_scores = neg_scores.view(-1)\n",
        "    scores = torch.cat([pos_scores, neg_scores])\n",
        "    labels = torch.cat([torch.ones_like(pos_scores), torch.zeros_like(neg_scores)])\n",
        "    # calculate loss - BCE was the default loss, while BPR is the new loss\n",
        "    print(f'Loss function = {loss_fn}')\n",
        "    if loss_fn == \"BCE\":\n",
        "      loss = nn.BCELoss(scores, labels)\n",
        "    elif loss_fn == \"BPR\":\n",
        "      loss = model.bpr_loss(pos_scores, neg_scores)\n",
        "\n",
        "    roc = metrics(labels, scores)\n",
        "\n",
        "  return loss, roc, neg_edge_index, neg_edge_label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOTBDTlR6k7x"
      },
      "source": [
        "### Arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5zUgPEW6k7x"
      },
      "source": [
        "Maybe put your splits in a dict for easy access?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Enxx1s7W6k7y"
      },
      "outputs": [],
      "source": [
        "datasets = {\n",
        "    'train':train_split,\n",
        "    'val':val_split,\n",
        "    'test': test_split\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYZzHIQg6k7y"
      },
      "outputs": [],
      "source": [
        "training_args = {\n",
        "    'device' : 'cuda' if torch.cuda.is_available() else 'cpu',  # For the love of God, please do use a GPU though.\n",
        "    'num_layers' :  8,                      # Do you think this is a good choice?\n",
        "    'emb_size' : 64,\n",
        "    'weight_decay': 1e-5,\n",
        "    'lr': 0.01,\n",
        "    'epochs': 301\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcDhyP7q6k7y"
      },
      "source": [
        "### Model Initialisation\n",
        "Initialise your model with the parameters and nodes and whatever.\n",
        "\n",
        "The Adam optimizer should work just fine for the task, so initialise that well as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mq2LcE_M6k7y"
      },
      "outputs": [],
      "source": [
        "# initialize model and and optimizer\n",
        "num_nodes = n_playlists + n_tracks\n",
        "model = GCN(num_nodes = num_nodes,\n",
        "            embedding_dim=training_args['emb_size'],\n",
        "            num_layers=training_args['num_layers'], device=training_args['device'])\n",
        "model.cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=training_args['lr'], weight_decay=training_args['weight_decay'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_F4u_yD6k7y"
      },
      "source": [
        "It's time to put this data on a GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pA_ZE4t36k7y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b181508c-fb54-4c4e-938f-4694efb270c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GCN(15720, 64, num_layers=8)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "playlists_idx = torch.Tensor(playlists_idx).type(torch.int64).to(training_args[\"device\"])\n",
        "tracks_idx =torch.Tensor(tracks_idx).type(torch.int64).to(training_args[\"device\"])\n",
        "datasets['train'].to(training_args['device'])\n",
        "datasets['val'].to(training_args['device'])\n",
        "datasets['test'].to(training_args['device'])\n",
        "model.to(training_args[\"device\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evaAQEdH6k7y"
      },
      "source": [
        "The below code defines the directory to store the epoch-wise stats of your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz1Smh0q6k7y"
      },
      "outputs": [],
      "source": [
        "# create directory to save model_stats\n",
        "MODEL_STATS_DIR = \"model_stats\"\n",
        "if not os.path.exists(MODEL_STATS_DIR):\n",
        "  os.makedirs(MODEL_STATS_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weccnrj16k7y"
      },
      "source": [
        "### Call train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xh4TmMB56k7y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d11dac2-b068-4235-921f-e2169aa809a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beginning training for LGCN_LGC_8_e64_nodes15720_\n",
            "Loss function = BPR\n",
            "Epoch 0; Train loss 1.0882538557052612; Val loss 1.072357177734375; Train ROC 0.022372256720311005; Val ROC 0.032270890988325855\n",
            "Loss function = BPR\n",
            "Epoch 1; Train loss 1.0538883209228516; Val loss 1.0348387956619263; Train ROC 0.036658100840450694; Val ROC 0.052931259344085\n",
            "Loss function = BPR\n",
            "Epoch 2; Train loss 1.0166198015213013; Val loss 0.9946533441543579; Train ROC 0.059082615916986464; Val ROC 0.0830549988866622\n",
            "Loss function = BPR\n",
            "Epoch 3; Train loss 0.9767004251480103; Val loss 0.9521196484565735; Train ROC 0.09076481240386167; Val ROC 0.12196615453128479\n",
            "Loss function = BPR\n",
            "Epoch 4; Train loss 0.9344555735588074; Val loss 0.9076398015022278; Train ROC 0.13180920701352583; Val ROC 0.17008620415434042\n",
            "Loss function = BPR\n",
            "Epoch 5; Train loss 0.8902927041053772; Val loss 0.8616909980773926; Train ROC 0.18226578599975915; Val ROC 0.22568947418646818\n",
            "Loss function = BPR\n",
            "Epoch 6; Train loss 0.8446932435035706; Val loss 0.8148108124732971; Train ROC 0.24104962044528663; Val ROC 0.2879091516366065\n",
            "Loss function = BPR\n",
            "Epoch 7; Train loss 0.7981963157653809; Val loss 0.7675728797912598; Train ROC 0.3057999959102156; Val ROC 0.35473327607596145\n",
            "Loss function = BPR\n",
            "Epoch 8; Train loss 0.7513749003410339; Val loss 0.720559298992157; Train ROC 0.37555694911172155; Val ROC 0.42441231669688584\n",
            "Loss function = BPR\n",
            "Epoch 9; Train loss 0.7048085331916809; Val loss 0.6743317246437073; Train ROC 0.4453843374890087; Val ROC 0.49134777491490916\n",
            "Loss function = BPR\n",
            "Epoch 10; Train loss 0.659054696559906; Val loss 0.6294039487838745; Train ROC 0.5138132468116723; Val ROC 0.5540605019562935\n",
            "Loss function = BPR\n",
            "Epoch 11; Train loss 0.6146217584609985; Val loss 0.5862201452255249; Train ROC 0.577924025438459; Val ROC 0.6126220695358972\n",
            "Loss function = BPR\n",
            "Epoch 12; Train loss 0.5719484090805054; Val loss 0.5451412200927734; Train ROC 0.6359472735906717; Val ROC 0.6639628463275757\n",
            "Loss function = BPR\n",
            "Epoch 13; Train loss 0.5313897132873535; Val loss 0.5064371228218079; Train ROC 0.6867861338132014; Val ROC 0.7082736902376181\n",
            "Loss function = BPR\n",
            "Epoch 14; Train loss 0.49321022629737854; Val loss 0.4702865481376648; Train ROC 0.7295584623319497; Val ROC 0.7465566052740401\n",
            "Loss function = BPR\n",
            "Epoch 15; Train loss 0.45758363604545593; Val loss 0.4367818534374237; Train ROC 0.7660154821060572; Val ROC 0.7788434010878901\n",
            "Loss function = BPR\n",
            "Epoch 16; Train loss 0.42459768056869507; Val loss 0.40593937039375305; Train ROC 0.7960935742670766; Val ROC 0.8051420300919299\n",
            "Loss function = BPR\n",
            "Epoch 17; Train loss 0.39426442980766296; Val loss 0.37771180272102356; Train ROC 0.8209696878812872; Val ROC 0.8262397811496008\n",
            "Loss function = BPR\n",
            "Epoch 18; Train loss 0.366533100605011; Val loss 0.3520025312900543; Train ROC 0.8414447390603947; Val ROC 0.8447529980596112\n",
            "Loss function = BPR\n",
            "Epoch 19; Train loss 0.34130406379699707; Val loss 0.3286791741847992; Train ROC 0.8584991400092248; Val ROC 0.8601886312307154\n",
            "Loss function = BPR\n",
            "Epoch 20; Train loss 0.3184424638748169; Val loss 0.3075859546661377; Train ROC 0.8723192031282306; Val ROC 0.8726818716798677\n",
            "Loss function = BPR\n",
            "Epoch 21; Train loss 0.29779061675071716; Val loss 0.28855404257774353; Train ROC 0.8837717355000102; Val ROC 0.8827496262366001\n",
            "Loss function = BPR\n",
            "Epoch 22; Train loss 0.27917855978012085; Val loss 0.2714102566242218; Train ROC 0.8934395314015918; Val ROC 0.8912189458281643\n",
            "Loss function = BPR\n",
            "Epoch 23; Train loss 0.26243212819099426; Val loss 0.25598323345184326; Train ROC 0.9013259989866423; Val ROC 0.898360212488469\n",
            "Loss function = BPR\n",
            "Epoch 24; Train loss 0.24737973511219025; Val loss 0.2421078085899353; Train ROC 0.9079696265345212; Val ROC 0.9048970957788594\n",
            "Loss function = BPR\n",
            "Epoch 25; Train loss 0.23385635018348694; Val loss 0.22962850332260132; Train ROC 0.9134101758380082; Val ROC 0.9103604033463752\n",
            "Loss function = BPR\n",
            "Epoch 26; Train loss 0.2217067927122116; Val loss 0.21840080618858337; Train ROC 0.9180077751345652; Val ROC 0.9151477558291186\n",
            "Loss function = BPR\n",
            "Epoch 27; Train loss 0.21078726649284363; Val loss 0.20829235017299652; Train ROC 0.9219385123636454; Val ROC 0.9188694850017496\n",
            "Loss function = BPR\n",
            "Epoch 28; Train loss 0.20096620917320251; Val loss 0.19918280839920044; Train ROC 0.9253739312598126; Val ROC 0.9221697362979928\n",
            "Loss function = BPR\n",
            "Epoch 29; Train loss 0.19212432205677032; Val loss 0.19096380472183228; Train ROC 0.9282401884936188; Val ROC 0.924690651143557\n",
            "Loss function = BPR\n",
            "Epoch 30; Train loss 0.184154212474823; Val loss 0.18353822827339172; Train ROC 0.9307224604142952; Val ROC 0.9273785666571238\n",
            "Loss function = BPR\n",
            "Epoch 31; Train loss 0.17695996165275574; Val loss 0.1768193393945694; Train ROC 0.9329741139368491; Val ROC 0.9297324808346853\n",
            "Loss function = BPR\n",
            "Epoch 32; Train loss 0.1704559475183487; Val loss 0.1707301139831543; Train ROC 0.9349644756782793; Val ROC 0.9316967267869072\n",
            "Loss function = BPR\n",
            "Epoch 33; Train loss 0.16456623375415802; Val loss 0.1652020364999771; Train ROC 0.9366878653824744; Val ROC 0.9335257817221745\n",
            "Loss function = BPR\n",
            "Epoch 34; Train loss 0.15922343730926514; Val loss 0.16017451882362366; Train ROC 0.9382374392496609; Val ROC 0.9350685497980087\n",
            "Loss function = BPR\n",
            "Epoch 35; Train loss 0.15436802804470062; Val loss 0.1555938571691513; Train ROC 0.9396154693822835; Val ROC 0.9362455068867894\n",
            "Loss function = BPR\n",
            "Epoch 36; Train loss 0.14994728565216064; Val loss 0.1514124721288681; Train ROC 0.9408685338804555; Val ROC 0.9375894646435728\n",
            "Loss function = BPR\n",
            "Epoch 37; Train loss 0.14591464400291443; Val loss 0.147588312625885; Train ROC 0.9420023130002886; Val ROC 0.9388141362089258\n",
            "Loss function = BPR\n",
            "Epoch 38; Train loss 0.142228901386261; Val loss 0.14408408105373383; Train ROC 0.9429884054612254; Val ROC 0.9400388077742787\n",
            "Loss function = BPR\n",
            "Epoch 39; Train loss 0.13885360956192017; Val loss 0.1408666968345642; Train ROC 0.9438767975170465; Val ROC 0.9409771924801985\n",
            "Loss function = BPR\n",
            "Epoch 40; Train loss 0.1357564628124237; Val loss 0.13790681958198547; Train ROC 0.9446924822946418; Val ROC 0.941828100645736\n",
            "Loss function = BPR\n",
            "Epoch 41; Train loss 0.13290885090827942; Val loss 0.135178342461586; Train ROC 0.9454706773819018; Val ROC 0.9427903425899418\n",
            "Loss function = BPR\n",
            "Epoch 42; Train loss 0.1302853375673294; Val loss 0.13265810906887054; Train ROC 0.9462466003667174; Val ROC 0.9435299169768108\n",
            "Loss function = BPR\n",
            "Epoch 43; Train loss 0.12786337733268738; Val loss 0.13032540678977966; Train ROC 0.9469270950488615; Val ROC 0.944341063078538\n",
            "Loss function = BPR\n",
            "Epoch 44; Train loss 0.12562282383441925; Val loss 0.12816175818443298; Train ROC 0.9475780523992265; Val ROC 0.945080637465407\n",
            "Loss function = BPR\n",
            "Epoch 45; Train loss 0.12354572862386703; Val loss 0.1261506825685501; Train ROC 0.9481506222152545; Val ROC 0.9455895918821771\n",
            "Loss function = BPR\n",
            "Epoch 46; Train loss 0.12161608785390854; Val loss 0.12427742779254913; Train ROC 0.9487106954678373; Val ROC 0.9462894042052359\n",
            "Loss function = BPR\n",
            "Epoch 47; Train loss 0.11981957405805588; Val loss 0.12252873927354813; Train ROC 0.949266224515531; Val ROC 0.9469574068772466\n",
            "Loss function = BPR\n",
            "Epoch 48; Train loss 0.11814335733652115; Val loss 0.120892733335495; Train ROC 0.9497660870533331; Val ROC 0.947553837834399\n",
            "Loss function = BPR\n",
            "Epoch 49; Train loss 0.11657591164112091; Val loss 0.11935873329639435; Train ROC 0.9501966504665762; Val ROC 0.9481661736170754\n",
            "Loss function = BPR\n",
            "Epoch 50; Train loss 0.11510690301656723; Val loss 0.11791712790727615; Train ROC 0.9506317580847086; Val ROC 0.9486115087317493\n",
            "Loss function = BPR\n",
            "Epoch 51; Train loss 0.11372701078653336; Val loss 0.11655924469232559; Train ROC 0.9510475528320621; Val ROC 0.9491761300378535\n",
            "Loss function = BPR\n",
            "Epoch 52; Train loss 0.11242785304784775; Val loss 0.11527732014656067; Train ROC 0.9514349462988587; Val ROC 0.949541941024907\n",
            "Loss function = BPR\n",
            "Epoch 53; Train loss 0.11120188981294632; Val loss 0.11406423151493073; Train ROC 0.9517746256143197; Val ROC 0.9499475140757706\n",
            "Loss function = BPR\n",
            "Epoch 54; Train loss 0.11004228889942169; Val loss 0.11291364580392838; Train ROC 0.9521733795933391; Val ROC 0.9504485160797786\n",
            "Loss function = BPR\n",
            "Epoch 55; Train loss 0.10894288867712021; Val loss 0.11181982606649399; Train ROC 0.9525278275746897; Val ROC 0.950790469828546\n",
            "Loss function = BPR\n",
            "Epoch 56; Train loss 0.10789813846349716; Val loss 0.11077755689620972; Train ROC 0.9528663708389283; Val ROC 0.951291471832554\n",
            "Loss function = BPR\n",
            "Epoch 57; Train loss 0.10690300166606903; Val loss 0.1097821444272995; Train ROC 0.9532083222568339; Val ROC 0.9516254731685594\n",
            "Loss function = BPR\n",
            "Epoch 58; Train loss 0.10595295578241348; Val loss 0.1088293120265007; Train ROC 0.953524144496627; Val ROC 0.952023093806661\n",
            "Loss function = BPR\n",
            "Epoch 59; Train loss 0.10504385828971863; Val loss 0.10791520774364471; Train ROC 0.9538615517096435; Val ROC 0.9523968572064765\n",
            "Loss function = BPR\n",
            "Epoch 60; Train loss 0.1041720062494278; Val loss 0.10703638195991516; Train ROC 0.9541682855396584; Val ROC 0.952794477844578\n",
            "Loss function = BPR\n",
            "Epoch 61; Train loss 0.10333403944969177; Val loss 0.10618966817855835; Train ROC 0.9544545704476723; Val ROC 0.9531284791805834\n",
            "Loss function = BPR\n",
            "Epoch 62; Train loss 0.10252691060304642; Val loss 0.10537222027778625; Train ROC 0.9547738008411323; Val ROC 0.9535101949931609\n",
            "Loss function = BPR\n",
            "Epoch 63; Train loss 0.10174787044525146; Val loss 0.10458147525787354; Train ROC 0.9550248681612556; Val ROC 0.9537249101377357\n",
            "Loss function = BPR\n",
            "Epoch 64; Train loss 0.10099445283412933; Val loss 0.10381513833999634; Train ROC 0.9553122891204918; Val ROC 0.9540032445844069\n",
            "Loss function = BPR\n",
            "Epoch 65; Train loss 0.10026443749666214; Val loss 0.10307110846042633; Train ROC 0.95561561479684; Val ROC 0.9542259121417438\n",
            "Loss function = BPR\n",
            "Epoch 66; Train loss 0.09955576062202454; Val loss 0.10234750807285309; Train ROC 0.9558973554999648; Val ROC 0.954536056239463\n",
            "Loss function = BPR\n",
            "Epoch 67; Train loss 0.09886665642261505; Val loss 0.10164263844490051; Train ROC 0.9561575112298664; Val ROC 0.9548462003371823\n",
            "Loss function = BPR\n",
            "Epoch 68; Train loss 0.09819547086954117; Val loss 0.10095501691102982; Train ROC 0.9564256193183238; Val ROC 0.9550768203072812\n",
            "Loss function = BPR\n",
            "Epoch 69; Train loss 0.09754077345132828; Val loss 0.10028324276208878; Train ROC 0.956659645870113; Val ROC 0.9553710595794764\n",
            "Loss function = BPR\n",
            "Epoch 70; Train loss 0.09690122306346893; Val loss 0.09962613880634308; Train ROC 0.9569311621122373; Val ROC 0.9557925374558641\n",
            "Loss function = BPR\n",
            "Epoch 71; Train loss 0.09627564996480942; Val loss 0.09898258745670319; Train ROC 0.9572038144055839; Val ROC 0.9560788243152972\n",
            "Loss function = BPR\n",
            "Epoch 72; Train loss 0.09566301107406616; Val loss 0.09835165739059448; Train ROC 0.9574810109038197; Val ROC 0.9563810160002545\n",
            "Loss function = BPR\n",
            "Epoch 73; Train loss 0.09506238251924515; Val loss 0.09773245453834534; Train ROC 0.9577150374556088; Val ROC 0.9566116359703534\n",
            "Loss function = BPR\n",
            "Epoch 74; Train loss 0.09447293728590012; Val loss 0.09712421149015427; Train ROC 0.9579626966220653; Val ROC 0.9569217800680726\n",
            "Loss function = BPR\n",
            "Epoch 75; Train loss 0.09389390051364899; Val loss 0.09652627259492874; Train ROC 0.9582046755324104; Val ROC 0.9571683048636956\n",
            "Loss function = BPR\n",
            "Epoch 76; Train loss 0.09332465380430222; Val loss 0.09593801945447922; Train ROC 0.9584352939305327; Val ROC 0.9574466393103668\n",
            "Loss function = BPR\n",
            "Epoch 77; Train loss 0.09276460111141205; Val loss 0.09535890817642212; Train ROC 0.9586761367896556; Val ROC 0.9576693068677037\n",
            "Loss function = BPR\n",
            "Epoch 78; Train loss 0.0922132208943367; Val loss 0.09478849172592163; Train ROC 0.9589147075463339; Val ROC 0.9579158316633266\n",
            "Loss function = BPR\n",
            "Epoch 79; Train loss 0.09167009592056274; Val loss 0.09422633051872253; Train ROC 0.9591635027640126; Val ROC 0.9581862136972358\n",
            "Loss function = BPR\n",
            "Epoch 80; Train loss 0.09113478660583496; Val loss 0.09367210417985916; Train ROC 0.9594111619304692; Val ROC 0.9583850240162866\n",
            "Loss function = BPR\n",
            "Epoch 81; Train loss 0.09060697257518768; Val loss 0.09312545508146286; Train ROC 0.9596474605847028; Val ROC 0.9586633584629577\n",
            "Loss function = BPR\n",
            "Epoch 82; Train loss 0.09008633345365524; Val loss 0.09258610010147095; Train ROC 0.9598883034438257; Val ROC 0.9589496453223908\n",
            "Loss function = BPR\n",
            "Epoch 83; Train loss 0.08957261592149734; Val loss 0.09205383062362671; Train ROC 0.9601268742005039; Val ROC 0.9593472659604925\n",
            "Loss function = BPR\n",
            "Epoch 84; Train loss 0.0890655592083931; Val loss 0.09152840822935104; Train ROC 0.9603790775718496; Val ROC 0.9595778859305913\n",
            "Loss function = BPR\n",
            "Epoch 85; Train loss 0.08856500685214996; Val loss 0.09100966155529022; Train ROC 0.9606176483285278; Val ROC 0.9598800776155486\n",
            "Loss function = BPR\n",
            "Epoch 86; Train loss 0.08807075023651123; Val loss 0.09049741178750992; Train ROC 0.9608539469827616; Val ROC 0.9601186499984096\n",
            "Loss function = BPR\n",
            "Epoch 87; Train loss 0.08758264034986496; Val loss 0.08999153226613998; Train ROC 0.9610857014321061; Val ROC 0.9603810796195565\n",
            "Loss function = BPR\n",
            "Epoch 88; Train loss 0.08710052818059921; Val loss 0.08949188143014908; Train ROC 0.9613276803424513; Val ROC 0.9605321754620352\n",
            "Loss function = BPR\n",
            "Epoch 89; Train loss 0.08662432432174683; Val loss 0.08899836242198944; Train ROC 0.9615969244821311; Val ROC 0.9608582243852785\n",
            "Loss function = BPR\n",
            "Epoch 90; Train loss 0.08615391701459885; Val loss 0.08851087838411331; Train ROC 0.9619013862097014; Val ROC 0.9611604160702357\n",
            "Loss function = BPR\n",
            "Epoch 91; Train loss 0.08568920195102692; Val loss 0.08802934736013412; Train ROC 0.9621126917370451; Val ROC 0.9614626077551929\n",
            "Loss function = BPR\n",
            "Epoch 92; Train loss 0.08523011207580566; Val loss 0.08755367249250412; Train ROC 0.9623501264425011; Val ROC 0.9617011801380538\n",
            "Loss function = BPR\n",
            "Epoch 93; Train loss 0.08477655798196793; Val loss 0.08708379417657852; Train ROC 0.962595513506513; Val ROC 0.961987466997487\n",
            "Loss function = BPR\n",
            "Epoch 94; Train loss 0.08432850241661072; Val loss 0.08661966770887375; Train ROC 0.96285339713397; Val ROC 0.9622737538569202\n",
            "Loss function = BPR\n",
            "Epoch 95; Train loss 0.08388587832450867; Val loss 0.08616118878126144; Train ROC 0.9630669747637581; Val ROC 0.962480516588733\n",
            "Loss function = BPR\n",
            "Epoch 96; Train loss 0.0834486111998558; Val loss 0.0857083648443222; Train ROC 0.9633146339302147; Val ROC 0.9628145179247384\n",
            "Loss function = BPR\n",
            "Epoch 97; Train loss 0.08301668614149094; Val loss 0.08526109904050827; Train ROC 0.9635850141211167; Val ROC 0.9631087571969336\n",
            "Loss function = BPR\n",
            "Epoch 98; Train loss 0.0825900286436081; Val loss 0.08481934666633606; Train ROC 0.96383608144124; Val ROC 0.9633632344053186\n",
            "Loss function = BPR\n",
            "Epoch 99; Train loss 0.08216860145330429; Val loss 0.08438309282064438; Train ROC 0.9640882848125857; Val ROC 0.9636415688519897\n",
            "Loss function = BPR\n",
            "Epoch 100; Train loss 0.08175237476825714; Val loss 0.08395227044820786; Train ROC 0.9643245834668194; Val ROC 0.963983522600757\n",
            "Loss function = BPR\n",
            "Epoch 101; Train loss 0.08134128153324127; Val loss 0.08352681249380112; Train ROC 0.964576786838165; Val ROC 0.9641902853325699\n",
            "Loss function = BPR\n",
            "Epoch 102; Train loss 0.08093530684709549; Val loss 0.08310669660568237; Train ROC 0.9648233099533992; Val ROC 0.964460667366479\n",
            "Loss function = BPR\n",
            "Epoch 103; Train loss 0.08053438365459442; Val loss 0.08269187062978745; Train ROC 0.9650573365051883; Val ROC 0.9647628590514362\n",
            "Loss function = BPR\n",
            "Epoch 104; Train loss 0.08013848960399628; Val loss 0.08228228986263275; Train ROC 0.9653311248497572; Val ROC 0.9650570983236314\n",
            "Loss function = BPR\n",
            "Epoch 105; Train loss 0.07974756509065628; Val loss 0.08187790960073471; Train ROC 0.9656037771431039; Val ROC 0.9652797658809683\n",
            "Loss function = BPR\n",
            "Epoch 106; Train loss 0.07936158776283264; Val loss 0.08147867769002914; Train ROC 0.9658718852315613; Val ROC 0.9655024334383052\n",
            "Loss function = BPR\n",
            "Epoch 107; Train loss 0.07898049801588058; Val loss 0.08108456432819366; Train ROC 0.9661399933200188; Val ROC 0.9658284823615485\n",
            "Loss function = BPR\n",
            "Epoch 108; Train loss 0.07860425114631653; Val loss 0.08069547265768051; Train ROC 0.9663626593595852; Val ROC 0.9660511499188854\n",
            "Loss function = BPR\n",
            "Epoch 109; Train loss 0.07823281735181808; Val loss 0.08031140267848969; Train ROC 0.966631903499265; Val ROC 0.9663056271272704\n",
            "Loss function = BPR\n",
            "Epoch 110; Train loss 0.07786613702774048; Val loss 0.07993228733539581; Train ROC 0.9669034197413893; Val ROC 0.9665362470973693\n",
            "Loss function = BPR\n",
            "Epoch 111; Train loss 0.07750416547060013; Val loss 0.07955805957317352; Train ROC 0.9671635754712908; Val ROC 0.9668225339568025\n",
            "Loss function = BPR\n",
            "Epoch 112; Train loss 0.07714684307575226; Val loss 0.07918868213891983; Train ROC 0.9674203230475256; Val ROC 0.9671088208162356\n",
            "Loss function = BPR\n",
            "Epoch 113; Train loss 0.07679413259029388; Val loss 0.07882408052682877; Train ROC 0.967675934572538; Val ROC 0.9673712504373827\n",
            "Loss function = BPR\n",
            "Epoch 114; Train loss 0.07644597440958023; Val loss 0.07846422493457794; Train ROC 0.9679167774316608; Val ROC 0.9676813945351019\n",
            "Loss function = BPR\n",
            "Epoch 115; Train loss 0.07610233128070831; Val loss 0.07810905575752258; Train ROC 0.9681792052640069; Val ROC 0.9681028724114896\n",
            "Loss function = BPR\n",
            "Epoch 116; Train loss 0.07576315104961395; Val loss 0.07775850594043732; Train ROC 0.9684245923280189; Val ROC 0.9683175875560646\n",
            "Loss function = BPR\n",
            "Epoch 117; Train loss 0.07542836666107178; Val loss 0.07741250842809677; Train ROC 0.9686892922628095; Val ROC 0.9685959220027356\n",
            "Loss function = BPR\n",
            "Epoch 118; Train loss 0.07509791851043701; Val loss 0.07707103341817856; Train ROC 0.9689255909170432; Val ROC 0.9687788274962623\n",
            "Loss function = BPR\n",
            "Epoch 119; Train loss 0.07477176189422607; Val loss 0.07673399895429611; Train ROC 0.9691664337761661; Val ROC 0.9690571619429335\n",
            "Loss function = BPR\n",
            "Epoch 120; Train loss 0.07444987446069717; Val loss 0.07640136033296585; Train ROC 0.9694129568914003; Val ROC 0.9693911632789388\n",
            "Loss function = BPR\n",
            "Epoch 121; Train loss 0.07413213700056076; Val loss 0.076073057949543; Train ROC 0.9696787928774132; Val ROC 0.9695740687724655\n",
            "Loss function = BPR\n",
            "Epoch 122; Train loss 0.07381853461265564; Val loss 0.07574900984764099; Train ROC 0.9699605335805381; Val ROC 0.9697887839170404\n",
            "Loss function = BPR\n",
            "Epoch 123; Train loss 0.07350900769233704; Val loss 0.07542919367551804; Train ROC 0.9702161451055505; Val ROC 0.9700194038871394\n",
            "Loss function = BPR\n",
            "Epoch 124; Train loss 0.07320348173379898; Val loss 0.07511350512504578; Train ROC 0.9704774368866743; Val ROC 0.9702341190317142\n",
            "Loss function = BPR\n",
            "Epoch 125; Train loss 0.07290191203355789; Val loss 0.07480192184448242; Train ROC 0.9707478170775764; Val ROC 0.9704806438273372\n",
            "Loss function = BPR\n",
            "Epoch 126; Train loss 0.07260425388813019; Val loss 0.07449435442686081; Train ROC 0.9710113809611448; Val ROC 0.9707192162101982\n",
            "Loss function = BPR\n",
            "Epoch 127; Train loss 0.0723104253411293; Val loss 0.07419077306985855; Train ROC 0.9712669924861572; Val ROC 0.9710452651334415\n",
            "Loss function = BPR\n",
            "Epoch 128; Train loss 0.07202038913965225; Val loss 0.07389109581708908; Train ROC 0.9715123795501691; Val ROC 0.9713872188822089\n",
            "Loss function = BPR\n",
            "Epoch 129; Train loss 0.07173407077789307; Val loss 0.07359527796506882; Train ROC 0.9717759434337375; Val ROC 0.971657600916118\n",
            "Loss function = BPR\n",
            "Epoch 130; Train loss 0.07145143300294876; Val loss 0.07330325245857239; Train ROC 0.9720304189075277; Val ROC 0.9718723160606928\n",
            "Loss function = BPR\n",
            "Epoch 131; Train loss 0.07117241621017456; Val loss 0.07301495224237442; Train ROC 0.9723064793545412; Val ROC 0.9721188408563158\n",
            "Loss function = BPR\n",
            "Epoch 132; Train loss 0.07089695334434509; Val loss 0.07273034006357193; Train ROC 0.9725552745722199; Val ROC 0.9724289849540351\n",
            "Loss function = BPR\n",
            "Epoch 133; Train loss 0.07062500715255737; Val loss 0.07244934886693954; Train ROC 0.9727983895337873; Val ROC 0.9727391290517543\n",
            "Loss function = BPR\n",
            "Epoch 134; Train loss 0.07035651057958603; Val loss 0.07217191904783249; Train ROC 0.973042640546577; Val ROC 0.9730015586729014\n",
            "Loss function = BPR\n",
            "Epoch 135; Train loss 0.07009140402078629; Val loss 0.07189799100160599; Train ROC 0.9733164288911459; Val ROC 0.9732083214047141\n",
            "Loss function = BPR\n",
            "Epoch 136; Train loss 0.06982965022325516; Val loss 0.07162752002477646; Train ROC 0.9735811288259365; Val ROC 0.9734389413748131\n",
            "Loss function = BPR\n",
            "Epoch 137; Train loss 0.06957118958234787; Val loss 0.07136044651269913; Train ROC 0.9738515090168386; Val ROC 0.9737252282342462\n",
            "Loss function = BPR\n",
            "Epoch 138; Train loss 0.06931595504283905; Val loss 0.07109671086072922; Train ROC 0.9741184810540737; Val ROC 0.973908133727773\n",
            "Loss function = BPR\n",
            "Epoch 139; Train loss 0.06906391680240631; Val loss 0.07083627581596375; Train ROC 0.9743525076058629; Val ROC 0.974138753697872\n",
            "Loss function = BPR\n",
            "Epoch 140; Train loss 0.06881501525640488; Val loss 0.07057906687259674; Train ROC 0.974622887796765; Val ROC 0.9743455164296848\n",
            "Loss function = BPR\n",
            "Epoch 141; Train loss 0.06856921315193176; Val loss 0.07032503932714462; Train ROC 0.9748591864509987; Val ROC 0.9745284219232115\n",
            "Loss function = BPR\n",
            "Epoch 142; Train loss 0.068326435983181; Val loss 0.07007414847612381; Train ROC 0.9751159340272334; Val ROC 0.9747908515443585\n",
            "Loss function = BPR\n",
            "Epoch 143; Train loss 0.0680866464972496; Val loss 0.06982634216547012; Train ROC 0.9753897223718023; Val ROC 0.9750055666889335\n",
            "Loss function = BPR\n",
            "Epoch 144; Train loss 0.0678497850894928; Val loss 0.06958156079053879; Train ROC 0.9756237489235915; Val ROC 0.9752361866590323\n",
            "Loss function = BPR\n",
            "Epoch 145; Train loss 0.06761582940816879; Val loss 0.06933976709842682; Train ROC 0.9758827686022707; Val ROC 0.9754668066291313\n",
            "Loss function = BPR\n",
            "Epoch 146; Train loss 0.06738472729921341; Val loss 0.06910090893507004; Train ROC 0.9761065706930594; Val ROC 0.9757212838375163\n",
            "Loss function = BPR\n",
            "Epoch 147; Train loss 0.06715641170740128; Val loss 0.06886494159698486; Train ROC 0.9763405972448486; Val ROC 0.9758803320927569\n",
            "Loss function = BPR\n",
            "Epoch 148; Train loss 0.06693085283041; Val loss 0.06863180547952652; Train ROC 0.9766018890259724; Val ROC 0.9761904761904762\n",
            "Loss function = BPR\n",
            "Epoch 149; Train loss 0.06670800596475601; Val loss 0.06840147078037262; Train ROC 0.9768529563460957; Val ROC 0.9764449533988612\n",
            "Loss function = BPR\n",
            "Epoch 150; Train loss 0.06648783385753632; Val loss 0.06817388534545898; Train ROC 0.9771006155125522; Val ROC 0.9767550974965805\n",
            "Loss function = BPR\n",
            "Epoch 151; Train loss 0.06627027690410614; Val loss 0.06794901937246323; Train ROC 0.9773403223204528; Val ROC 0.9769698126411553\n",
            "Loss function = BPR\n",
            "Epoch 152; Train loss 0.0660553053021431; Val loss 0.06772679835557938; Train ROC 0.9775879814869093; Val ROC 0.977136813309158\n",
            "Loss function = BPR\n",
            "Epoch 153; Train loss 0.0658428743481636; Val loss 0.06750722229480743; Train ROC 0.9778254161923653; Val ROC 0.9773276712154467\n",
            "Loss function = BPR\n",
            "Epoch 154; Train loss 0.06563294678926468; Val loss 0.06729020923376083; Train ROC 0.9780299054123752; Val ROC 0.9775741960110698\n",
            "Loss function = BPR\n",
            "Epoch 155; Train loss 0.06542547792196274; Val loss 0.06707572937011719; Train ROC 0.978258251708053; Val ROC 0.9778127683939307\n",
            "Loss function = BPR\n",
            "Epoch 156; Train loss 0.0652204304933548; Val loss 0.0668637603521347; Train ROC 0.978495686413509; Val ROC 0.9780195311257436\n",
            "Loss function = BPR\n",
            "Epoch 157; Train loss 0.06501777470111847; Val loss 0.0666542500257492; Train ROC 0.9787035837871858; Val ROC 0.9782978655724147\n",
            "Loss function = BPR\n",
            "Epoch 158; Train loss 0.06481745094060898; Val loss 0.06644714623689651; Train ROC 0.9789205695706408; Val ROC 0.9786000572573719\n",
            "Loss function = BPR\n",
            "Epoch 159; Train loss 0.06461945176124573; Val loss 0.06624244153499603; Train ROC 0.9791693647883196; Val ROC 0.9788306772274709\n",
            "Loss function = BPR\n",
            "Epoch 160; Train loss 0.06442371755838394; Val loss 0.06604006886482239; Train ROC 0.9793659016497736; Val ROC 0.9790772020230938\n",
            "Loss function = BPR\n",
            "Epoch 161; Train loss 0.06423022598028183; Val loss 0.06584001332521439; Train ROC 0.9795635745624499; Val ROC 0.979363488882527\n",
            "Loss function = BPR\n",
            "Epoch 162; Train loss 0.06403893232345581; Val loss 0.06564222276210785; Train ROC 0.9798123697801286; Val ROC 0.9795861564398639\n",
            "Loss function = BPR\n",
            "Epoch 163; Train loss 0.0638498067855835; Val loss 0.06544667482376099; Train ROC 0.9800100426928049; Val ROC 0.9798088239972007\n",
            "Loss function = BPR\n",
            "Epoch 164; Train loss 0.0636628195643425; Val loss 0.06525333225727081; Train ROC 0.9802383889884827; Val ROC 0.9800076343162516\n",
            "Loss function = BPR\n",
            "Epoch 165; Train loss 0.06347793340682983; Val loss 0.06506215780973434; Train ROC 0.9804190211328249; Val ROC 0.9802223494608264\n",
            "Loss function = BPR\n",
            "Epoch 166; Train loss 0.06329511851072311; Val loss 0.0648731142282486; Train ROC 0.9806291906089462; Val ROC 0.9804211597798772\n",
            "Loss function = BPR\n",
            "Epoch 167; Train loss 0.06311433762311935; Val loss 0.06468617916107178; Train ROC 0.980857536904624; Val ROC 0.980635874924452\n",
            "Loss function = BPR\n",
            "Epoch 168; Train loss 0.06293556094169617; Val loss 0.0645013228058815; Train ROC 0.981054073766078; Val ROC 0.980874447307313\n",
            "Loss function = BPR\n",
            "Epoch 169; Train loss 0.06275876611471176; Val loss 0.06431850045919418; Train ROC 0.9812653792934216; Val ROC 0.981113019690174\n",
            "Loss function = BPR\n",
            "Epoch 170; Train loss 0.06258392333984375; Val loss 0.06413768976926804; Train ROC 0.9814687324622092; Val ROC 0.9813197824219868\n",
            "Loss function = BPR\n",
            "Epoch 171; Train loss 0.062410999089479446; Val loss 0.06395885348320007; Train ROC 0.9816789019383306; Val ROC 0.9815344975665617\n",
            "Loss function = BPR\n",
            "Epoch 172; Train loss 0.062239959836006165; Val loss 0.0637819692492485; Train ROC 0.9818788469534514; Val ROC 0.9817730699494227\n",
            "Loss function = BPR\n",
            "Epoch 173; Train loss 0.06207077577710152; Val loss 0.06360700726509094; Train ROC 0.9820867443271282; Val ROC 0.9819321182046633\n",
            "Loss function = BPR\n",
            "Epoch 174; Train loss 0.06190342828631401; Val loss 0.063433937728405; Train ROC 0.9823014580081386; Val ROC 0.98211502369819\n",
            "Loss function = BPR\n",
            "Epoch 175; Train loss 0.06173789128661156; Val loss 0.06326273083686829; Train ROC 0.9824957227671481; Val ROC 0.9822740719534306\n",
            "Loss function = BPR\n",
            "Epoch 176; Train loss 0.06157413497567177; Val loss 0.06309336423873901; Train ROC 0.982687715423713; Val ROC 0.9824808346852435\n",
            "Loss function = BPR\n",
            "Epoch 177; Train loss 0.06141212210059166; Val loss 0.0629258081316948; Train ROC 0.9828910685925007; Val ROC 0.9827671215446766\n",
            "Loss function = BPR\n",
            "Epoch 178; Train loss 0.061251845210790634; Val loss 0.06276003271341324; Train ROC 0.9830864694027325; Val ROC 0.9829261697999173\n",
            "Loss function = BPR\n",
            "Epoch 179; Train loss 0.061093274503946304; Val loss 0.06259602308273315; Train ROC 0.9832705097007414; Val ROC 0.983077265642396\n",
            "Loss function = BPR\n",
            "Epoch 180; Train loss 0.060936376452445984; Val loss 0.06243373826146126; Train ROC 0.9834590942036394; Val ROC 0.9832522187231606\n",
            "Loss function = BPR\n",
            "Epoch 181; Train loss 0.06078113242983818; Val loss 0.062273167073726654; Train ROC 0.9836351821430924; Val ROC 0.9834669338677354\n",
            "Loss function = BPR\n",
            "Epoch 182; Train loss 0.060627520084381104; Val loss 0.062114279717206955; Train ROC 0.9838124061337677; Val ROC 0.9836180297102141\n",
            "Loss function = BPR\n",
            "Epoch 183; Train loss 0.06047551706433296; Val loss 0.06195705384016037; Train ROC 0.9840089429952218; Val ROC 0.9837770779654548\n",
            "Loss function = BPR\n",
            "Epoch 184; Train loss 0.060325101017951965; Val loss 0.06180146709084511; Train ROC 0.9841986635493422; Val ROC 0.9839361262206954\n",
            "Loss function = BPR\n",
            "Epoch 185; Train loss 0.060176242142915726; Val loss 0.061647482216358185; Train ROC 0.9843724793863505; Val ROC 0.9840554124121258\n",
            "Loss function = BPR\n",
            "Epoch 186; Train loss 0.06002891808748245; Val loss 0.0614950954914093; Train ROC 0.9845326626086917; Val ROC 0.9841985558418425\n",
            "Loss function = BPR\n",
            "Epoch 187; Train loss 0.05988312140107155; Val loss 0.06134428083896637; Train ROC 0.9847042063432556; Val ROC 0.9843576040970831\n",
            "Loss function = BPR\n",
            "Epoch 188; Train loss 0.059738822281360626; Val loss 0.061195008456707; Train ROC 0.9848825663851531; Val ROC 0.9845166523523237\n",
            "Loss function = BPR\n",
            "Epoch 189; Train loss 0.059595994651317596; Val loss 0.0610472597181797; Train ROC 0.9850507019660503; Val ROC 0.9847075102586125\n",
            "Loss function = BPR\n",
            "Epoch 190; Train loss 0.059454627335071564; Val loss 0.06090101972222328; Train ROC 0.9852392864689483; Val ROC 0.9849619874669975\n",
            "Loss function = BPR\n",
            "Epoch 191; Train loss 0.05931468680500984; Val loss 0.06075626239180565; Train ROC 0.9854085581010676; Val ROC 0.9851369405477622\n",
            "Loss function = BPR\n",
            "Epoch 192; Train loss 0.05917617306113243; Val loss 0.06061296910047531; Train ROC 0.9855551087087414; Val ROC 0.9852880363902408\n",
            "Loss function = BPR\n",
            "Epoch 193; Train loss 0.059039052575826645; Val loss 0.06047111749649048; Train ROC 0.985700523265193; Val ROC 0.9854073225816713\n",
            "Loss function = BPR\n",
            "Epoch 194; Train loss 0.05890331417322159; Val loss 0.06033069267868996; Train ROC 0.9858572983338673; Val ROC 0.985590228075198\n",
            "Loss function = BPR\n",
            "Epoch 195; Train loss 0.05876893550157547; Val loss 0.060191668570041656; Train ROC 0.9860049849927633; Val ROC 0.9857572287432007\n",
            "Loss function = BPR\n",
            "Epoch 196; Train loss 0.0586358942091465; Val loss 0.06005403771996498; Train ROC 0.9861424471906589; Val ROC 0.9859003721729173\n",
            "Loss function = BPR\n",
            "Epoch 197; Train loss 0.05850417539477348; Val loss 0.05991777032613754; Train ROC 0.9862946780544442; Val ROC 0.9860196583643478\n",
            "Loss function = BPR\n",
            "Epoch 198; Train loss 0.05837376415729523; Val loss 0.05978285148739815; Train ROC 0.9864480449694516; Val ROC 0.9861946114451124\n",
            "Loss function = BPR\n",
            "Epoch 199; Train loss 0.05824463814496994; Val loss 0.059649258852005005; Train ROC 0.986593459525903; Val ROC 0.986353659700353\n",
            "Loss function = BPR\n",
            "Epoch 200; Train loss 0.05811678618192673; Val loss 0.059516988694667816; Train ROC 0.9867297856725764; Val ROC 0.9865365651938798\n",
            "Loss function = BPR\n",
            "Epoch 201; Train loss 0.0579901859164238; Val loss 0.059386011213064194; Train ROC 0.9868627036655828; Val ROC 0.9866558513853103\n",
            "Loss function = BPR\n",
            "Epoch 202; Train loss 0.057864826172590256; Val loss 0.05925631523132324; Train ROC 0.9870001658634785; Val ROC 0.986830804466075\n",
            "Loss function = BPR\n",
            "Epoch 203; Train loss 0.05774068459868431; Val loss 0.05912787839770317; Train ROC 0.9871342199077072; Val ROC 0.9869819003085536\n",
            "Loss function = BPR\n",
            "Epoch 204; Train loss 0.057617753744125366; Val loss 0.059000689536333084; Train ROC 0.9872909949763815; Val ROC 0.9871011864999841\n",
            "Loss function = BPR\n",
            "Epoch 205; Train loss 0.05749600753188133; Val loss 0.05887472629547119; Train ROC 0.9874500421475003; Val ROC 0.9872284251041766\n",
            "Loss function = BPR\n",
            "Epoch 206; Train loss 0.057375434786081314; Val loss 0.058749984949827194; Train ROC 0.9875875043453959; Val ROC 0.9872999968190349\n",
            "Loss function = BPR\n",
            "Epoch 207; Train loss 0.057256024330854416; Val loss 0.058626435697078705; Train ROC 0.9877113339286242; Val ROC 0.9874431402487515\n",
            "Loss function = BPR\n",
            "Epoch 208; Train loss 0.057137757539749146; Val loss 0.058504074811935425; Train ROC 0.9878692450485207; Val ROC 0.987570378852944\n",
            "Loss function = BPR\n",
            "Epoch 209; Train loss 0.057020626962184906; Val loss 0.058382876217365265; Train ROC 0.9880135235537499; Val ROC 0.9876737602188504\n",
            "Loss function = BPR\n",
            "Epoch 210; Train loss 0.05690460652112961; Val loss 0.05826283618807793; Train ROC 0.9881407612906451; Val ROC 0.9878566657123772\n",
            "Loss function = BPR\n",
            "Epoch 211; Train loss 0.05678968131542206; Val loss 0.05814393237233162; Train ROC 0.9882611827202065; Val ROC 0.9880077615548557\n",
            "Loss function = BPR\n",
            "Epoch 212; Train loss 0.056675851345062256; Val loss 0.05802614986896515; Train ROC 0.9883804680985456; Val ROC 0.9881429525718103\n",
            "Loss function = BPR\n",
            "Epoch 213; Train loss 0.05656309425830841; Val loss 0.05790948122739792; Train ROC 0.9885054337329962; Val ROC 0.9882383815249547\n",
            "Loss function = BPR\n",
            "Epoch 214; Train loss 0.05645139142870903; Val loss 0.057793907821178436; Train ROC 0.9886326714698912; Val ROC 0.9883894773674333\n",
            "Loss function = BPR\n",
            "Epoch 215; Train loss 0.056340742856264114; Val loss 0.057679418474435806; Train ROC 0.9887724057702314; Val ROC 0.9885087635588637\n",
            "Loss function = BPR\n",
            "Epoch 216; Train loss 0.05623112618923187; Val loss 0.05756599083542824; Train ROC 0.9889041877120156; Val ROC 0.9886519069885803\n",
            "Loss function = BPR\n",
            "Epoch 217; Train loss 0.05612252280116081; Val loss 0.05745362490415573; Train ROC 0.989024609141577; Val ROC 0.988834812482107\n",
            "Loss function = BPR\n",
            "Epoch 218; Train loss 0.05601493641734123; Val loss 0.057342302054166794; Train ROC 0.9891359421613601; Val ROC 0.9889779559118237\n",
            "Loss function = BPR\n",
            "Epoch 219; Train loss 0.055908337235450745; Val loss 0.05723200738430023; Train ROC 0.9892450030786988; Val ROC 0.9890574800394439\n",
            "Loss function = BPR\n",
            "Epoch 220; Train loss 0.05580272898077965; Val loss 0.05712272226810455; Train ROC 0.9893506558423706; Val ROC 0.9891926710563985\n",
            "Loss function = BPR\n",
            "Epoch 221; Train loss 0.05569808557629585; Val loss 0.057014454156160355; Train ROC 0.9894415399401528; Val ROC 0.9892721951840189\n",
            "Loss function = BPR\n",
            "Epoch 222; Train loss 0.05559440329670906; Val loss 0.05690716952085495; Train ROC 0.9895687776770479; Val ROC 0.9893835289626873\n",
            "Loss function = BPR\n",
            "Epoch 223; Train loss 0.055491670966148376; Val loss 0.05680086836218834; Train ROC 0.9896994235676099; Val ROC 0.989558482043452\n",
            "Loss function = BPR\n",
            "Epoch 224; Train loss 0.0553898811340332; Val loss 0.05669553205370903; Train ROC 0.9898096205361707; Val ROC 0.9896459585838343\n",
            "Loss function = BPR\n",
            "Epoch 225; Train loss 0.05528900399804115; Val loss 0.056591156870126724; Train ROC 0.989913001197398; Val ROC 0.9897095778859306\n",
            "Loss function = BPR\n",
            "Epoch 226; Train loss 0.05518904700875282; Val loss 0.05648772045969963; Train ROC 0.9900391028830708; Val ROC 0.989805006839075\n",
            "Loss function = BPR\n",
            "Epoch 227; Train loss 0.05508998781442642; Val loss 0.05638522654771805; Train ROC 0.9901311230320753; Val ROC 0.9899640550943156\n",
            "Loss function = BPR\n",
            "Epoch 228; Train loss 0.05499182641506195; Val loss 0.05628364905714989; Train ROC 0.990244728154303; Val ROC 0.990043579221936\n",
            "Loss function = BPR\n",
            "Epoch 229; Train loss 0.054894544184207916; Val loss 0.05618298798799515; Train ROC 0.9903322040984184; Val ROC 0.9901310557623183\n",
            "Loss function = BPR\n",
            "Epoch 230; Train loss 0.05479812994599342; Val loss 0.05608321726322174; Train ROC 0.9904401289645347; Val ROC 0.9902901040175589\n",
            "Loss function = BPR\n",
            "Epoch 231; Train loss 0.054702576249837875; Val loss 0.05598434805870056; Train ROC 0.9905298770110946; Val ROC 0.9903934853834654\n",
            "Loss function = BPR\n",
            "Epoch 232; Train loss 0.054607879370450974; Val loss 0.055886346846818924; Train ROC 0.990625305313766; Val ROC 0.9905286764004199\n",
            "Loss function = BPR\n",
            "Epoch 233; Train loss 0.054514020681381226; Val loss 0.05578922480344772; Train ROC 0.9906946044383249; Val ROC 0.9906559150046124\n",
            "Loss function = BPR\n",
            "Epoch 234; Train loss 0.05442098528146744; Val loss 0.05569295585155487; Train ROC 0.9907729919726621; Val ROC 0.9907752011960429\n",
            "Loss function = BPR\n",
            "Epoch 235; Train loss 0.05432877689599991; Val loss 0.05559753626585007; Train ROC 0.9908638760704442; Val ROC 0.9908944873874733\n",
            "Loss function = BPR\n",
            "Epoch 236; Train loss 0.05423738434910774; Val loss 0.05550295487046242; Train ROC 0.9909581683218933; Val ROC 0.9909501542768075\n",
            "Loss function = BPR\n",
            "Epoch 237; Train loss 0.05414678156375885; Val loss 0.055409207940101624; Train ROC 0.9910570047782314; Val ROC 0.991053535642714\n",
            "Loss function = BPR\n",
            "Epoch 238; Train loss 0.05405697226524353; Val loss 0.055316273123025894; Train ROC 0.9911399365174577; Val ROC 0.9911569170086204\n",
            "Loss function = BPR\n",
            "Epoch 239; Train loss 0.053967952728271484; Val loss 0.05522415414452553; Train ROC 0.9912285485127953; Val ROC 0.9912125838979546\n",
            "Loss function = BPR\n",
            "Epoch 240; Train loss 0.05387970432639122; Val loss 0.05513283237814903; Train ROC 0.9913239768154667; Val ROC 0.991292108025575\n",
            "Loss function = BPR\n",
            "Epoch 241; Train loss 0.05379221960902214; Val loss 0.05504230409860611; Train ROC 0.991411452759582; Val ROC 0.9914511562808156\n",
            "Loss function = BPR\n",
            "Epoch 242; Train loss 0.05370549485087395; Val loss 0.054952554404735565; Train ROC 0.9914909763451415; Val ROC 0.9915386328211979\n",
            "Loss function = BPR\n",
            "Epoch 243; Train loss 0.05361951142549515; Val loss 0.0548635758459568; Train ROC 0.9915773162380346; Val ROC 0.9916738238381525\n",
            "Loss function = BPR\n",
            "Epoch 244; Train loss 0.05353426933288574; Val loss 0.05477537214756012; Train ROC 0.9916625200797053; Val ROC 0.9917772052040589\n",
            "Loss function = BPR\n",
            "Epoch 245; Train loss 0.053449761122465134; Val loss 0.05468791723251343; Train ROC 0.9917409076140425; Val ROC 0.9918646817444413\n",
            "Loss function = BPR\n",
            "Epoch 246; Train loss 0.05336597561836243; Val loss 0.05460121110081673; Train ROC 0.9918181590971574; Val ROC 0.9919998727613958\n",
            "Loss function = BPR\n",
            "Epoch 247; Train loss 0.053282901644706726; Val loss 0.05451524630188942; Train ROC 0.9919033629388282; Val ROC 0.9920873493017781\n",
            "Loss function = BPR\n",
            "Epoch 248; Train loss 0.053200531750917435; Val loss 0.05443000793457031; Train ROC 0.9919919749341658; Val ROC 0.9921509686038744\n",
            "Loss function = BPR\n",
            "Epoch 249; Train loss 0.05311886593699455; Val loss 0.05434549227356911; Train ROC 0.992062410109947; Val ROC 0.9922384451442567\n",
            "Loss function = BPR\n",
            "Epoch 250; Train loss 0.05303788557648659; Val loss 0.054261695593595505; Train ROC 0.9921305731832837; Val ROC 0.992310016859115\n",
            "Loss function = BPR\n",
            "Epoch 251; Train loss 0.05295759066939354; Val loss 0.05417859926819801; Train ROC 0.9922100967688431; Val ROC 0.9923656837484492\n",
            "Loss function = BPR\n",
            "Epoch 252; Train loss 0.052877962589263916; Val loss 0.054096199572086334; Train ROC 0.992295300610514; Val ROC 0.9924531602888317\n",
            "Loss function = BPR\n",
            "Epoch 253; Train loss 0.05279901251196861; Val loss 0.054014500230550766; Train ROC 0.9923509671204055; Val ROC 0.9925088271781659\n",
            "Loss function = BPR\n",
            "Epoch 254; Train loss 0.05272072181105614; Val loss 0.05393347889184952; Train ROC 0.992410041783964; Val ROC 0.9925803988930242\n",
            "Loss function = BPR\n",
            "Epoch 255; Train loss 0.05264308676123619; Val loss 0.05385313183069229; Train ROC 0.992483885113412; Val ROC 0.9926440181951204\n",
            "Loss function = BPR\n",
            "Epoch 256; Train loss 0.05256608873605728; Val loss 0.05377344787120819; Train ROC 0.9925690889550828; Val ROC 0.9926996850844546\n",
            "Loss function = BPR\n",
            "Epoch 257; Train loss 0.05248973146080971; Val loss 0.053694434463977814; Train ROC 0.99264747648942; Val ROC 0.9928110188631231\n",
            "Loss function = BPR\n",
            "Epoch 258; Train loss 0.052414003759622574; Val loss 0.05361606553196907; Train ROC 0.9927076872042007; Val ROC 0.9928825905779813\n",
            "Loss function = BPR\n",
            "Epoch 259; Train loss 0.05233890935778618; Val loss 0.053538352251052856; Train ROC 0.9927815305336487; Val ROC 0.9929382574673156\n",
            "Loss function = BPR\n",
            "Epoch 260; Train loss 0.05226442590355873; Val loss 0.05346127226948738; Train ROC 0.9928610541192081; Val ROC 0.9929859719438878\n",
            "Loss function = BPR\n",
            "Epoch 261; Train loss 0.05219055712223053; Val loss 0.053384821861982346; Train ROC 0.9929269450901003; Val ROC 0.99303368642046\n",
            "Loss function = BPR\n",
            "Epoch 262; Train loss 0.05211728811264038; Val loss 0.05330900102853775; Train ROC 0.9930019244707705; Val ROC 0.9931211629608423\n",
            "Loss function = BPR\n",
            "Epoch 263; Train loss 0.052044618874788284; Val loss 0.0532337985932827; Train ROC 0.9930666793904404; Val ROC 0.9932006870884627\n",
            "Loss function = BPR\n",
            "Epoch 264; Train loss 0.05197254195809364; Val loss 0.0531592071056366; Train ROC 0.9931257540539988; Val ROC 0.9932961160416071\n",
            "Loss function = BPR\n",
            "Epoch 265; Train loss 0.05190104991197586; Val loss 0.053085219115018845; Train ROC 0.9931859647687795; Val ROC 0.9933517829309413\n",
            "Loss function = BPR\n",
            "Epoch 266; Train loss 0.05183013528585434; Val loss 0.05301183462142944; Train ROC 0.9932529917908939; Val ROC 0.9933915449947514\n",
            "Loss function = BPR\n",
            "Epoch 267; Train loss 0.05175980180501938; Val loss 0.0529390387237072; Train ROC 0.993339331683787; Val ROC 0.9934392594713236\n",
            "Loss function = BPR\n",
            "Epoch 268; Train loss 0.05169002711772919; Val loss 0.05286683142185211; Train ROC 0.9933972702961231; Val ROC 0.9934869739478958\n",
            "Loss function = BPR\n",
            "Epoch 269; Train loss 0.051620807498693466; Val loss 0.05279519781470299; Train ROC 0.9934563449596815; Val ROC 0.9935664980755161\n",
            "Loss function = BPR\n",
            "Epoch 270; Train loss 0.051552146673202515; Val loss 0.05272414907813072; Train ROC 0.9935097393671286; Val ROC 0.9936221649648503\n",
            "Loss function = BPR\n",
            "Epoch 271; Train loss 0.051484037190675735; Val loss 0.05265365540981293; Train ROC 0.9935790384916875; Val ROC 0.9936619270286605\n",
            "Loss function = BPR\n",
            "Epoch 272; Train loss 0.05141647160053253; Val loss 0.0525837317109108; Train ROC 0.9936358410528013; Val ROC 0.9937096415052327\n",
            "Loss function = BPR\n",
            "Epoch 273; Train loss 0.051349442452192307; Val loss 0.05251436308026314; Train ROC 0.9936949157163598; Val ROC 0.993773260807329\n",
            "Loss function = BPR\n",
            "Epoch 274; Train loss 0.05128294229507446; Val loss 0.05244554206728935; Train ROC 0.9937471740725846; Val ROC 0.9938289276966632\n",
            "Loss function = BPR\n",
            "Epoch 275; Train loss 0.0512169674038887; Val loss 0.05237726494669914; Train ROC 0.9938085208385876; Val ROC 0.9938766421732353\n",
            "Loss function = BPR\n",
            "Epoch 276; Train loss 0.051151517778635025; Val loss 0.05230953171849251; Train ROC 0.9938573710411455; Val ROC 0.9939402614753317\n",
            "Loss function = BPR\n",
            "Epoch 277; Train loss 0.05108657851815224; Val loss 0.05224232003092766; Train ROC 0.9939243980632598; Val ROC 0.994043642841238\n",
            "Loss function = BPR\n",
            "Epoch 278; Train loss 0.05102215334773064; Val loss 0.05217564478516579; Train ROC 0.9939846087780406; Val ROC 0.9940595476667621\n",
            "Loss function = BPR\n",
            "Epoch 279; Train loss 0.05095822736620903; Val loss 0.0521094873547554; Train ROC 0.9940357310830431; Val ROC 0.9941231669688584\n",
            "Loss function = BPR\n",
            "Epoch 280; Train loss 0.050894804298877716; Val loss 0.0520438477396965; Train ROC 0.9941016220539352; Val ROC 0.9942185959220028\n",
            "Loss function = BPR\n",
            "Epoch 281; Train loss 0.0508318729698658; Val loss 0.05197872221469879; Train ROC 0.9941493362052708; Val ROC 0.9942742628113369\n",
            "Loss function = BPR\n",
            "Epoch 282; Train loss 0.05076943710446358; Val loss 0.05191410332918167; Train ROC 0.9942061387663848; Val ROC 0.9943617393517193\n",
            "Loss function = BPR\n",
            "Epoch 283; Train loss 0.05070747807621956; Val loss 0.05184997618198395; Train ROC 0.99426862158361; Val ROC 0.9944015014155295\n",
            "Loss function = BPR\n",
            "Epoch 284; Train loss 0.05064599588513374; Val loss 0.05178635194897652; Train ROC 0.994329968349613; Val ROC 0.9944969303686738\n",
            "Loss function = BPR\n",
            "Epoch 285; Train loss 0.05058498680591583; Val loss 0.051723212003707886; Train ROC 0.9943754103985041; Val ROC 0.994528740019722\n",
            "Loss function = BPR\n",
            "Epoch 286; Train loss 0.05052445828914642; Val loss 0.05166056379675865; Train ROC 0.9944253966522842; Val ROC 0.9945605496707701\n",
            "Loss function = BPR\n",
            "Epoch 287; Train loss 0.050464384257793427; Val loss 0.05159839615225792; Train ROC 0.9944924236743986; Val ROC 0.9946241689728663\n",
            "Loss function = BPR\n",
            "Epoch 288; Train loss 0.05040477588772774; Val loss 0.05153670534491539; Train ROC 0.9945526343891793; Val ROC 0.9946798358622005\n",
            "Loss function = BPR\n",
            "Epoch 289; Train loss 0.05034561827778816; Val loss 0.05147547647356987; Train ROC 0.9945889880282922; Val ROC 0.9947355027515348\n",
            "Loss function = BPR\n",
            "Epoch 290; Train loss 0.0502869114279747; Val loss 0.05141472443938255; Train ROC 0.994653742947962; Val ROC 0.994783217228107\n",
            "Loss function = BPR\n",
            "Epoch 291; Train loss 0.05022864788770676; Val loss 0.05135442689061165; Train ROC 0.9946912326382972; Val ROC 0.9948229792919172\n",
            "Loss function = BPR\n",
            "Epoch 292; Train loss 0.050170835107564926; Val loss 0.051294587552547455; Train ROC 0.9947321304822991; Val ROC 0.9948468365302032\n",
            "Loss function = BPR\n",
            "Epoch 293; Train loss 0.05011345446109772; Val loss 0.051235198974609375; Train ROC 0.9947843888385239; Val ROC 0.9949025034195375\n",
            "Loss function = BPR\n",
            "Epoch 294; Train loss 0.05005650222301483; Val loss 0.05117626115679741; Train ROC 0.9948264227337482; Val ROC 0.9949184082450615\n",
            "Loss function = BPR\n",
            "Epoch 295; Train loss 0.04999998211860657; Val loss 0.05111776292324066; Train ROC 0.9948616403216388; Val ROC 0.9949502178961097\n",
            "Loss function = BPR\n",
            "Epoch 296; Train loss 0.04994388669729233; Val loss 0.05105970799922943; Train ROC 0.9949048102680854; Val ROC 0.9949899799599199\n",
            "Loss function = BPR\n",
            "Epoch 297; Train loss 0.04988820478320122; Val loss 0.051002081483602524; Train ROC 0.9949547965218656; Val ROC 0.99502974202373\n",
            "Loss function = BPR\n",
            "Epoch 298; Train loss 0.04983294755220413; Val loss 0.050944890826940536; Train ROC 0.9949968304170899; Val ROC 0.9950695040875401\n",
            "Loss function = BPR\n",
            "Epoch 299; Train loss 0.04977809637784958; Val loss 0.050888124853372574; Train ROC 0.9950320480049805; Val ROC 0.9951251709768744\n",
            "Loss function = BPR\n",
            "Epoch 300; Train loss 0.049723654985427856; Val loss 0.05083177611231804; Train ROC 0.9950695376953156; Val ROC 0.9951649330406845\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'train': {'loss': [tensor(1.0883, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(1.0539, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(1.0166, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.9767, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.9345, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.8903, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.8447, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.7982, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.7514, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.7048, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.6591, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.6146, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.5719, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.5314, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.4932, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.4576, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.4246, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.3943, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.3665, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.3413, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.3184, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.2978, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.2792, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.2624, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.2474, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.2339, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.2217, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.2108, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.2010, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1921, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1842, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1770, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1705, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1646, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1592, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1544, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1499, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1459, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1422, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1389, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1358, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1329, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1303, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1279, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1256, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1235, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1216, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1198, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1181, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1166, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1151, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1137, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1124, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1112, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1100, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1089, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1079, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1069, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1060, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1050, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1042, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1033, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1025, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1017, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1010, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.1003, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0996, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0989, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0982, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0975, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0969, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0963, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0957, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0951, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0945, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0939, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0933, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0928, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0922, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0917, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0911, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0906, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0901, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0896, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0891, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0886, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0881, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0876, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0871, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0866, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0862, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0857, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0852, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0848, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0843, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0839, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0834, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0830, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0826, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0822, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0818, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0813, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0809, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0805, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0801, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0797, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0794, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0790, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0786, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0782, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0779, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0775, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0771, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0768, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0764, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0761, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0758, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0754, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0751, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0748, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0744, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0741, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0738, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0735, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0732, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0729, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0726, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0723, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0720, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0717, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0715, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0712, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0709, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0706, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0704, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0701, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0698, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0696, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0693, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0691, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0688, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0686, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0683, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0681, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0678, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0676, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0674, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0672, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0669, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0667, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0665, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0663, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0661, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0658, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0656, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0654, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0652, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0650, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0648, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0646, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0644, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0642, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0640, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0638, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0637, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0635, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0633, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0631, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0629, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0628, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0626, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0624, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0622, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0621, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0619, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0617, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0616, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0614, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0613, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0611, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0609, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0608, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0606, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0605, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0603, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0602, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0600, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0599, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0597, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0596, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0595, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0593, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0592, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0590, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0589, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0588, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0586, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0585, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0584, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0582, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0581, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0580, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0579, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0577, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0576, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0575, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0574, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0573, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0571, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0570, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0569, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0568, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0567, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0566, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0565, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0563, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0562, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0561, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0560, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0559, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0558, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0557, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0556, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0555, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0554, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0553, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0552, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0551, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0550, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0549, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0548, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0547, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0546, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0545, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0544, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0543, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0542, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0541, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0541, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0540, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0539, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0538, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0537, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0536, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0535, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0534, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0534, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0533, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0532, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0531, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0530, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0530, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0529, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0528, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0527, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0526, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0526, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0525, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0524, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0523, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0523, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0522, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0521, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0520, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0520, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0519, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0518, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0518, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0517, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0516, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0516, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0515, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0514, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0513, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0513, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0512, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0512, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0511, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0510, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0510, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0509, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0508, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0508, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0507, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0506, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0506, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0505, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0505, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0504, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0503, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0503, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0502, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0502, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0501, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0501, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0500, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0499, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0499, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0498, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0498, device='cuda:0', grad_fn=<NegBackward0>),\n",
              "   tensor(0.0497, device='cuda:0', grad_fn=<NegBackward0>)],\n",
              "  'roc': [0.022372256720311005,\n",
              "   0.036658100840450694,\n",
              "   0.059082615916986464,\n",
              "   0.09076481240386167,\n",
              "   0.13180920701352583,\n",
              "   0.18226578599975915,\n",
              "   0.24104962044528663,\n",
              "   0.3057999959102156,\n",
              "   0.37555694911172155,\n",
              "   0.4453843374890087,\n",
              "   0.5138132468116723,\n",
              "   0.577924025438459,\n",
              "   0.6359472735906717,\n",
              "   0.6867861338132014,\n",
              "   0.7295584623319497,\n",
              "   0.7660154821060572,\n",
              "   0.7960935742670766,\n",
              "   0.8209696878812872,\n",
              "   0.8414447390603947,\n",
              "   0.8584991400092248,\n",
              "   0.8723192031282306,\n",
              "   0.8837717355000102,\n",
              "   0.8934395314015918,\n",
              "   0.9013259989866423,\n",
              "   0.9079696265345212,\n",
              "   0.9134101758380082,\n",
              "   0.9180077751345652,\n",
              "   0.9219385123636454,\n",
              "   0.9253739312598126,\n",
              "   0.9282401884936188,\n",
              "   0.9307224604142952,\n",
              "   0.9329741139368491,\n",
              "   0.9349644756782793,\n",
              "   0.9366878653824744,\n",
              "   0.9382374392496609,\n",
              "   0.9396154693822835,\n",
              "   0.9408685338804555,\n",
              "   0.9420023130002886,\n",
              "   0.9429884054612254,\n",
              "   0.9438767975170465,\n",
              "   0.9446924822946418,\n",
              "   0.9454706773819018,\n",
              "   0.9462466003667174,\n",
              "   0.9469270950488615,\n",
              "   0.9475780523992265,\n",
              "   0.9481506222152545,\n",
              "   0.9487106954678373,\n",
              "   0.949266224515531,\n",
              "   0.9497660870533331,\n",
              "   0.9501966504665762,\n",
              "   0.9506317580847086,\n",
              "   0.9510475528320621,\n",
              "   0.9514349462988587,\n",
              "   0.9517746256143197,\n",
              "   0.9521733795933391,\n",
              "   0.9525278275746897,\n",
              "   0.9528663708389283,\n",
              "   0.9532083222568339,\n",
              "   0.953524144496627,\n",
              "   0.9538615517096435,\n",
              "   0.9541682855396584,\n",
              "   0.9544545704476723,\n",
              "   0.9547738008411323,\n",
              "   0.9550248681612556,\n",
              "   0.9553122891204918,\n",
              "   0.95561561479684,\n",
              "   0.9558973554999648,\n",
              "   0.9561575112298664,\n",
              "   0.9564256193183238,\n",
              "   0.956659645870113,\n",
              "   0.9569311621122373,\n",
              "   0.9572038144055839,\n",
              "   0.9574810109038197,\n",
              "   0.9577150374556088,\n",
              "   0.9579626966220653,\n",
              "   0.9582046755324104,\n",
              "   0.9584352939305327,\n",
              "   0.9586761367896556,\n",
              "   0.9589147075463339,\n",
              "   0.9591635027640126,\n",
              "   0.9594111619304692,\n",
              "   0.9596474605847028,\n",
              "   0.9598883034438257,\n",
              "   0.9601268742005039,\n",
              "   0.9603790775718496,\n",
              "   0.9606176483285278,\n",
              "   0.9608539469827616,\n",
              "   0.9610857014321061,\n",
              "   0.9613276803424513,\n",
              "   0.9615969244821311,\n",
              "   0.9619013862097014,\n",
              "   0.9621126917370451,\n",
              "   0.9623501264425011,\n",
              "   0.962595513506513,\n",
              "   0.96285339713397,\n",
              "   0.9630669747637581,\n",
              "   0.9633146339302147,\n",
              "   0.9635850141211167,\n",
              "   0.96383608144124,\n",
              "   0.9640882848125857,\n",
              "   0.9643245834668194,\n",
              "   0.964576786838165,\n",
              "   0.9648233099533992,\n",
              "   0.9650573365051883,\n",
              "   0.9653311248497572,\n",
              "   0.9656037771431039,\n",
              "   0.9658718852315613,\n",
              "   0.9661399933200188,\n",
              "   0.9663626593595852,\n",
              "   0.966631903499265,\n",
              "   0.9669034197413893,\n",
              "   0.9671635754712908,\n",
              "   0.9674203230475256,\n",
              "   0.967675934572538,\n",
              "   0.9679167774316608,\n",
              "   0.9681792052640069,\n",
              "   0.9684245923280189,\n",
              "   0.9686892922628095,\n",
              "   0.9689255909170432,\n",
              "   0.9691664337761661,\n",
              "   0.9694129568914003,\n",
              "   0.9696787928774132,\n",
              "   0.9699605335805381,\n",
              "   0.9702161451055505,\n",
              "   0.9704774368866743,\n",
              "   0.9707478170775764,\n",
              "   0.9710113809611448,\n",
              "   0.9712669924861572,\n",
              "   0.9715123795501691,\n",
              "   0.9717759434337375,\n",
              "   0.9720304189075277,\n",
              "   0.9723064793545412,\n",
              "   0.9725552745722199,\n",
              "   0.9727983895337873,\n",
              "   0.973042640546577,\n",
              "   0.9733164288911459,\n",
              "   0.9735811288259365,\n",
              "   0.9738515090168386,\n",
              "   0.9741184810540737,\n",
              "   0.9743525076058629,\n",
              "   0.974622887796765,\n",
              "   0.9748591864509987,\n",
              "   0.9751159340272334,\n",
              "   0.9753897223718023,\n",
              "   0.9756237489235915,\n",
              "   0.9758827686022707,\n",
              "   0.9761065706930594,\n",
              "   0.9763405972448486,\n",
              "   0.9766018890259724,\n",
              "   0.9768529563460957,\n",
              "   0.9771006155125522,\n",
              "   0.9773403223204528,\n",
              "   0.9775879814869093,\n",
              "   0.9778254161923653,\n",
              "   0.9780299054123752,\n",
              "   0.978258251708053,\n",
              "   0.978495686413509,\n",
              "   0.9787035837871858,\n",
              "   0.9789205695706408,\n",
              "   0.9791693647883196,\n",
              "   0.9793659016497736,\n",
              "   0.9795635745624499,\n",
              "   0.9798123697801286,\n",
              "   0.9800100426928049,\n",
              "   0.9802383889884827,\n",
              "   0.9804190211328249,\n",
              "   0.9806291906089462,\n",
              "   0.980857536904624,\n",
              "   0.981054073766078,\n",
              "   0.9812653792934216,\n",
              "   0.9814687324622092,\n",
              "   0.9816789019383306,\n",
              "   0.9818788469534514,\n",
              "   0.9820867443271282,\n",
              "   0.9823014580081386,\n",
              "   0.9824957227671481,\n",
              "   0.982687715423713,\n",
              "   0.9828910685925007,\n",
              "   0.9830864694027325,\n",
              "   0.9832705097007414,\n",
              "   0.9834590942036394,\n",
              "   0.9836351821430924,\n",
              "   0.9838124061337677,\n",
              "   0.9840089429952218,\n",
              "   0.9841986635493422,\n",
              "   0.9843724793863505,\n",
              "   0.9845326626086917,\n",
              "   0.9847042063432556,\n",
              "   0.9848825663851531,\n",
              "   0.9850507019660503,\n",
              "   0.9852392864689483,\n",
              "   0.9854085581010676,\n",
              "   0.9855551087087414,\n",
              "   0.985700523265193,\n",
              "   0.9858572983338673,\n",
              "   0.9860049849927633,\n",
              "   0.9861424471906589,\n",
              "   0.9862946780544442,\n",
              "   0.9864480449694516,\n",
              "   0.986593459525903,\n",
              "   0.9867297856725764,\n",
              "   0.9868627036655828,\n",
              "   0.9870001658634785,\n",
              "   0.9871342199077072,\n",
              "   0.9872909949763815,\n",
              "   0.9874500421475003,\n",
              "   0.9875875043453959,\n",
              "   0.9877113339286242,\n",
              "   0.9878692450485207,\n",
              "   0.9880135235537499,\n",
              "   0.9881407612906451,\n",
              "   0.9882611827202065,\n",
              "   0.9883804680985456,\n",
              "   0.9885054337329962,\n",
              "   0.9886326714698912,\n",
              "   0.9887724057702314,\n",
              "   0.9889041877120156,\n",
              "   0.989024609141577,\n",
              "   0.9891359421613601,\n",
              "   0.9892450030786988,\n",
              "   0.9893506558423706,\n",
              "   0.9894415399401528,\n",
              "   0.9895687776770479,\n",
              "   0.9896994235676099,\n",
              "   0.9898096205361707,\n",
              "   0.989913001197398,\n",
              "   0.9900391028830708,\n",
              "   0.9901311230320753,\n",
              "   0.990244728154303,\n",
              "   0.9903322040984184,\n",
              "   0.9904401289645347,\n",
              "   0.9905298770110946,\n",
              "   0.990625305313766,\n",
              "   0.9906946044383249,\n",
              "   0.9907729919726621,\n",
              "   0.9908638760704442,\n",
              "   0.9909581683218933,\n",
              "   0.9910570047782314,\n",
              "   0.9911399365174577,\n",
              "   0.9912285485127953,\n",
              "   0.9913239768154667,\n",
              "   0.991411452759582,\n",
              "   0.9914909763451415,\n",
              "   0.9915773162380346,\n",
              "   0.9916625200797053,\n",
              "   0.9917409076140425,\n",
              "   0.9918181590971574,\n",
              "   0.9919033629388282,\n",
              "   0.9919919749341658,\n",
              "   0.992062410109947,\n",
              "   0.9921305731832837,\n",
              "   0.9922100967688431,\n",
              "   0.992295300610514,\n",
              "   0.9923509671204055,\n",
              "   0.992410041783964,\n",
              "   0.992483885113412,\n",
              "   0.9925690889550828,\n",
              "   0.99264747648942,\n",
              "   0.9927076872042007,\n",
              "   0.9927815305336487,\n",
              "   0.9928610541192081,\n",
              "   0.9929269450901003,\n",
              "   0.9930019244707705,\n",
              "   0.9930666793904404,\n",
              "   0.9931257540539988,\n",
              "   0.9931859647687795,\n",
              "   0.9932529917908939,\n",
              "   0.993339331683787,\n",
              "   0.9933972702961231,\n",
              "   0.9934563449596815,\n",
              "   0.9935097393671286,\n",
              "   0.9935790384916875,\n",
              "   0.9936358410528013,\n",
              "   0.9936949157163598,\n",
              "   0.9937471740725846,\n",
              "   0.9938085208385876,\n",
              "   0.9938573710411455,\n",
              "   0.9939243980632598,\n",
              "   0.9939846087780406,\n",
              "   0.9940357310830431,\n",
              "   0.9941016220539352,\n",
              "   0.9941493362052708,\n",
              "   0.9942061387663848,\n",
              "   0.99426862158361,\n",
              "   0.994329968349613,\n",
              "   0.9943754103985041,\n",
              "   0.9944253966522842,\n",
              "   0.9944924236743986,\n",
              "   0.9945526343891793,\n",
              "   0.9945889880282922,\n",
              "   0.994653742947962,\n",
              "   0.9946912326382972,\n",
              "   0.9947321304822991,\n",
              "   0.9947843888385239,\n",
              "   0.9948264227337482,\n",
              "   0.9948616403216388,\n",
              "   0.9949048102680854,\n",
              "   0.9949547965218656,\n",
              "   0.9949968304170899,\n",
              "   0.9950320480049805,\n",
              "   0.9950695376953156]},\n",
              " 'val': {'loss': [tensor(1.0724, device='cuda:0'),\n",
              "   tensor(1.0348, device='cuda:0'),\n",
              "   tensor(0.9947, device='cuda:0'),\n",
              "   tensor(0.9521, device='cuda:0'),\n",
              "   tensor(0.9076, device='cuda:0'),\n",
              "   tensor(0.8617, device='cuda:0'),\n",
              "   tensor(0.8148, device='cuda:0'),\n",
              "   tensor(0.7676, device='cuda:0'),\n",
              "   tensor(0.7206, device='cuda:0'),\n",
              "   tensor(0.6743, device='cuda:0'),\n",
              "   tensor(0.6294, device='cuda:0'),\n",
              "   tensor(0.5862, device='cuda:0'),\n",
              "   tensor(0.5451, device='cuda:0'),\n",
              "   tensor(0.5064, device='cuda:0'),\n",
              "   tensor(0.4703, device='cuda:0'),\n",
              "   tensor(0.4368, device='cuda:0'),\n",
              "   tensor(0.4059, device='cuda:0'),\n",
              "   tensor(0.3777, device='cuda:0'),\n",
              "   tensor(0.3520, device='cuda:0'),\n",
              "   tensor(0.3287, device='cuda:0'),\n",
              "   tensor(0.3076, device='cuda:0'),\n",
              "   tensor(0.2886, device='cuda:0'),\n",
              "   tensor(0.2714, device='cuda:0'),\n",
              "   tensor(0.2560, device='cuda:0'),\n",
              "   tensor(0.2421, device='cuda:0'),\n",
              "   tensor(0.2296, device='cuda:0'),\n",
              "   tensor(0.2184, device='cuda:0'),\n",
              "   tensor(0.2083, device='cuda:0'),\n",
              "   tensor(0.1992, device='cuda:0'),\n",
              "   tensor(0.1910, device='cuda:0'),\n",
              "   tensor(0.1835, device='cuda:0'),\n",
              "   tensor(0.1768, device='cuda:0'),\n",
              "   tensor(0.1707, device='cuda:0'),\n",
              "   tensor(0.1652, device='cuda:0'),\n",
              "   tensor(0.1602, device='cuda:0'),\n",
              "   tensor(0.1556, device='cuda:0'),\n",
              "   tensor(0.1514, device='cuda:0'),\n",
              "   tensor(0.1476, device='cuda:0'),\n",
              "   tensor(0.1441, device='cuda:0'),\n",
              "   tensor(0.1409, device='cuda:0'),\n",
              "   tensor(0.1379, device='cuda:0'),\n",
              "   tensor(0.1352, device='cuda:0'),\n",
              "   tensor(0.1327, device='cuda:0'),\n",
              "   tensor(0.1303, device='cuda:0'),\n",
              "   tensor(0.1282, device='cuda:0'),\n",
              "   tensor(0.1262, device='cuda:0'),\n",
              "   tensor(0.1243, device='cuda:0'),\n",
              "   tensor(0.1225, device='cuda:0'),\n",
              "   tensor(0.1209, device='cuda:0'),\n",
              "   tensor(0.1194, device='cuda:0'),\n",
              "   tensor(0.1179, device='cuda:0'),\n",
              "   tensor(0.1166, device='cuda:0'),\n",
              "   tensor(0.1153, device='cuda:0'),\n",
              "   tensor(0.1141, device='cuda:0'),\n",
              "   tensor(0.1129, device='cuda:0'),\n",
              "   tensor(0.1118, device='cuda:0'),\n",
              "   tensor(0.1108, device='cuda:0'),\n",
              "   tensor(0.1098, device='cuda:0'),\n",
              "   tensor(0.1088, device='cuda:0'),\n",
              "   tensor(0.1079, device='cuda:0'),\n",
              "   tensor(0.1070, device='cuda:0'),\n",
              "   tensor(0.1062, device='cuda:0'),\n",
              "   tensor(0.1054, device='cuda:0'),\n",
              "   tensor(0.1046, device='cuda:0'),\n",
              "   tensor(0.1038, device='cuda:0'),\n",
              "   tensor(0.1031, device='cuda:0'),\n",
              "   tensor(0.1023, device='cuda:0'),\n",
              "   tensor(0.1016, device='cuda:0'),\n",
              "   tensor(0.1010, device='cuda:0'),\n",
              "   tensor(0.1003, device='cuda:0'),\n",
              "   tensor(0.0996, device='cuda:0'),\n",
              "   tensor(0.0990, device='cuda:0'),\n",
              "   tensor(0.0984, device='cuda:0'),\n",
              "   tensor(0.0977, device='cuda:0'),\n",
              "   tensor(0.0971, device='cuda:0'),\n",
              "   tensor(0.0965, device='cuda:0'),\n",
              "   tensor(0.0959, device='cuda:0'),\n",
              "   tensor(0.0954, device='cuda:0'),\n",
              "   tensor(0.0948, device='cuda:0'),\n",
              "   tensor(0.0942, device='cuda:0'),\n",
              "   tensor(0.0937, device='cuda:0'),\n",
              "   tensor(0.0931, device='cuda:0'),\n",
              "   tensor(0.0926, device='cuda:0'),\n",
              "   tensor(0.0921, device='cuda:0'),\n",
              "   tensor(0.0915, device='cuda:0'),\n",
              "   tensor(0.0910, device='cuda:0'),\n",
              "   tensor(0.0905, device='cuda:0'),\n",
              "   tensor(0.0900, device='cuda:0'),\n",
              "   tensor(0.0895, device='cuda:0'),\n",
              "   tensor(0.0890, device='cuda:0'),\n",
              "   tensor(0.0885, device='cuda:0'),\n",
              "   tensor(0.0880, device='cuda:0'),\n",
              "   tensor(0.0876, device='cuda:0'),\n",
              "   tensor(0.0871, device='cuda:0'),\n",
              "   tensor(0.0866, device='cuda:0'),\n",
              "   tensor(0.0862, device='cuda:0'),\n",
              "   tensor(0.0857, device='cuda:0'),\n",
              "   tensor(0.0853, device='cuda:0'),\n",
              "   tensor(0.0848, device='cuda:0'),\n",
              "   tensor(0.0844, device='cuda:0'),\n",
              "   tensor(0.0840, device='cuda:0'),\n",
              "   tensor(0.0835, device='cuda:0'),\n",
              "   tensor(0.0831, device='cuda:0'),\n",
              "   tensor(0.0827, device='cuda:0'),\n",
              "   tensor(0.0823, device='cuda:0'),\n",
              "   tensor(0.0819, device='cuda:0'),\n",
              "   tensor(0.0815, device='cuda:0'),\n",
              "   tensor(0.0811, device='cuda:0'),\n",
              "   tensor(0.0807, device='cuda:0'),\n",
              "   tensor(0.0803, device='cuda:0'),\n",
              "   tensor(0.0799, device='cuda:0'),\n",
              "   tensor(0.0796, device='cuda:0'),\n",
              "   tensor(0.0792, device='cuda:0'),\n",
              "   tensor(0.0788, device='cuda:0'),\n",
              "   tensor(0.0785, device='cuda:0'),\n",
              "   tensor(0.0781, device='cuda:0'),\n",
              "   tensor(0.0778, device='cuda:0'),\n",
              "   tensor(0.0774, device='cuda:0'),\n",
              "   tensor(0.0771, device='cuda:0'),\n",
              "   tensor(0.0767, device='cuda:0'),\n",
              "   tensor(0.0764, device='cuda:0'),\n",
              "   tensor(0.0761, device='cuda:0'),\n",
              "   tensor(0.0757, device='cuda:0'),\n",
              "   tensor(0.0754, device='cuda:0'),\n",
              "   tensor(0.0751, device='cuda:0'),\n",
              "   tensor(0.0748, device='cuda:0'),\n",
              "   tensor(0.0745, device='cuda:0'),\n",
              "   tensor(0.0742, device='cuda:0'),\n",
              "   tensor(0.0739, device='cuda:0'),\n",
              "   tensor(0.0736, device='cuda:0'),\n",
              "   tensor(0.0733, device='cuda:0'),\n",
              "   tensor(0.0730, device='cuda:0'),\n",
              "   tensor(0.0727, device='cuda:0'),\n",
              "   tensor(0.0724, device='cuda:0'),\n",
              "   tensor(0.0722, device='cuda:0'),\n",
              "   tensor(0.0719, device='cuda:0'),\n",
              "   tensor(0.0716, device='cuda:0'),\n",
              "   tensor(0.0714, device='cuda:0'),\n",
              "   tensor(0.0711, device='cuda:0'),\n",
              "   tensor(0.0708, device='cuda:0'),\n",
              "   tensor(0.0706, device='cuda:0'),\n",
              "   tensor(0.0703, device='cuda:0'),\n",
              "   tensor(0.0701, device='cuda:0'),\n",
              "   tensor(0.0698, device='cuda:0'),\n",
              "   tensor(0.0696, device='cuda:0'),\n",
              "   tensor(0.0693, device='cuda:0'),\n",
              "   tensor(0.0691, device='cuda:0'),\n",
              "   tensor(0.0689, device='cuda:0'),\n",
              "   tensor(0.0686, device='cuda:0'),\n",
              "   tensor(0.0684, device='cuda:0'),\n",
              "   tensor(0.0682, device='cuda:0'),\n",
              "   tensor(0.0679, device='cuda:0'),\n",
              "   tensor(0.0677, device='cuda:0'),\n",
              "   tensor(0.0675, device='cuda:0'),\n",
              "   tensor(0.0673, device='cuda:0'),\n",
              "   tensor(0.0671, device='cuda:0'),\n",
              "   tensor(0.0669, device='cuda:0'),\n",
              "   tensor(0.0667, device='cuda:0'),\n",
              "   tensor(0.0664, device='cuda:0'),\n",
              "   tensor(0.0662, device='cuda:0'),\n",
              "   tensor(0.0660, device='cuda:0'),\n",
              "   tensor(0.0658, device='cuda:0'),\n",
              "   tensor(0.0656, device='cuda:0'),\n",
              "   tensor(0.0654, device='cuda:0'),\n",
              "   tensor(0.0653, device='cuda:0'),\n",
              "   tensor(0.0651, device='cuda:0'),\n",
              "   tensor(0.0649, device='cuda:0'),\n",
              "   tensor(0.0647, device='cuda:0'),\n",
              "   tensor(0.0645, device='cuda:0'),\n",
              "   tensor(0.0643, device='cuda:0'),\n",
              "   tensor(0.0641, device='cuda:0'),\n",
              "   tensor(0.0640, device='cuda:0'),\n",
              "   tensor(0.0638, device='cuda:0'),\n",
              "   tensor(0.0636, device='cuda:0'),\n",
              "   tensor(0.0634, device='cuda:0'),\n",
              "   tensor(0.0633, device='cuda:0'),\n",
              "   tensor(0.0631, device='cuda:0'),\n",
              "   tensor(0.0629, device='cuda:0'),\n",
              "   tensor(0.0628, device='cuda:0'),\n",
              "   tensor(0.0626, device='cuda:0'),\n",
              "   tensor(0.0624, device='cuda:0'),\n",
              "   tensor(0.0623, device='cuda:0'),\n",
              "   tensor(0.0621, device='cuda:0'),\n",
              "   tensor(0.0620, device='cuda:0'),\n",
              "   tensor(0.0618, device='cuda:0'),\n",
              "   tensor(0.0616, device='cuda:0'),\n",
              "   tensor(0.0615, device='cuda:0'),\n",
              "   tensor(0.0613, device='cuda:0'),\n",
              "   tensor(0.0612, device='cuda:0'),\n",
              "   tensor(0.0610, device='cuda:0'),\n",
              "   tensor(0.0609, device='cuda:0'),\n",
              "   tensor(0.0608, device='cuda:0'),\n",
              "   tensor(0.0606, device='cuda:0'),\n",
              "   tensor(0.0605, device='cuda:0'),\n",
              "   tensor(0.0603, device='cuda:0'),\n",
              "   tensor(0.0602, device='cuda:0'),\n",
              "   tensor(0.0601, device='cuda:0'),\n",
              "   tensor(0.0599, device='cuda:0'),\n",
              "   tensor(0.0598, device='cuda:0'),\n",
              "   tensor(0.0596, device='cuda:0'),\n",
              "   tensor(0.0595, device='cuda:0'),\n",
              "   tensor(0.0594, device='cuda:0'),\n",
              "   tensor(0.0593, device='cuda:0'),\n",
              "   tensor(0.0591, device='cuda:0'),\n",
              "   tensor(0.0590, device='cuda:0'),\n",
              "   tensor(0.0589, device='cuda:0'),\n",
              "   tensor(0.0587, device='cuda:0'),\n",
              "   tensor(0.0586, device='cuda:0'),\n",
              "   tensor(0.0585, device='cuda:0'),\n",
              "   tensor(0.0584, device='cuda:0'),\n",
              "   tensor(0.0583, device='cuda:0'),\n",
              "   tensor(0.0581, device='cuda:0'),\n",
              "   tensor(0.0580, device='cuda:0'),\n",
              "   tensor(0.0579, device='cuda:0'),\n",
              "   tensor(0.0578, device='cuda:0'),\n",
              "   tensor(0.0577, device='cuda:0'),\n",
              "   tensor(0.0576, device='cuda:0'),\n",
              "   tensor(0.0575, device='cuda:0'),\n",
              "   tensor(0.0573, device='cuda:0'),\n",
              "   tensor(0.0572, device='cuda:0'),\n",
              "   tensor(0.0571, device='cuda:0'),\n",
              "   tensor(0.0570, device='cuda:0'),\n",
              "   tensor(0.0569, device='cuda:0'),\n",
              "   tensor(0.0568, device='cuda:0'),\n",
              "   tensor(0.0567, device='cuda:0'),\n",
              "   tensor(0.0566, device='cuda:0'),\n",
              "   tensor(0.0565, device='cuda:0'),\n",
              "   tensor(0.0564, device='cuda:0'),\n",
              "   tensor(0.0563, device='cuda:0'),\n",
              "   tensor(0.0562, device='cuda:0'),\n",
              "   tensor(0.0561, device='cuda:0'),\n",
              "   tensor(0.0560, device='cuda:0'),\n",
              "   tensor(0.0559, device='cuda:0'),\n",
              "   tensor(0.0558, device='cuda:0'),\n",
              "   tensor(0.0557, device='cuda:0'),\n",
              "   tensor(0.0556, device='cuda:0'),\n",
              "   tensor(0.0555, device='cuda:0'),\n",
              "   tensor(0.0554, device='cuda:0'),\n",
              "   tensor(0.0553, device='cuda:0'),\n",
              "   tensor(0.0552, device='cuda:0'),\n",
              "   tensor(0.0551, device='cuda:0'),\n",
              "   tensor(0.0550, device='cuda:0'),\n",
              "   tensor(0.0550, device='cuda:0'),\n",
              "   tensor(0.0549, device='cuda:0'),\n",
              "   tensor(0.0548, device='cuda:0'),\n",
              "   tensor(0.0547, device='cuda:0'),\n",
              "   tensor(0.0546, device='cuda:0'),\n",
              "   tensor(0.0545, device='cuda:0'),\n",
              "   tensor(0.0544, device='cuda:0'),\n",
              "   tensor(0.0543, device='cuda:0'),\n",
              "   tensor(0.0543, device='cuda:0'),\n",
              "   tensor(0.0542, device='cuda:0'),\n",
              "   tensor(0.0541, device='cuda:0'),\n",
              "   tensor(0.0540, device='cuda:0'),\n",
              "   tensor(0.0539, device='cuda:0'),\n",
              "   tensor(0.0539, device='cuda:0'),\n",
              "   tensor(0.0538, device='cuda:0'),\n",
              "   tensor(0.0537, device='cuda:0'),\n",
              "   tensor(0.0536, device='cuda:0'),\n",
              "   tensor(0.0535, device='cuda:0'),\n",
              "   tensor(0.0535, device='cuda:0'),\n",
              "   tensor(0.0534, device='cuda:0'),\n",
              "   tensor(0.0533, device='cuda:0'),\n",
              "   tensor(0.0532, device='cuda:0'),\n",
              "   tensor(0.0532, device='cuda:0'),\n",
              "   tensor(0.0531, device='cuda:0'),\n",
              "   tensor(0.0530, device='cuda:0'),\n",
              "   tensor(0.0529, device='cuda:0'),\n",
              "   tensor(0.0529, device='cuda:0'),\n",
              "   tensor(0.0528, device='cuda:0'),\n",
              "   tensor(0.0527, device='cuda:0'),\n",
              "   tensor(0.0527, device='cuda:0'),\n",
              "   tensor(0.0526, device='cuda:0'),\n",
              "   tensor(0.0525, device='cuda:0'),\n",
              "   tensor(0.0524, device='cuda:0'),\n",
              "   tensor(0.0524, device='cuda:0'),\n",
              "   tensor(0.0523, device='cuda:0'),\n",
              "   tensor(0.0522, device='cuda:0'),\n",
              "   tensor(0.0522, device='cuda:0'),\n",
              "   tensor(0.0521, device='cuda:0'),\n",
              "   tensor(0.0520, device='cuda:0'),\n",
              "   tensor(0.0520, device='cuda:0'),\n",
              "   tensor(0.0519, device='cuda:0'),\n",
              "   tensor(0.0518, device='cuda:0'),\n",
              "   tensor(0.0518, device='cuda:0'),\n",
              "   tensor(0.0517, device='cuda:0'),\n",
              "   tensor(0.0517, device='cuda:0'),\n",
              "   tensor(0.0516, device='cuda:0'),\n",
              "   tensor(0.0515, device='cuda:0'),\n",
              "   tensor(0.0515, device='cuda:0'),\n",
              "   tensor(0.0514, device='cuda:0'),\n",
              "   tensor(0.0514, device='cuda:0'),\n",
              "   tensor(0.0513, device='cuda:0'),\n",
              "   tensor(0.0512, device='cuda:0'),\n",
              "   tensor(0.0512, device='cuda:0'),\n",
              "   tensor(0.0511, device='cuda:0'),\n",
              "   tensor(0.0511, device='cuda:0'),\n",
              "   tensor(0.0510, device='cuda:0'),\n",
              "   tensor(0.0509, device='cuda:0'),\n",
              "   tensor(0.0509, device='cuda:0'),\n",
              "   tensor(0.0508, device='cuda:0')],\n",
              "  'recall': [],\n",
              "  'roc': [0.032270890988325855,\n",
              "   0.052931259344085,\n",
              "   0.0830549988866622,\n",
              "   0.12196615453128479,\n",
              "   0.17008620415434042,\n",
              "   0.22568947418646818,\n",
              "   0.2879091516366065,\n",
              "   0.35473327607596145,\n",
              "   0.42441231669688584,\n",
              "   0.49134777491490916,\n",
              "   0.5540605019562935,\n",
              "   0.6126220695358972,\n",
              "   0.6639628463275757,\n",
              "   0.7082736902376181,\n",
              "   0.7465566052740401,\n",
              "   0.7788434010878901,\n",
              "   0.8051420300919299,\n",
              "   0.8262397811496008,\n",
              "   0.8447529980596112,\n",
              "   0.8601886312307154,\n",
              "   0.8726818716798677,\n",
              "   0.8827496262366001,\n",
              "   0.8912189458281643,\n",
              "   0.898360212488469,\n",
              "   0.9048970957788594,\n",
              "   0.9103604033463752,\n",
              "   0.9151477558291186,\n",
              "   0.9188694850017496,\n",
              "   0.9221697362979928,\n",
              "   0.924690651143557,\n",
              "   0.9273785666571238,\n",
              "   0.9297324808346853,\n",
              "   0.9316967267869072,\n",
              "   0.9335257817221745,\n",
              "   0.9350685497980087,\n",
              "   0.9362455068867894,\n",
              "   0.9375894646435728,\n",
              "   0.9388141362089258,\n",
              "   0.9400388077742787,\n",
              "   0.9409771924801985,\n",
              "   0.941828100645736,\n",
              "   0.9427903425899418,\n",
              "   0.9435299169768108,\n",
              "   0.944341063078538,\n",
              "   0.945080637465407,\n",
              "   0.9455895918821771,\n",
              "   0.9462894042052359,\n",
              "   0.9469574068772466,\n",
              "   0.947553837834399,\n",
              "   0.9481661736170754,\n",
              "   0.9486115087317493,\n",
              "   0.9491761300378535,\n",
              "   0.949541941024907,\n",
              "   0.9499475140757706,\n",
              "   0.9504485160797786,\n",
              "   0.950790469828546,\n",
              "   0.951291471832554,\n",
              "   0.9516254731685594,\n",
              "   0.952023093806661,\n",
              "   0.9523968572064765,\n",
              "   0.952794477844578,\n",
              "   0.9531284791805834,\n",
              "   0.9535101949931609,\n",
              "   0.9537249101377357,\n",
              "   0.9540032445844069,\n",
              "   0.9542259121417438,\n",
              "   0.954536056239463,\n",
              "   0.9548462003371823,\n",
              "   0.9550768203072812,\n",
              "   0.9553710595794764,\n",
              "   0.9557925374558641,\n",
              "   0.9560788243152972,\n",
              "   0.9563810160002545,\n",
              "   0.9566116359703534,\n",
              "   0.9569217800680726,\n",
              "   0.9571683048636956,\n",
              "   0.9574466393103668,\n",
              "   0.9576693068677037,\n",
              "   0.9579158316633266,\n",
              "   0.9581862136972358,\n",
              "   0.9583850240162866,\n",
              "   0.9586633584629577,\n",
              "   0.9589496453223908,\n",
              "   0.9593472659604925,\n",
              "   0.9595778859305913,\n",
              "   0.9598800776155486,\n",
              "   0.9601186499984096,\n",
              "   0.9603810796195565,\n",
              "   0.9605321754620352,\n",
              "   0.9608582243852785,\n",
              "   0.9611604160702357,\n",
              "   0.9614626077551929,\n",
              "   0.9617011801380538,\n",
              "   0.961987466997487,\n",
              "   0.9622737538569202,\n",
              "   0.962480516588733,\n",
              "   0.9628145179247384,\n",
              "   0.9631087571969336,\n",
              "   0.9633632344053186,\n",
              "   0.9636415688519897,\n",
              "   0.963983522600757,\n",
              "   0.9641902853325699,\n",
              "   0.964460667366479,\n",
              "   0.9647628590514362,\n",
              "   0.9650570983236314,\n",
              "   0.9652797658809683,\n",
              "   0.9655024334383052,\n",
              "   0.9658284823615485,\n",
              "   0.9660511499188854,\n",
              "   0.9663056271272704,\n",
              "   0.9665362470973693,\n",
              "   0.9668225339568025,\n",
              "   0.9671088208162356,\n",
              "   0.9673712504373827,\n",
              "   0.9676813945351019,\n",
              "   0.9681028724114896,\n",
              "   0.9683175875560646,\n",
              "   0.9685959220027356,\n",
              "   0.9687788274962623,\n",
              "   0.9690571619429335,\n",
              "   0.9693911632789388,\n",
              "   0.9695740687724655,\n",
              "   0.9697887839170404,\n",
              "   0.9700194038871394,\n",
              "   0.9702341190317142,\n",
              "   0.9704806438273372,\n",
              "   0.9707192162101982,\n",
              "   0.9710452651334415,\n",
              "   0.9713872188822089,\n",
              "   0.971657600916118,\n",
              "   0.9718723160606928,\n",
              "   0.9721188408563158,\n",
              "   0.9724289849540351,\n",
              "   0.9727391290517543,\n",
              "   0.9730015586729014,\n",
              "   0.9732083214047141,\n",
              "   0.9734389413748131,\n",
              "   0.9737252282342462,\n",
              "   0.973908133727773,\n",
              "   0.974138753697872,\n",
              "   0.9743455164296848,\n",
              "   0.9745284219232115,\n",
              "   0.9747908515443585,\n",
              "   0.9750055666889335,\n",
              "   0.9752361866590323,\n",
              "   0.9754668066291313,\n",
              "   0.9757212838375163,\n",
              "   0.9758803320927569,\n",
              "   0.9761904761904762,\n",
              "   0.9764449533988612,\n",
              "   0.9767550974965805,\n",
              "   0.9769698126411553,\n",
              "   0.977136813309158,\n",
              "   0.9773276712154467,\n",
              "   0.9775741960110698,\n",
              "   0.9778127683939307,\n",
              "   0.9780195311257436,\n",
              "   0.9782978655724147,\n",
              "   0.9786000572573719,\n",
              "   0.9788306772274709,\n",
              "   0.9790772020230938,\n",
              "   0.979363488882527,\n",
              "   0.9795861564398639,\n",
              "   0.9798088239972007,\n",
              "   0.9800076343162516,\n",
              "   0.9802223494608264,\n",
              "   0.9804211597798772,\n",
              "   0.980635874924452,\n",
              "   0.980874447307313,\n",
              "   0.981113019690174,\n",
              "   0.9813197824219868,\n",
              "   0.9815344975665617,\n",
              "   0.9817730699494227,\n",
              "   0.9819321182046633,\n",
              "   0.98211502369819,\n",
              "   0.9822740719534306,\n",
              "   0.9824808346852435,\n",
              "   0.9827671215446766,\n",
              "   0.9829261697999173,\n",
              "   0.983077265642396,\n",
              "   0.9832522187231606,\n",
              "   0.9834669338677354,\n",
              "   0.9836180297102141,\n",
              "   0.9837770779654548,\n",
              "   0.9839361262206954,\n",
              "   0.9840554124121258,\n",
              "   0.9841985558418425,\n",
              "   0.9843576040970831,\n",
              "   0.9845166523523237,\n",
              "   0.9847075102586125,\n",
              "   0.9849619874669975,\n",
              "   0.9851369405477622,\n",
              "   0.9852880363902408,\n",
              "   0.9854073225816713,\n",
              "   0.985590228075198,\n",
              "   0.9857572287432007,\n",
              "   0.9859003721729173,\n",
              "   0.9860196583643478,\n",
              "   0.9861946114451124,\n",
              "   0.986353659700353,\n",
              "   0.9865365651938798,\n",
              "   0.9866558513853103,\n",
              "   0.986830804466075,\n",
              "   0.9869819003085536,\n",
              "   0.9871011864999841,\n",
              "   0.9872284251041766,\n",
              "   0.9872999968190349,\n",
              "   0.9874431402487515,\n",
              "   0.987570378852944,\n",
              "   0.9876737602188504,\n",
              "   0.9878566657123772,\n",
              "   0.9880077615548557,\n",
              "   0.9881429525718103,\n",
              "   0.9882383815249547,\n",
              "   0.9883894773674333,\n",
              "   0.9885087635588637,\n",
              "   0.9886519069885803,\n",
              "   0.988834812482107,\n",
              "   0.9889779559118237,\n",
              "   0.9890574800394439,\n",
              "   0.9891926710563985,\n",
              "   0.9892721951840189,\n",
              "   0.9893835289626873,\n",
              "   0.989558482043452,\n",
              "   0.9896459585838343,\n",
              "   0.9897095778859306,\n",
              "   0.989805006839075,\n",
              "   0.9899640550943156,\n",
              "   0.990043579221936,\n",
              "   0.9901310557623183,\n",
              "   0.9902901040175589,\n",
              "   0.9903934853834654,\n",
              "   0.9905286764004199,\n",
              "   0.9906559150046124,\n",
              "   0.9907752011960429,\n",
              "   0.9908944873874733,\n",
              "   0.9909501542768075,\n",
              "   0.991053535642714,\n",
              "   0.9911569170086204,\n",
              "   0.9912125838979546,\n",
              "   0.991292108025575,\n",
              "   0.9914511562808156,\n",
              "   0.9915386328211979,\n",
              "   0.9916738238381525,\n",
              "   0.9917772052040589,\n",
              "   0.9918646817444413,\n",
              "   0.9919998727613958,\n",
              "   0.9920873493017781,\n",
              "   0.9921509686038744,\n",
              "   0.9922384451442567,\n",
              "   0.992310016859115,\n",
              "   0.9923656837484492,\n",
              "   0.9924531602888317,\n",
              "   0.9925088271781659,\n",
              "   0.9925803988930242,\n",
              "   0.9926440181951204,\n",
              "   0.9926996850844546,\n",
              "   0.9928110188631231,\n",
              "   0.9928825905779813,\n",
              "   0.9929382574673156,\n",
              "   0.9929859719438878,\n",
              "   0.99303368642046,\n",
              "   0.9931211629608423,\n",
              "   0.9932006870884627,\n",
              "   0.9932961160416071,\n",
              "   0.9933517829309413,\n",
              "   0.9933915449947514,\n",
              "   0.9934392594713236,\n",
              "   0.9934869739478958,\n",
              "   0.9935664980755161,\n",
              "   0.9936221649648503,\n",
              "   0.9936619270286605,\n",
              "   0.9937096415052327,\n",
              "   0.993773260807329,\n",
              "   0.9938289276966632,\n",
              "   0.9938766421732353,\n",
              "   0.9939402614753317,\n",
              "   0.994043642841238,\n",
              "   0.9940595476667621,\n",
              "   0.9941231669688584,\n",
              "   0.9942185959220028,\n",
              "   0.9942742628113369,\n",
              "   0.9943617393517193,\n",
              "   0.9944015014155295,\n",
              "   0.9944969303686738,\n",
              "   0.994528740019722,\n",
              "   0.9945605496707701,\n",
              "   0.9946241689728663,\n",
              "   0.9946798358622005,\n",
              "   0.9947355027515348,\n",
              "   0.994783217228107,\n",
              "   0.9948229792919172,\n",
              "   0.9948468365302032,\n",
              "   0.9949025034195375,\n",
              "   0.9949184082450615,\n",
              "   0.9949502178961097,\n",
              "   0.9949899799599199,\n",
              "   0.99502974202373,\n",
              "   0.9950695040875401,\n",
              "   0.9951251709768744,\n",
              "   0.9951649330406845]}}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "train(datasets, model, optimizer, \"BPR\", training_args, neg_samp = \"random\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5puYOjJ-6k7y"
      },
      "source": [
        "### Call test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hP2YwDZk6k7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6727db28-33fc-45a5-ce0e-0118dd82c9ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss function = BPR\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.0450, device='cuda:0'),\n",
              " 0.9958886026020295,\n",
              " tensor([[ 9057., 12216., 10780.,  ..., 10976.,  5590.,  2637.],\n",
              "         [10853., 13871., 13273.,  ..., 12051., 13797., 13941.]],\n",
              "        device='cuda:0'),\n",
              " tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "test(model, datasets['test'], \"BPR\", neg_samp = \"random\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiNyC-Da6k7z"
      },
      "source": [
        "### Pipeline Simplification\n",
        "\n",
        "Instead of having to call everything one by one, let's package this training process into a pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmEcP_hZ6k7z"
      },
      "outputs": [],
      "source": [
        "def init_model(conv_layer, args, alpha = False):\n",
        "  num_nodes = n_playlists + n_tracks\n",
        "  model = GCN(num_nodes = num_nodes,\n",
        "            embedding_dim=args['emb_size'],\n",
        "            num_layers=args['num_layers'],\n",
        "            alpha = alpha,\n",
        "            conv_layer = conv_layer,\n",
        "            device=args['device'])\n",
        "  model.cuda()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
        "  return model, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJv0wcsy6k7z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77c3c929-8c31-4811-a843-2390e2aea58d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beginning training for LGCN_LGC_4_e64_nodes15720_\n",
            "Loss function = BPR\n",
            "Epoch 0; Train loss 2.5462191104888916; Val loss 2.527089834213257; Train ROC 0.0016893081675266575; Val ROC 0.001677959092788752\n",
            "Loss function = BPR\n",
            "Epoch 1; Train loss 2.4487617015838623; Val loss 2.4185843467712402; Train ROC 0.0028901143094739856; Val ROC 0.003872825015109584\n",
            "Loss function = BPR\n",
            "Epoch 2; Train loss 2.3411779403686523; Val loss 2.2999908924102783; Train ROC 0.005563242835492967; Val ROC 0.008851035404141616\n",
            "Loss function = BPR\n",
            "Epoch 3; Train loss 2.2236578464508057; Val loss 2.171929121017456; Train ROC 0.010990159524312633; Val ROC 0.01748735566370837\n",
            "Loss function = BPR\n",
            "Epoch 4; Train loss 2.096874475479126; Val loss 2.035595417022705; Train ROC 0.02129868831525876; Val ROC 0.03291503642205045\n",
            "Loss function = BPR\n",
            "Epoch 5; Train loss 1.9620866775512695; Val loss 1.892789602279663; Train ROC 0.03854280981820908; Val ROC 0.05722556223558228\n",
            "Loss function = BPR\n",
            "Epoch 6; Train loss 1.8211517333984375; Val loss 1.74583899974823; Train ROC 0.06480490592359828; Val ROC 0.0908563158062156\n",
            "Loss function = BPR\n",
            "Epoch 7; Train loss 1.6764349937438965; Val loss 1.597434639930725; Train ROC 0.10097109658480281; Val ROC 0.13309157998536755\n",
            "Loss function = BPR\n",
            "Epoch 8; Train loss 1.5306355953216553; Val loss 1.4504040479660034; Train ROC 0.1472470070730549; Val ROC 0.1847663581130515\n",
            "Loss function = BPR\n",
            "Epoch 9; Train loss 1.3865597248077393; Val loss 1.3074778318405151; Train ROC 0.20193992106716108; Val ROC 0.24297801953112574\n",
            "Loss function = BPR\n",
            "Epoch 10; Train loss 1.2468940019607544; Val loss 1.1710933446884155; Train ROC 0.2632764626091461; Val ROC 0.30540445971307695\n",
            "Loss function = BPR\n",
            "Epoch 11; Train loss 1.1140156984329224; Val loss 1.043252944946289; Train ROC 0.3301228525791771; Val ROC 0.37212520278652544\n",
            "Loss function = BPR\n",
            "Epoch 12; Train loss 0.9898579716682434; Val loss 0.925438404083252; Train ROC 0.39954637474694454; Val ROC 0.44039666634857016\n",
            "Loss function = BPR\n",
            "Epoch 13; Train loss 0.875826895236969; Val loss 0.8185821175575256; Train ROC 0.4691300801370532; Val ROC 0.5075468397111683\n",
            "Loss function = BPR\n",
            "Epoch 14; Train loss 0.7727720737457275; Val loss 0.7230814099311829; Train ROC 0.5366910463258967; Val ROC 0.5717864300028629\n",
            "Loss function = BPR\n",
            "Epoch 15; Train loss 0.6810058951377869; Val loss 0.63885098695755; Train ROC 0.600120194219317; Val ROC 0.630475236186659\n",
            "Loss function = BPR\n",
            "Epoch 16; Train loss 0.6003624200820923; Val loss 0.5654052495956421; Train ROC 0.6573067406463222; Val ROC 0.6827703025097814\n",
            "Loss function = BPR\n",
            "Epoch 17; Train loss 0.5302901864051819; Val loss 0.5019615888595581; Train ROC 0.7070498794649653; Val ROC 0.7275344339472596\n",
            "Loss function = BPR\n",
            "Epoch 18; Train loss 0.4699612259864807; Val loss 0.4475502371788025; Train ROC 0.749439358721806; Val ROC 0.7638371982059357\n",
            "Loss function = BPR\n",
            "Epoch 19; Train loss 0.41838279366493225; Val loss 0.40111613273620605; Train ROC 0.7846933002515217; Val ROC 0.7941756528930878\n",
            "Loss function = BPR\n",
            "Epoch 20; Train loss 0.37449565529823303; Val loss 0.3616020381450653; Train ROC 0.8130854923986812; Val ROC 0.8188201800426249\n",
            "Loss function = BPR\n",
            "Epoch 21; Train loss 0.33725205063819885; Val loss 0.3280075490474701; Train ROC 0.8360825772912449; Val ROC 0.8390511181092344\n",
            "Loss function = BPR\n",
            "Epoch 22; Train loss 0.30566954612731934; Val loss 0.2994249165058136; Train ROC 0.8547410825659307; Val ROC 0.8550116105226325\n",
            "Loss function = BPR\n",
            "Epoch 23; Train loss 0.27886271476745605; Val loss 0.2750551998615265; Train ROC 0.8695597347093186; Val ROC 0.8681569488182714\n",
            "Loss function = BPR\n",
            "Epoch 24; Train loss 0.25605639815330505; Val loss 0.25421085953712463; Train ROC 0.8814814562358988; Val ROC 0.8790756115405414\n",
            "Loss function = BPR\n",
            "Epoch 25; Train loss 0.23658667504787445; Val loss 0.23631051182746887; Train ROC 0.8911344834715907; Val ROC 0.8879584565957311\n",
            "Loss function = BPR\n",
            "Epoch 26; Train loss 0.2198944240808487; Val loss 0.2208675891160965; Train ROC 0.8991845424326492; Val ROC 0.8956166300855679\n",
            "Loss function = BPR\n",
            "Epoch 27; Train loss 0.2055135816335678; Val loss 0.20747850835323334; Train ROC 0.9056997961924107; Val ROC 0.902097846486624\n",
            "Loss function = BPR\n",
            "Epoch 28; Train loss 0.1930590718984604; Val loss 0.19581012427806854; Train ROC 0.9111960120057894; Val ROC 0.9074021057988993\n",
            "Loss function = BPR\n",
            "Epoch 29; Train loss 0.18221405148506165; Val loss 0.18558818101882935; Train ROC 0.9157277203314543; Val ROC 0.9115771224989662\n",
            "Loss function = BPR\n",
            "Epoch 30; Train loss 0.17271873354911804; Val loss 0.17658720910549164; Train ROC 0.9195062266967493; Val ROC 0.9151238985908324\n",
            "Loss function = BPR\n",
            "Epoch 31; Train loss 0.1643601953983307; Val loss 0.16862158477306366; Train ROC 0.9228155439072437; Val ROC 0.9181776250914527\n",
            "Loss function = BPR\n",
            "Epoch 32; Train loss 0.15696382522583008; Val loss 0.16153843700885773; Train ROC 0.9256886174483835; Val ROC 0.9210245888602602\n",
            "Loss function = BPR\n",
            "Epoch 33; Train loss 0.15038615465164185; Val loss 0.1552114635705948; Train ROC 0.9281811138300604; Val ROC 0.9235693609441105\n",
            "Loss function = BPR\n",
            "Epoch 34; Train loss 0.14450910687446594; Val loss 0.1495359241962433; Train ROC 0.9303952776622793; Val ROC 0.92595508477272\n",
            "Loss function = BPR\n",
            "Epoch 35; Train loss 0.1392350196838379; Val loss 0.14442478120326996; Train ROC 0.9323901836085986; Val ROC 0.9279829500270382\n",
            "Loss function = BPR\n",
            "Epoch 36; Train loss 0.13448293507099152; Val loss 0.13980522751808167; Train ROC 0.9341601514129069; Val ROC 0.9299551483920221\n",
            "Loss function = BPR\n",
            "Epoch 37; Train loss 0.13018538057804108; Val loss 0.13561618328094482; Train ROC 0.9357335823557612; Val ROC 0.9315138212933805\n",
            "Loss function = BPR\n",
            "Epoch 38; Train loss 0.1262858659029007; Val loss 0.13180607557296753; Train ROC 0.937185455817832; Val ROC 0.9330963514330248\n",
            "Loss function = BPR\n",
            "Epoch 39; Train loss 0.12273675948381424; Val loss 0.12833121418952942; Train ROC 0.9385044112868961; Val ROC 0.9344721188408563\n",
            "Loss function = BPR\n",
            "Epoch 40; Train loss 0.11949783563613892; Val loss 0.12515421211719513; Train ROC 0.9397336187094004; Val ROC 0.9357604097083055\n",
            "Loss function = BPR\n",
            "Epoch 41; Train loss 0.11653473228216171; Val loss 0.12224305421113968; Train ROC 0.9408639896755665; Val ROC 0.936865795082228\n",
            "Loss function = BPR\n",
            "Epoch 42; Train loss 0.11381801962852478; Val loss 0.11957007646560669; Train ROC 0.941905748646395; Val ROC 0.9379950376944365\n",
            "Loss function = BPR\n",
            "Epoch 43; Train loss 0.11132238805294037; Val loss 0.1171112060546875; Train ROC 0.9429316029001116; Val ROC 0.9389731844641664\n",
            "Loss function = BPR\n",
            "Epoch 44; Train loss 0.1090257316827774; Val loss 0.11484541743993759; Train ROC 0.9438643009536014; Val ROC 0.939863854693514\n",
            "Loss function = BPR\n",
            "Epoch 45; Train loss 0.10690880566835403; Val loss 0.112754225730896; Train ROC 0.9447595093167561; Val ROC 0.9407783821611477\n",
            "Loss function = BPR\n",
            "Epoch 46; Train loss 0.10495467483997345; Val loss 0.11082123965024948; Train ROC 0.9456115477334642; Val ROC 0.9416372427394472\n",
            "Loss function = BPR\n",
            "Epoch 47; Train loss 0.10314830392599106; Val loss 0.10903184860944748; Train ROC 0.9463670217962787; Val ROC 0.9424324840156504\n",
            "Loss function = BPR\n",
            "Epoch 48; Train loss 0.10147629678249359; Val loss 0.10737299174070358; Train ROC 0.9471054550907592; Val ROC 0.9431402487514712\n",
            "Loss function = BPR\n",
            "Epoch 49; Train loss 0.0999266654253006; Val loss 0.10583294183015823; Train ROC 0.9477280111605673; Val ROC 0.9439195852021504\n",
            "Loss function = BPR\n",
            "Epoch 50; Train loss 0.09848862886428833; Val loss 0.10440100729465485; Train ROC 0.948394873228044; Val ROC 0.9445478258103509\n",
            "Loss function = BPR\n",
            "Epoch 51; Train loss 0.09715232998132706; Val loss 0.10306757688522339; Train ROC 0.9490151571954076; Val ROC 0.9452476381334097\n",
            "Loss function = BPR\n",
            "Epoch 52; Train loss 0.09590889513492584; Val loss 0.10182380676269531; Train ROC 0.949605903830992; Val ROC 0.9458997359798963\n",
            "Loss function = BPR\n",
            "Epoch 53; Train loss 0.09475015848875046; Val loss 0.10066163539886475; Train ROC 0.9501637049811302; Val ROC 0.9463927855711423\n",
            "Loss function = BPR\n",
            "Epoch 54; Train loss 0.0936686247587204; Val loss 0.09957370907068253; Train ROC 0.9507578597703813; Val ROC 0.9468937875751503\n",
            "Loss function = BPR\n",
            "Epoch 55; Train loss 0.09265746921300888; Val loss 0.09855317324399948; Train ROC 0.9512758991277399; Val ROC 0.9474981709450647\n",
            "Loss function = BPR\n",
            "Epoch 56; Train loss 0.09171033650636673; Val loss 0.09759385138750076; Train ROC 0.9517541766923187; Val ROC 0.9479594108852626\n",
            "Loss function = BPR\n",
            "Epoch 57; Train loss 0.09082140028476715; Val loss 0.09668992459774017; Train ROC 0.9522472229227872; Val ROC 0.9484047459999364\n",
            "Loss function = BPR\n",
            "Epoch 58; Train loss 0.08998526632785797; Val loss 0.09583612531423569; Train ROC 0.9526834665921418; Val ROC 0.9489375576549925\n",
            "Loss function = BPR\n",
            "Epoch 59; Train loss 0.08919695019721985; Val loss 0.09502754360437393; Train ROC 0.9531038055443843; Val ROC 0.9493908451824283\n",
            "Loss function = BPR\n",
            "Epoch 60; Train loss 0.08845186233520508; Val loss 0.09425967931747437; Train ROC 0.9535684504942958; Val ROC 0.9497805134077679\n",
            "Loss function = BPR\n",
            "Epoch 61; Train loss 0.08774574846029282; Val loss 0.0935283899307251; Train ROC 0.9539467555513143; Val ROC 0.9502576581734898\n",
            "Loss function = BPR\n",
            "Epoch 62; Train loss 0.08707468211650848; Val loss 0.0928298681974411; Train ROC 0.9543273327107773; Val ROC 0.9507109457009256\n",
            "Loss function = BPR\n",
            "Epoch 63; Train loss 0.08643506467342377; Val loss 0.09216060489416122; Train ROC 0.9546681480774605; Val ROC 0.9511721856411235\n",
            "Loss function = BPR\n",
            "Epoch 64; Train loss 0.08582353591918945; Val loss 0.0915173813700676; Train ROC 0.9550373647247007; Val ROC 0.9515698062792252\n",
            "Loss function = BPR\n",
            "Epoch 65; Train loss 0.08523707091808319; Val loss 0.09089726954698563; Train ROC 0.9554009011158295; Val ROC 0.9519117600279925\n",
            "Loss function = BPR\n",
            "Epoch 66; Train loss 0.08467284590005875; Val loss 0.09029759466648102; Train ROC 0.9557371722776237; Val ROC 0.952309380666094\n",
            "Loss function = BPR\n",
            "Epoch 67; Train loss 0.08412829786539078; Val loss 0.08971590548753738; Train ROC 0.9560791236955292; Val ROC 0.9525877151127652\n",
            "Loss function = BPR\n",
            "Epoch 68; Train loss 0.08360110223293304; Val loss 0.08914999663829803; Train ROC 0.9563756330645437; Val ROC 0.9529217164487706\n",
            "Loss function = BPR\n",
            "Epoch 69; Train loss 0.08308911323547363; Val loss 0.088597871363163; Train ROC 0.9566641900750021; Val ROC 0.9532000508954417\n",
            "Loss function = BPR\n",
            "Epoch 70; Train loss 0.08259040117263794; Val loss 0.088057741522789; Train ROC 0.9569686518025725; Val ROC 0.9535817667080192\n",
            "Loss function = BPR\n",
            "Epoch 71; Train loss 0.08210323750972748; Val loss 0.08752794563770294; Train ROC 0.9572856100935879; Val ROC 0.9540032445844069\n",
            "Loss function = BPR\n",
            "Epoch 72; Train loss 0.0816260352730751; Val loss 0.0870070531964302; Train ROC 0.9575673507967127; Val ROC 0.9542974838566021\n",
            "Loss function = BPR\n",
            "Epoch 73; Train loss 0.08115736395120621; Val loss 0.08649377524852753; Train ROC 0.9578627241145049; Val ROC 0.9545996755415593\n",
            "Loss function = BPR\n",
            "Epoch 74; Train loss 0.08069602400064468; Val loss 0.08598697185516357; Train ROC 0.9581115193321836; Val ROC 0.9548700575754684\n",
            "Loss function = BPR\n",
            "Epoch 75; Train loss 0.08024083822965622; Val loss 0.08548563718795776; Train ROC 0.9583716750620852; Val ROC 0.9552199637369978\n",
            "Loss function = BPR\n",
            "Epoch 76; Train loss 0.079790860414505; Val loss 0.08498890697956085; Train ROC 0.9586704565335442; Val ROC 0.9554823933581449\n",
            "Loss function = BPR\n",
            "Epoch 77; Train loss 0.0793452039361; Val loss 0.08449600636959076; Train ROC 0.9589567414415581; Val ROC 0.9557209657410058\n",
            "Loss function = BPR\n",
            "Epoch 78; Train loss 0.07890313863754272; Val loss 0.08400628715753555; Train ROC 0.9592146250690151; Val ROC 0.9560788243152972\n",
            "Loss function = BPR\n",
            "Epoch 79; Train loss 0.07846400886774063; Val loss 0.08351922035217285; Train ROC 0.9594815971062504; Val ROC 0.9564207780640647\n",
            "Loss function = BPR\n",
            "Epoch 80; Train loss 0.07802727073431015; Val loss 0.08303431421518326; Train ROC 0.9597553854508192; Val ROC 0.9566275407958774\n",
            "Loss function = BPR\n",
            "Epoch 81; Train loss 0.07759243994951248; Val loss 0.08255121111869812; Train ROC 0.9599814596440525; Val ROC 0.9568263511149283\n",
            "Loss function = BPR\n",
            "Epoch 82; Train loss 0.07715915143489838; Val loss 0.08206962794065475; Train ROC 0.9602404793227317; Val ROC 0.9571126379743614\n",
            "Loss function = BPR\n",
            "Epoch 83; Train loss 0.07672710716724396; Val loss 0.08158931136131287; Train ROC 0.9604779140281877; Val ROC 0.9574307344848427\n",
            "Loss function = BPR\n",
            "Epoch 84; Train loss 0.07629604637622833; Val loss 0.08111010491847992; Train ROC 0.9607346616044224; Val ROC 0.9576454496294176\n",
            "Loss function = BPR\n",
            "Epoch 85; Train loss 0.07586581259965897; Val loss 0.08063187450170517; Train ROC 0.9609857289245457; Val ROC 0.9579396889016127\n",
            "Loss function = BPR\n",
            "Epoch 86; Train loss 0.07543624192476273; Val loss 0.08015458285808563; Train ROC 0.9612402043983359; Val ROC 0.9582180233482839\n",
            "Loss function = BPR\n",
            "Epoch 87; Train loss 0.07500731199979782; Val loss 0.07967821508646011; Train ROC 0.9614924077696815; Val ROC 0.9584725005566689\n",
            "Loss function = BPR\n",
            "Epoch 88; Train loss 0.07457895576953888; Val loss 0.07920277863740921; Train ROC 0.9617184819629148; Val ROC 0.9587031205267679\n",
            "Loss function = BPR\n",
            "Epoch 89; Train loss 0.0741511881351471; Val loss 0.0787283331155777; Train ROC 0.9619468282585926; Val ROC 0.9589814549734389\n",
            "Loss function = BPR\n",
            "Epoch 90; Train loss 0.07372406125068665; Val loss 0.07825496792793274; Train ROC 0.9622047118860495; Val ROC 0.9593234087222063\n",
            "Loss function = BPR\n",
            "Epoch 91; Train loss 0.0732976421713829; Val loss 0.07778280228376389; Train ROC 0.9624478268476169; Val ROC 0.9596415052326877\n",
            "Loss function = BPR\n",
            "Epoch 92; Train loss 0.07287202775478363; Val loss 0.07731197029352188; Train ROC 0.9627159349360744; Val ROC 0.9598800776155486\n",
            "Loss function = BPR\n",
            "Epoch 93; Train loss 0.07244735211133957; Val loss 0.07684262096881866; Train ROC 0.9630249408685339; Val ROC 0.9602061265387919\n",
            "Loss function = BPR\n",
            "Epoch 94; Train loss 0.07202374190092087; Val loss 0.07637492567300797; Train ROC 0.9632828244959909; Val ROC 0.9604367465088908\n",
            "Loss function = BPR\n",
            "Epoch 95; Train loss 0.07160136103630066; Val loss 0.07590906322002411; Train ROC 0.9635509325844483; Val ROC 0.9607150809555619\n",
            "Loss function = BPR\n",
            "Epoch 96; Train loss 0.07118037343025208; Val loss 0.07544520497322083; Train ROC 0.9638019999045717; Val ROC 0.9610013678149951\n",
            "Loss function = BPR\n",
            "Epoch 97; Train loss 0.07076094299554825; Val loss 0.07498355954885483; Train ROC 0.9640678358905846; Val ROC 0.9612717498489042\n",
            "Loss function = BPR\n",
            "Epoch 98; Train loss 0.07034327834844589; Val loss 0.07452433556318283; Train ROC 0.9643029984935961; Val ROC 0.961502369819003\n",
            "Loss function = BPR\n",
            "Epoch 99; Train loss 0.06992755830287933; Val loss 0.07406769692897797; Train ROC 0.9645631542234976; Val ROC 0.9617886566784363\n",
            "Loss function = BPR\n",
            "Epoch 100; Train loss 0.0695139542222023; Val loss 0.07361386716365814; Train ROC 0.9648289902095105; Val ROC 0.9620192766485351\n",
            "Loss function = BPR\n",
            "Epoch 101; Train loss 0.0691026821732521; Val loss 0.07316303253173828; Train ROC 0.9650505201978546; Val ROC 0.9622658014441582\n",
            "Loss function = BPR\n",
            "Epoch 102; Train loss 0.06869390606880188; Val loss 0.07271537184715271; Train ROC 0.9653061317228672; Val ROC 0.9625520883035913\n",
            "Loss function = BPR\n",
            "Epoch 103; Train loss 0.06828782707452774; Val loss 0.07227107137441635; Train ROC 0.9655742398113246; Val ROC 0.9628463275757865\n",
            "Loss function = BPR\n",
            "Epoch 104; Train loss 0.06788461655378342; Val loss 0.07183032482862473; Train ROC 0.9658639328730054; Val ROC 0.9631087571969336\n",
            "Loss function = BPR\n",
            "Epoch 105; Train loss 0.06748443841934204; Val loss 0.07139325886964798; Train ROC 0.9661104559882396; Val ROC 0.9632837102776982\n",
            "Loss function = BPR\n",
            "Epoch 106; Train loss 0.06708747893571854; Val loss 0.07096005976200104; Train ROC 0.9663626593595852; Val ROC 0.9636097592009416\n",
            "Loss function = BPR\n",
            "Epoch 107; Train loss 0.06669386476278305; Val loss 0.07053086906671524; Train ROC 0.9665875975015962; Val ROC 0.9639119508858988\n",
            "Loss function = BPR\n",
            "Epoch 108; Train loss 0.06630375236272812; Val loss 0.07010582834482193; Train ROC 0.966861385846165; Val ROC 0.9642459522219041\n",
            "Loss function = BPR\n",
            "Epoch 109; Train loss 0.06591729819774628; Val loss 0.06968504935503006; Train ROC 0.967086323988176; Val ROC 0.9644845246047651\n",
            "Loss function = BPR\n",
            "Epoch 110; Train loss 0.06553460657596588; Val loss 0.06926866620779037; Train ROC 0.9673419355131884; Val ROC 0.9648185259407704\n",
            "Loss function = BPR\n",
            "Epoch 111; Train loss 0.06515580415725708; Val loss 0.06885675340890884; Train ROC 0.9675850504747558; Val ROC 0.9651286700384897\n",
            "Loss function = BPR\n",
            "Epoch 112; Train loss 0.06478097289800644; Val loss 0.06844943016767502; Train ROC 0.9678145328216559; Val ROC 0.9653990520723987\n",
            "Loss function = BPR\n",
            "Epoch 113; Train loss 0.0644102543592453; Val loss 0.06804675608873367; Train ROC 0.968040607014889; Val ROC 0.9656932913445939\n",
            "Loss function = BPR\n",
            "Epoch 114; Train loss 0.06404368579387665; Val loss 0.06764881312847137; Train ROC 0.9682803138227897; Val ROC 0.9660352450933614\n",
            "Loss function = BPR\n",
            "Epoch 115; Train loss 0.06368135660886765; Val loss 0.0672556459903717; Train ROC 0.9685143403745788; Val ROC 0.9663135795400325\n",
            "Loss function = BPR\n",
            "Epoch 116; Train loss 0.06332332640886307; Val loss 0.06686730682849884; Train ROC 0.9687517750800348; Val ROC 0.9665362470973693\n",
            "Loss function = BPR\n",
            "Epoch 117; Train loss 0.0629696473479271; Val loss 0.06648383289575577; Train ROC 0.9689676248122675; Val ROC 0.9668304863695645\n",
            "Loss function = BPR\n",
            "Epoch 118; Train loss 0.06262034177780151; Val loss 0.06610523909330368; Train ROC 0.9691857466469448; Val ROC 0.9670611063396635\n",
            "Loss function = BPR\n",
            "Epoch 119; Train loss 0.0622754767537117; Val loss 0.06573152542114258; Train ROC 0.9694095487377334; Val ROC 0.9673155835480485\n",
            "Loss function = BPR\n",
            "Epoch 120; Train loss 0.061935026198625565; Val loss 0.06536274403333664; Train ROC 0.9696560718529678; Val ROC 0.9676177752330057\n",
            "Loss function = BPR\n",
            "Epoch 121; Train loss 0.061599038541316986; Val loss 0.06499886512756348; Train ROC 0.9698707855339782; Val ROC 0.9678483952031046\n",
            "Loss function = BPR\n",
            "Epoch 122; Train loss 0.06126750633120537; Val loss 0.0646398663520813; Train ROC 0.9700843631637663; Val ROC 0.9680949199987277\n",
            "Loss function = BPR\n",
            "Epoch 123; Train loss 0.06094040349125862; Val loss 0.0642857477068901; Train ROC 0.9703149815618887; Val ROC 0.9683653020326367\n",
            "Loss function = BPR\n",
            "Epoch 124; Train loss 0.06061774864792824; Val loss 0.06393646448850632; Train ROC 0.9705183347306764; Val ROC 0.9686515888920698\n",
            "Loss function = BPR\n",
            "Epoch 125; Train loss 0.06029949337244034; Val loss 0.06359200179576874; Train ROC 0.9707103273872413; Val ROC 0.9688344943855965\n",
            "Loss function = BPR\n",
            "Epoch 126; Train loss 0.05998563393950462; Val loss 0.06325230747461319; Train ROC 0.970905728197473; Val ROC 0.9690253522918854\n",
            "Loss function = BPR\n",
            "Epoch 127; Train loss 0.059676118195056915; Val loss 0.0629173293709755; Train ROC 0.9711124895199275; Val ROC 0.9692957343257944\n",
            "Loss function = BPR\n",
            "Epoch 128; Train loss 0.05937090888619423; Val loss 0.06258705258369446; Train ROC 0.9713056182277147; Val ROC 0.9696058784235136\n",
            "Loss function = BPR\n",
            "Epoch 129; Train loss 0.05906999111175537; Val loss 0.062261395156383514; Train ROC 0.9715328284721702; Val ROC 0.9698842128701848\n",
            "Loss function = BPR\n",
            "Epoch 130; Train loss 0.05877329036593437; Val loss 0.061940304934978485; Train ROC 0.9717293653336242; Val ROC 0.9701068804275217\n",
            "Loss function = BPR\n",
            "Epoch 131; Train loss 0.05848075449466705; Val loss 0.061623718589544296; Train ROC 0.9719406708609678; Val ROC 0.9703454528103826\n",
            "Loss function = BPR\n",
            "Epoch 132; Train loss 0.05819234251976013; Val loss 0.06131157651543617; Train ROC 0.9721497042858669; Val ROC 0.9705522155421955\n",
            "Loss function = BPR\n",
            "Epoch 133; Train loss 0.057907987385988235; Val loss 0.06100381910800934; Train ROC 0.9723530574546545; Val ROC 0.9708385024016286\n",
            "Loss function = BPR\n",
            "Epoch 134; Train loss 0.05762763321399689; Val loss 0.060700368136167526; Train ROC 0.9725620908795536; Val ROC 0.9710293603079174\n",
            "Loss function = BPR\n",
            "Epoch 135; Train loss 0.057351209223270416; Val loss 0.060401156544685364; Train ROC 0.9727495393312293; Val ROC 0.9711963609759201\n",
            "Loss function = BPR\n",
            "Epoch 136; Train loss 0.05707867443561554; Val loss 0.06010612100362778; Train ROC 0.9729381238341275; Val ROC 0.9714587905970672\n",
            "Loss function = BPR\n",
            "Epoch 137; Train loss 0.05680994316935539; Val loss 0.0598151795566082; Train ROC 0.9731255722858032; Val ROC 0.9717132678054522\n",
            "Loss function = BPR\n",
            "Epoch 138; Train loss 0.0565449595451355; Val loss 0.05952826514840126; Train ROC 0.973313020737479; Val ROC 0.9719438877755511\n",
            "Loss function = BPR\n",
            "Epoch 139; Train loss 0.05628364905714989; Val loss 0.05924530327320099; Train ROC 0.9735209181111558; Val ROC 0.9720711263797436\n",
            "Loss function = BPR\n",
            "Epoch 140; Train loss 0.056025948375463486; Val loss 0.05896623060107231; Train ROC 0.9737083665628316; Val ROC 0.9722937939370805\n",
            "Loss function = BPR\n",
            "Epoch 141; Train loss 0.05577179789543152; Val loss 0.058690961450338364; Train ROC 0.973910583680397; Val ROC 0.9724926042561313\n",
            "Loss function = BPR\n",
            "Epoch 142; Train loss 0.055521126836538315; Val loss 0.05841943621635437; Train ROC 0.9741082565930733; Val ROC 0.972675509749658\n",
            "Loss function = BPR\n",
            "Epoch 143; Train loss 0.055273864418268204; Val loss 0.058151572942733765; Train ROC 0.9743025213520827; Val ROC 0.9728186531793747\n",
            "Loss function = BPR\n",
            "Epoch 144; Train loss 0.05502994731068611; Val loss 0.05788731947541237; Train ROC 0.9744649766768684; Val ROC 0.9730333683239495\n",
            "Loss function = BPR\n",
            "Epoch 145; Train loss 0.05478931590914726; Val loss 0.05762659013271332; Train ROC 0.9746331122577655; Val ROC 0.9731447021026179\n",
            "Loss function = BPR\n",
            "Epoch 146; Train loss 0.054551903158426285; Val loss 0.057369329035282135; Train ROC 0.9748148804533299; Val ROC 0.9733594172471928\n",
            "Loss function = BPR\n",
            "Epoch 147; Train loss 0.0543176531791687; Val loss 0.05711545795202255; Train ROC 0.9749989207513389; Val ROC 0.9734707510258612\n",
            "Loss function = BPR\n",
            "Epoch 148; Train loss 0.05408649146556854; Val loss 0.05686492472887039; Train ROC 0.9751806889469032; Val ROC 0.9737093234087222\n",
            "Loss function = BPR\n",
            "Epoch 149; Train loss 0.05385836958885193; Val loss 0.056617651134729385; Train ROC 0.9753579129375786; Val ROC 0.9738842764894869\n",
            "Loss function = BPR\n",
            "Epoch 150; Train loss 0.05363321676850319; Val loss 0.05637358874082565; Train ROC 0.9755362729794761; Val ROC 0.9740671819830137\n",
            "Loss function = BPR\n",
            "Epoch 151; Train loss 0.053410980850458145; Val loss 0.05613267421722412; Train ROC 0.9757294016872633; Val ROC 0.9742262302382543\n",
            "Loss function = BPR\n",
            "Epoch 152; Train loss 0.053191617131233215; Val loss 0.055894844233989716; Train ROC 0.9758941291144935; Val ROC 0.9744250405573051\n",
            "Loss function = BPR\n",
            "Epoch 153; Train loss 0.052975062280893326; Val loss 0.05566004663705826; Train ROC 0.9760418157733896; Val ROC 0.9746715653529281\n",
            "Loss function = BPR\n",
            "Epoch 154; Train loss 0.0527612529695034; Val loss 0.055428214371204376; Train ROC 0.9761974547908416; Val ROC 0.9748385660209308\n",
            "Loss function = BPR\n",
            "Epoch 155; Train loss 0.05255015194416046; Val loss 0.05519929528236389; Train ROC 0.9763394611936262; Val ROC 0.9750373763399816\n",
            "Loss function = BPR\n",
            "Epoch 156; Train loss 0.05234169587492943; Val loss 0.05497324466705322; Train ROC 0.9764985083647452; Val ROC 0.9751884721824602\n",
            "Loss function = BPR\n",
            "Epoch 157; Train loss 0.05213584750890732; Val loss 0.05475000664591789; Train ROC 0.9766518752797526; Val ROC 0.9753793300887489\n",
            "Loss function = BPR\n",
            "Epoch 158; Train loss 0.05193255841732025; Val loss 0.05452952906489372; Train ROC 0.9768177387582051; Val ROC 0.9755145211057035\n",
            "Loss function = BPR\n",
            "Epoch 159; Train loss 0.05173176899552345; Val loss 0.05431176349520683; Train ROC 0.9769631533146567; Val ROC 0.9757371886630404\n",
            "Loss function = BPR\n",
            "Epoch 160; Train loss 0.051533449441194534; Val loss 0.054096657782793045; Train ROC 0.9771142481272196; Val ROC 0.975872379679995\n",
            "Loss function = BPR\n",
            "Epoch 161; Train loss 0.051337551325559616; Val loss 0.053884170949459076; Train ROC 0.9772892000154503; Val ROC 0.9761189044756179\n",
            "Loss function = BPR\n",
            "Epoch 162; Train loss 0.05114402249455452; Val loss 0.05367424711585045; Train ROC 0.9774277982645682; Val ROC 0.9762700003180965\n",
            "Loss function = BPR\n",
            "Epoch 163; Train loss 0.05095282942056656; Val loss 0.05346686765551567; Train ROC 0.9775709407185751; Val ROC 0.9764449533988612\n",
            "Loss function = BPR\n",
            "Epoch 164; Train loss 0.050763942301273346; Val loss 0.05326196551322937; Train ROC 0.9777015866091371; Val ROC 0.9766040016541019\n",
            "Loss function = BPR\n",
            "Epoch 165; Train loss 0.050577301532030106; Val loss 0.053059499710798264; Train ROC 0.9778606337802559; Val ROC 0.9767630499093425\n",
            "Loss function = BPR\n",
            "Epoch 166; Train loss 0.05039287731051445; Val loss 0.052859436720609665; Train ROC 0.9780037762342628; Val ROC 0.976882336100773\n",
            "Loss function = BPR\n",
            "Epoch 167; Train loss 0.050210632383823395; Val loss 0.05266173928976059; Train ROC 0.9781628234053817; Val ROC 0.9770413843560136\n",
            "Loss function = BPR\n",
            "Epoch 168; Train loss 0.050030533224344254; Val loss 0.05246635153889656; Train ROC 0.9783252787301674; Val ROC 0.9772163374367783\n",
            "Loss function = BPR\n",
            "Epoch 169; Train loss 0.049852535128593445; Val loss 0.05227326229214668; Train ROC 0.978483189850064; Val ROC 0.977383338104781\n",
            "Loss function = BPR\n",
            "Epoch 170; Train loss 0.04967661574482918; Val loss 0.05208240821957588; Train ROC 0.978638828867516; Val ROC 0.9775185291217355\n",
            "Loss function = BPR\n",
            "Epoch 171; Train loss 0.04950273409485817; Val loss 0.05189376696944237; Train ROC 0.9787887876288566; Val ROC 0.9776696249642142\n",
            "Loss function = BPR\n",
            "Epoch 172; Train loss 0.049330856651067734; Val loss 0.051707301288843155; Train ROC 0.9789069369559734; Val ROC 0.9777889111556446\n",
            "Loss function = BPR\n",
            "Epoch 173; Train loss 0.04916095733642578; Val loss 0.05152296647429466; Train ROC 0.9790529195380362; Val ROC 0.9779241021725992\n",
            "Loss function = BPR\n",
            "Epoch 174; Train loss 0.04899298772215843; Val loss 0.0513407364487648; Train ROC 0.9791989021200987; Val ROC 0.9780592931895538\n",
            "Loss function = BPR\n",
            "Epoch 175; Train loss 0.048826929181814194; Val loss 0.05116058886051178; Train ROC 0.9793204596008824; Val ROC 0.9781626745554601\n",
            "Loss function = BPR\n",
            "Epoch 176; Train loss 0.04866277053952217; Val loss 0.05098246783018112; Train ROC 0.9794579217987781; Val ROC 0.9783137703979388\n",
            "Loss function = BPR\n",
            "Epoch 177; Train loss 0.04850044846534729; Val loss 0.050806354731321335; Train ROC 0.9795635745624499; Val ROC 0.9784410090021313\n",
            "Loss function = BPR\n",
            "Epoch 178; Train loss 0.04833994433283806; Val loss 0.05063222721219063; Train ROC 0.9797146693750127; Val ROC 0.9785762000190857\n",
            "Loss function = BPR\n",
            "Epoch 179; Train loss 0.04818124324083328; Val loss 0.05046003684401512; Train ROC 0.9798544036753529; Val ROC 0.9787193434488024\n",
            "Loss function = BPR\n",
            "Epoch 180; Train loss 0.04802430421113968; Val loss 0.050289757549762726; Train ROC 0.9799986821805822; Val ROC 0.978862486878519\n",
            "Loss function = BPR\n",
            "Epoch 181; Train loss 0.04786909744143486; Val loss 0.050121355801820755; Train ROC 0.9801554572492565; Val ROC 0.9789658682444253\n",
            "Loss function = BPR\n",
            "Epoch 182; Train loss 0.047715600579977036; Val loss 0.04995480552315712; Train ROC 0.9802736065763733; Val ROC 0.9791010592613799\n",
            "Loss function = BPR\n",
            "Epoch 183; Train loss 0.04756378382444382; Val loss 0.049790091812610626; Train ROC 0.9804087966718243; Val ROC 0.9792442026910965\n",
            "Loss function = BPR\n",
            "Epoch 184; Train loss 0.04741362854838371; Val loss 0.04962716996669769; Train ROC 0.9805496670233868; Val ROC 0.9794112033590991\n",
            "Loss function = BPR\n",
            "Epoch 185; Train loss 0.04726510867476463; Val loss 0.04946601763367653; Train ROC 0.9806746326578373; Val ROC 0.9795941088526259\n",
            "Loss function = BPR\n",
            "Epoch 186; Train loss 0.04711819440126419; Val loss 0.04930660501122475; Train ROC 0.9807791493702868; Val ROC 0.9797213474568184\n",
            "Loss function = BPR\n",
            "Epoch 187; Train loss 0.0469728484749794; Val loss 0.04914890602231026; Train ROC 0.9809245639267383; Val ROC 0.9798406336482489\n",
            "Loss function = BPR\n",
            "Epoch 188; Train loss 0.046829067170619965; Val loss 0.04899289458990097; Train ROC 0.9810449853562997; Val ROC 0.9800076343162516\n",
            "Loss function = BPR\n",
            "Epoch 189; Train loss 0.04668682813644409; Val loss 0.04883855953812599; Train ROC 0.9811676788883057; Val ROC 0.9801507777459681\n",
            "Loss function = BPR\n",
            "Epoch 190; Train loss 0.0465460941195488; Val loss 0.04868584871292114; Train ROC 0.9812892363690894; Val ROC 0.9802939211756847\n",
            "Loss function = BPR\n",
            "Epoch 191; Train loss 0.04640684649348259; Val loss 0.048534754663705826; Train ROC 0.9814142020035399; Val ROC 0.9803973025415911\n",
            "Loss function = BPR\n",
            "Epoch 192; Train loss 0.046269066631793976; Val loss 0.04838525131344795; Train ROC 0.9815368955355459; Val ROC 0.9804847790819735\n",
            "Loss function = BPR\n",
            "Epoch 193; Train loss 0.046132732182741165; Val loss 0.048237308859825134; Train ROC 0.9816686774773301; Val ROC 0.9806438273372141\n",
            "Loss function = BPR\n",
            "Epoch 194; Train loss 0.04599781706929207; Val loss 0.04809090867638588; Train ROC 0.9817913710093361; Val ROC 0.9808028755924547\n",
            "Loss function = BPR\n",
            "Epoch 195; Train loss 0.0458642952144146; Val loss 0.0479460284113884; Train ROC 0.9818845272095629; Val ROC 0.9809460190221714\n",
            "Loss function = BPR\n",
            "Epoch 196; Train loss 0.04573215916752815; Val loss 0.047802649438381195; Train ROC 0.982016309151347; Val ROC 0.9810891624518879\n",
            "Loss function = BPR\n",
            "Epoch 197; Train loss 0.04560138285160065; Val loss 0.04766073822975159; Train ROC 0.9821412747857975; Val ROC 0.9812641155326526\n",
            "Loss function = BPR\n",
            "Epoch 198; Train loss 0.04547194391489029; Val loss 0.04752027615904808; Train ROC 0.9822673764714703; Val ROC 0.9814629258517034\n",
            "Loss function = BPR\n",
            "Epoch 199; Train loss 0.04534382373094559; Val loss 0.04738125205039978; Train ROC 0.9823696210814753; Val ROC 0.9815583548048478\n",
            "Loss function = BPR\n",
            "Epoch 200; Train loss 0.04521700367331505; Val loss 0.047243621200323105; Train ROC 0.9824798180500363; Val ROC 0.9817174030600885\n",
            "Loss function = BPR\n",
            "Epoch 201; Train loss 0.045091450214385986; Val loss 0.04710739850997925; Train ROC 0.982593423172264; Val ROC 0.9818366892515189\n",
            "Loss function = BPR\n",
            "Epoch 202; Train loss 0.044967178255319595; Val loss 0.04697253927588463; Train ROC 0.9827104364481586; Val ROC 0.9819639278557114\n",
            "Loss function = BPR\n",
            "Epoch 203; Train loss 0.04484414681792259; Val loss 0.04683902487158775; Train ROC 0.9828070008020522; Val ROC 0.98211502369819\n",
            "Loss function = BPR\n",
            "Epoch 204; Train loss 0.044722333550453186; Val loss 0.04670684412121773; Train ROC 0.9829058372583903; Val ROC 0.9822263574768585\n",
            "Loss function = BPR\n",
            "Epoch 205; Train loss 0.04460173472762108; Val loss 0.04657597467303276; Train ROC 0.9830239865855072; Val ROC 0.9822979291917168\n",
            "Loss function = BPR\n",
            "Epoch 206; Train loss 0.04448232054710388; Val loss 0.04644639790058136; Train ROC 0.9831239590930676; Val ROC 0.9824251677959093\n",
            "Loss function = BPR\n",
            "Epoch 207; Train loss 0.0443640872836113; Val loss 0.04631809517741203; Train ROC 0.9832284758055171; Val ROC 0.9825126443362916\n",
            "Loss function = BPR\n",
            "Epoch 208; Train loss 0.04424700513482094; Val loss 0.046191051602363586; Train ROC 0.9833159517496325; Val ROC 0.982608073289436\n",
            "Loss function = BPR\n",
            "Epoch 209; Train loss 0.04413106292486191; Val loss 0.04606524109840393; Train ROC 0.9834181963596375; Val ROC 0.9826875974170564\n",
            "Loss function = BPR\n",
            "Epoch 210; Train loss 0.04401624947786331; Val loss 0.04594065994024277; Train ROC 0.9835124886110865; Val ROC 0.9828227884340108\n",
            "Loss function = BPR\n",
            "Epoch 211; Train loss 0.04390254616737366; Val loss 0.04581727460026741; Train ROC 0.9836260937333142; Val ROC 0.9829659318637275\n",
            "Loss function = BPR\n",
            "Epoch 212; Train loss 0.043789930641651154; Val loss 0.045695073902606964; Train ROC 0.9837226580872078; Val ROC 0.98309317046792\n",
            "Loss function = BPR\n",
            "Epoch 213; Train loss 0.0436783991754055; Val loss 0.04557405784726143; Train ROC 0.9838271747996574; Val ROC 0.9832045042465885\n",
            "Loss function = BPR\n",
            "Epoch 214; Train loss 0.04356793686747551; Val loss 0.04545418545603752; Train ROC 0.9839021541803277; Val ROC 0.9833237904380189\n",
            "Loss function = BPR\n",
            "Epoch 215; Train loss 0.04345851019024849; Val loss 0.045335449278354645; Train ROC 0.984005534841555; Val ROC 0.9833953621528772\n",
            "Loss function = BPR\n",
            "Epoch 216; Train loss 0.04335012659430504; Val loss 0.0452178455889225; Train ROC 0.9840975549905594; Val ROC 0.9834589814549735\n",
            "Loss function = BPR\n",
            "Epoch 217; Train loss 0.043242763727903366; Val loss 0.0451013445854187; Train ROC 0.9841861669858971; Val ROC 0.9836259821229761\n",
            "Loss function = BPR\n",
            "Epoch 218; Train loss 0.04313639551401138; Val loss 0.04498594254255295; Train ROC 0.9842861394934574; Val ROC 0.9837214110761205\n",
            "Loss function = BPR\n",
            "Epoch 219; Train loss 0.04303103685379028; Val loss 0.04487162083387375; Train ROC 0.9843883841034624; Val ROC 0.9838725069185991\n",
            "Loss function = BPR\n",
            "Epoch 220; Train loss 0.042926643043756485; Val loss 0.04475835710763931; Train ROC 0.9844883566110229; Val ROC 0.9840076979355536\n",
            "Loss function = BPR\n",
            "Epoch 221; Train loss 0.04282322898507118; Val loss 0.04464614763855934; Train ROC 0.9845826488624719; Val ROC 0.9841428889525082\n",
            "Loss function = BPR\n",
            "Epoch 222; Train loss 0.04272076115012169; Val loss 0.044534970074892044; Train ROC 0.9846803492675877; Val ROC 0.9842939847949868\n",
            "Loss function = BPR\n",
            "Epoch 223; Train loss 0.042619235813617706; Val loss 0.044424816966056824; Train ROC 0.9847678252117031; Val ROC 0.9844530330502275\n",
            "Loss function = BPR\n",
            "Epoch 224; Train loss 0.04251864179968834; Val loss 0.04431567341089249; Train ROC 0.9848553011558185; Val ROC 0.9845484620033719\n",
            "Loss function = BPR\n",
            "Epoch 225; Train loss 0.0424189567565918; Val loss 0.04420752823352814; Train ROC 0.9849530015609343; Val ROC 0.9846359385437542\n",
            "Loss function = BPR\n",
            "Epoch 226; Train loss 0.04232018440961838; Val loss 0.04410036280751228; Train ROC 0.9850541101197171; Val ROC 0.9846757006075644\n",
            "Loss function = BPR\n",
            "Epoch 227; Train loss 0.04222230240702629; Val loss 0.04399416595697403; Train ROC 0.9851529465760552; Val ROC 0.9848108916245188\n",
            "Loss function = BPR\n",
            "Epoch 228; Train loss 0.04212530329823494; Val loss 0.04388893023133278; Train ROC 0.9852551911860602; Val ROC 0.9848824633393771\n",
            "Loss function = BPR\n",
            "Epoch 229; Train loss 0.04202916473150253; Val loss 0.043784644454717636; Train ROC 0.9853562997448428; Val ROC 0.9849699398797596\n",
            "Loss function = BPR\n",
            "Epoch 230; Train loss 0.04193389415740967; Val loss 0.04368128627538681; Train ROC 0.9854585443548479; Val ROC 0.9850733212456659\n",
            "Loss function = BPR\n",
            "Epoch 231; Train loss 0.041839469224214554; Val loss 0.043578848242759705; Train ROC 0.98554942845263; Val ROC 0.9852323695009065\n",
            "Loss function = BPR\n",
            "Epoch 232; Train loss 0.041745878756046295; Val loss 0.04347732290625572; Train ROC 0.9856528091138573; Val ROC 0.9853277984540509\n",
            "Loss function = BPR\n",
            "Epoch 233; Train loss 0.04165312275290489; Val loss 0.04337669163942337; Train ROC 0.9857391490067504; Val ROC 0.9854311798199574\n",
            "Loss function = BPR\n",
            "Epoch 234; Train loss 0.04156116768717766; Val loss 0.04327695071697235; Train ROC 0.9858368494118663; Val ROC 0.9855425135986258\n",
            "Loss function = BPR\n",
            "Epoch 235; Train loss 0.04147002846002579; Val loss 0.04317808523774147; Train ROC 0.9859515905853163; Val ROC 0.9856856570283424\n",
            "Loss function = BPR\n",
            "Epoch 236; Train loss 0.04137967899441719; Val loss 0.04308008402585983; Train ROC 0.9860549712465435; Val ROC 0.9857572287432007\n",
            "Loss function = BPR\n",
            "Epoch 237; Train loss 0.04129011929035187; Val loss 0.042982932180166245; Train ROC 0.9861197261662134; Val ROC 0.9858208480452969\n",
            "Loss function = BPR\n",
            "Epoch 238; Train loss 0.04120132699608803; Val loss 0.042886629700660706; Train ROC 0.9862117463152179; Val ROC 0.9859242294112034\n",
            "Loss function = BPR\n",
            "Epoch 239; Train loss 0.04111330956220627; Val loss 0.042791157960891724; Train ROC 0.9862844535934436; Val ROC 0.9860355631898718\n",
            "Loss function = BPR\n",
            "Epoch 240; Train loss 0.0410260446369648; Val loss 0.042696502059698105; Train ROC 0.9863821539985594; Val ROC 0.9861309921430161\n",
            "Loss function = BPR\n",
            "Epoch 241; Train loss 0.04093952104449272; Val loss 0.04260266572237015; Train ROC 0.9864855346597867; Val ROC 0.9862502783344467\n",
            "Loss function = BPR\n",
            "Epoch 242; Train loss 0.040853746235370636; Val loss 0.04250963032245636; Train ROC 0.9865786908600135; Val ROC 0.9864093265896873\n",
            "Loss function = BPR\n",
            "Epoch 243; Train loss 0.040768686681985855; Val loss 0.042417384684085846; Train ROC 0.9866661668041289; Val ROC 0.9864490886534975\n",
            "Loss function = BPR\n",
            "Epoch 244; Train loss 0.040684353560209274; Val loss 0.04232591763138771; Train ROC 0.9867752277214675; Val ROC 0.986568374844928\n",
            "Loss function = BPR\n",
            "Epoch 245; Train loss 0.04060073569417; Val loss 0.04223523288965225; Train ROC 0.9868717920753611; Val ROC 0.9866638037980723\n",
            "Loss function = BPR\n",
            "Epoch 246; Train loss 0.040517810732126236; Val loss 0.04214530438184738; Train ROC 0.9869604040706987; Val ROC 0.9867751375767407\n",
            "Loss function = BPR\n",
            "Epoch 247; Train loss 0.040435586124658585; Val loss 0.0420561358332634; Train ROC 0.9870615126294814; Val ROC 0.986830804466075\n",
            "Loss function = BPR\n",
            "Epoch 248; Train loss 0.04035404324531555; Val loss 0.04196770861744881; Train ROC 0.9871535327784859; Val ROC 0.9869580430702675\n",
            "Loss function = BPR\n",
            "Epoch 249; Train loss 0.04027317836880684; Val loss 0.04188001900911331; Train ROC 0.9872353284664899; Val ROC 0.9870296147851259\n",
            "Loss function = BPR\n",
            "Epoch 250; Train loss 0.04019298031926155; Val loss 0.04179304838180542; Train ROC 0.9873250765130498; Val ROC 0.9871568533893184\n",
            "Loss function = BPR\n",
            "Epoch 251; Train loss 0.04011344537138939; Val loss 0.04170680791139603; Train ROC 0.9873943756376088; Val ROC 0.9872284251041766\n",
            "Loss function = BPR\n",
            "Epoch 252; Train loss 0.04003455862402916; Val loss 0.04162127524614334; Train ROC 0.9874829876329464; Val ROC 0.9872681871679868\n",
            "Loss function = BPR\n",
            "Epoch 253; Train loss 0.03995632380247116; Val loss 0.041536446660757065; Train ROC 0.9875545588599499; Val ROC 0.987331806470083\n",
            "Loss function = BPR\n",
            "Epoch 254; Train loss 0.0398787260055542; Val loss 0.041452307254076004; Train ROC 0.9876374905991762; Val ROC 0.9874033781849413\n",
            "Loss function = BPR\n",
            "Epoch 255; Train loss 0.03980175405740738; Val loss 0.04136885330080986; Train ROC 0.9877136060310687; Val ROC 0.9875067595508478\n",
            "Loss function = BPR\n",
            "Epoch 256; Train loss 0.0397254079580307; Val loss 0.041286077350378036; Train ROC 0.9877942656678504; Val ROC 0.987570378852944\n",
            "Loss function = BPR\n",
            "Epoch 257; Train loss 0.03964966908097267; Val loss 0.041203971952199936; Train ROC 0.9878840137144104; Val ROC 0.9877214746954226\n",
            "Loss function = BPR\n",
            "Epoch 258; Train loss 0.03957454860210419; Val loss 0.041122522205114365; Train ROC 0.9879555849414139; Val ROC 0.987800998823043\n",
            "Loss function = BPR\n",
            "Epoch 259; Train loss 0.03950002044439316; Val loss 0.04104173183441162; Train ROC 0.9880214759123059; Val ROC 0.9878646181251391\n",
            "Loss function = BPR\n",
            "Epoch 260; Train loss 0.03942608833312988; Val loss 0.04096158593893051; Train ROC 0.9880850947807535; Val ROC 0.9879361898399974\n",
            "Loss function = BPR\n",
            "Epoch 261; Train loss 0.03935274854302406; Val loss 0.04088208079338074; Train ROC 0.9881668904687575; Val ROC 0.9880077615548557\n",
            "Loss function = BPR\n",
            "Epoch 262; Train loss 0.03927998244762421; Val loss 0.0408032089471817; Train ROC 0.9882327814396495; Val ROC 0.9880952380952381\n",
            "Loss function = BPR\n",
            "Epoch 263; Train loss 0.03920779377222061; Val loss 0.04072495922446251; Train ROC 0.9883202573837649; Val ROC 0.9881986194611445\n",
            "Loss function = BPR\n",
            "Epoch 264; Train loss 0.03913617506623268; Val loss 0.04064732789993286; Train ROC 0.9884043251742135; Val ROC 0.9882463339377167\n",
            "Loss function = BPR\n",
            "Epoch 265; Train loss 0.03906511887907982; Val loss 0.04057030379772186; Train ROC 0.9884781685036615; Val ROC 0.9883656201291472\n",
            "Loss function = BPR\n",
            "Epoch 266; Train loss 0.03899461030960083; Val loss 0.040493883192539215; Train ROC 0.9885474676282204; Val ROC 0.9884371918440055\n",
            "Loss function = BPR\n",
            "Epoch 267; Train loss 0.03892464563250542; Val loss 0.04041805490851402; Train ROC 0.9886247191113353; Val ROC 0.9884849063205776\n",
            "Loss function = BPR\n",
            "Epoch 268; Train loss 0.038855232298374176; Val loss 0.040342822670936584; Train ROC 0.9887110590042284; Val ROC 0.9885882876864841\n",
            "Loss function = BPR\n",
            "Epoch 269; Train loss 0.03878634423017502; Val loss 0.040268175303936005; Train ROC 0.988787174436121; Val ROC 0.9886360021630562\n",
            "Loss function = BPR\n",
            "Epoch 270; Train loss 0.03871799632906914; Val loss 0.04019409418106079; Train ROC 0.9888826027387922; Val ROC 0.9887155262906766\n",
            "Loss function = BPR\n",
            "Epoch 271; Train loss 0.03865016624331474; Val loss 0.04012059420347214; Train ROC 0.9889609902731294; Val ROC 0.988795050418297\n",
            "Loss function = BPR\n",
            "Epoch 272; Train loss 0.038582853972911835; Val loss 0.040047649294137955; Train ROC 0.9890359696537997; Val ROC 0.9888666221331552\n",
            "Loss function = BPR\n",
            "Epoch 273; Train loss 0.03851605951786041; Val loss 0.03997526690363884; Train ROC 0.9891086769320255; Val ROC 0.9889620510862995\n",
            "Loss function = BPR\n",
            "Epoch 274; Train loss 0.038449764251708984; Val loss 0.0399034321308136; Train ROC 0.9891757039541399; Val ROC 0.9890018131501097\n",
            "Loss function = BPR\n",
            "Epoch 275; Train loss 0.03838396444916725; Val loss 0.03983214125037193; Train ROC 0.9892654520006998; Val ROC 0.989065432452206\n",
            "Loss function = BPR\n",
            "Epoch 276; Train loss 0.03831867128610611; Val loss 0.03976139426231384; Train ROC 0.9893483837399261; Val ROC 0.9890892896904921\n",
            "Loss function = BPR\n",
            "Epoch 277; Train loss 0.038253866136074066; Val loss 0.039691172540187836; Train ROC 0.9894120026083736; Val ROC 0.9891608614053504\n",
            "Loss function = BPR\n",
            "Epoch 278; Train loss 0.038189541548490524; Val loss 0.039621490985155106; Train ROC 0.9894733493743766; Val ROC 0.9892324331202087\n",
            "Loss function = BPR\n",
            "Epoch 279; Train loss 0.03812570124864578; Val loss 0.039552319794893265; Train ROC 0.9895324240379351; Val ROC 0.9892881000095429\n",
            "Loss function = BPR\n",
            "Epoch 280; Train loss 0.03806233033537865; Val loss 0.03948366641998291; Train ROC 0.9896119476234945; Val ROC 0.9893676241371632\n",
            "Loss function = BPR\n",
            "Epoch 281; Train loss 0.03799942135810852; Val loss 0.03941553086042404; Train ROC 0.9896744304407197; Val ROC 0.9894710055030697\n",
            "Loss function = BPR\n",
            "Epoch 282; Train loss 0.0379369892179966; Val loss 0.03934789076447487; Train ROC 0.9897357772067227; Val ROC 0.9895664344562141\n",
            "Loss function = BPR\n",
            "Epoch 283; Train loss 0.03787500783801079; Val loss 0.03928075730800629; Train ROC 0.9898084844849484; Val ROC 0.9896221013455483\n",
            "Loss function = BPR\n",
            "Epoch 284; Train loss 0.03781348466873169; Val loss 0.0392141155898571; Train ROC 0.9898789196607297; Val ROC 0.9897095778859306\n",
            "Loss function = BPR\n",
            "Epoch 285; Train loss 0.03775240480899811; Val loss 0.039147958159446716; Train ROC 0.9899345861706212; Val ROC 0.989805006839075\n",
            "Loss function = BPR\n",
            "Epoch 286; Train loss 0.03769177198410034; Val loss 0.03908228874206543; Train ROC 0.9899891166292906; Val ROC 0.9898845309666953\n",
            "Loss function = BPR\n",
            "Epoch 287; Train loss 0.03763158619403839; Val loss 0.039017099887132645; Train ROC 0.9900606878562941; Val ROC 0.9899322454432675\n",
            "Loss function = BPR\n",
            "Epoch 288; Train loss 0.037571825087070465; Val loss 0.03895237669348717; Train ROC 0.9901277148784084; Val ROC 0.9900117695708878\n",
            "Loss function = BPR\n",
            "Epoch 289; Train loss 0.03751249983906746; Val loss 0.038888126611709595; Train ROC 0.9901879255931891; Val ROC 0.9900276743964118\n",
            "Loss function = BPR\n",
            "Epoch 290; Train loss 0.03745359182357788; Val loss 0.03882433846592903; Train ROC 0.9902470002567476; Val ROC 0.9901151509367943\n",
            "Loss function = BPR\n",
            "Epoch 291; Train loss 0.037395112216472626; Val loss 0.03876101225614548; Train ROC 0.9903128912276397; Val ROC 0.9901946750644145\n",
            "Loss function = BPR\n",
            "Epoch 292; Train loss 0.0373370461165905; Val loss 0.03869813308119774; Train ROC 0.9903617414301976; Val ROC 0.9902662467792729\n",
            "Loss function = BPR\n",
            "Epoch 293; Train loss 0.037279389798641205; Val loss 0.03863570839166641; Train ROC 0.9904162718888669; Val ROC 0.9903298660813691\n",
            "Loss function = BPR\n",
            "Epoch 294; Train loss 0.03722214326262474; Val loss 0.038573719561100006; Train ROC 0.9904617139377581; Val ROC 0.9904252950345135\n",
            "Loss function = BPR\n",
            "Epoch 295; Train loss 0.03716530278325081; Val loss 0.038512181490659714; Train ROC 0.9905173804476496; Val ROC 0.9904650570983237\n",
            "Loss function = BPR\n",
            "Epoch 296; Train loss 0.03710886090993881; Val loss 0.038451071828603745; Train ROC 0.9905741830087635; Val ROC 0.9905843432897541\n",
            "Loss function = BPR\n",
            "Epoch 297; Train loss 0.037052810192108154; Val loss 0.0383903943002224; Train ROC 0.9906298495186551; Val ROC 0.9906718198301364\n",
            "Loss function = BPR\n",
            "Epoch 298; Train loss 0.03699715808033943; Val loss 0.038330141454935074; Train ROC 0.9906877881309912; Val ROC 0.9907354391322327\n",
            "Loss function = BPR\n",
            "Epoch 299; Train loss 0.03694188967347145; Val loss 0.038270313292741776; Train ROC 0.9907411825384383; Val ROC 0.9908308680853771\n",
            "Loss function = BPR\n",
            "Epoch 300; Train loss 0.03688700497150421; Val loss 0.038210898637771606; Train ROC 0.9907991211507744; Val ROC 0.9908706301491873\n"
          ]
        }
      ],
      "source": [
        "## For example:\n",
        "\n",
        "# using BPR loss\n",
        "loss_fn = \"BPR\"\n",
        "\n",
        "# using random sampling\n",
        "neg_samp = \"random\"\n",
        "\n",
        "# Num epochs, layers, etc.\n",
        "training_args['epochs'] = 301\n",
        "training_args['num_layers'] = 4\n",
        "#model, optimizer = init_model(\"LGC\", training_args)\n",
        "lgc_stats_random = train(datasets, model, optimizer, loss_fn, training_args, neg_samp = \"random\")\n",
        "torch.save(model.state_dict(), f\"model_stats/{model.name}_{loss_fn}_{neg_samp}.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rko0jeRq6k7z"
      },
      "source": [
        "### Tuning\n",
        "\n",
        "Use the simplified pipeline to train a couple of combinations of the model - changing the following variables (Do atleast 3 of each)\n",
        "\n",
        "1. Negative Sampling Method (You should have 3 of them)\n",
        "2. Number of Layers (<10 always)\n",
        "3. Number of Epochs (vary by 100 atleast)\n",
        "\n",
        "Be sure to save the results into different files. We'll use them in the next section to visualise better"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auAD4Nr_6k7z"
      },
      "outputs": [],
      "source": [
        "# If you had to stop for whatever reason, you can always reload the stats here! (just uncomment and change to correct paths)\n",
        "# lgc_stats = pickle.load(open(f\"{MODEL_STATS_DIR}/LGCN_LGC_4_e64_nodes34810__BPR_random.pkl\", \"rb\"))\n",
        "# lgc_stats_hard = pickle.load(open(f\"{MODEL_STATS_DIR}/LGCN_LGC_4_e64_nodes34810__BPR_hard.pkl\", \"rb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0LyueNb6k7z"
      },
      "source": [
        "## Section 3 - Training Visualisation\n",
        "Phew. The hard part is done. But let's visualise this data better.\n",
        "\n",
        "Write up code to plot the training and validation losses over epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0OOamTV6k7z"
      },
      "outputs": [],
      "source": [
        "def plot_train_val_loss(stats_dict):\n",
        "    train_loss = []\n",
        "    val_loss = []\n",
        "    for loss in stats_dict['train']['loss']:\n",
        "      train_loss.append(loss.detach().cpu())\n",
        "    train_loss = np.array(train_loss)\n",
        "\n",
        "    for loss in stats_dict['val']['loss']:\n",
        "      val_loss.append(loss.detach().cpu())\n",
        "    val_loss = np.array(val_loss)\n",
        "\n",
        "    num_loss = len(train_loss)\n",
        "    num_epoch = range(1, num_loss + 1)\n",
        "    plt.plot(num_epoch, train_loss, label='Train')\n",
        "    plt.plot(num_epoch, val_loss, label='Val')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPeBwuIm6k7z"
      },
      "source": [
        "Now pass each of the different stats dict and plot the graphs - label them accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q85xenKU6k7z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "outputId": "3a7efd1c-5496-4b50-b6e2-a26b248455f6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plot_train_val_loss(lgc_stats_random)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFnyiOqK6k70"
      },
      "source": [
        "# Section 4 - (Bonus) Embedding Visualisation\n",
        "\n",
        "- This section is for bonus points. We'll use a manifold projection technique to visualise the embeddings. (You could also use other methods like PCA for example)\n",
        "\n",
        "- The way this works is that we use dimensionality reduction to capture the most useful things the model learns in just 3 dimensions.\n",
        "\n",
        "- UMAP is a library that will help you with this. You will pass in your high-dimensional embeddings to a function you can find from[UMAP Docs](https://umap-learn.readthedocs.io/en/latest/) and it will return 3-dimensional embeddings.\n",
        "\n",
        "- We'll plot the embedding states of your model as it trains, every 10 epochs. This was the use of saving the states every 10 epochs in your training and testing code. This utilises the animation submodule of matplotlib, `matplotlib.animation`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W34z-NNX6k70"
      },
      "outputs": [],
      "source": [
        "# Imports specifc to visualizations\n",
        "import matplotlib.animation as animation\n",
        "import umap\n",
        "from IPython import display\n",
        "from math import floor, ceil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRuDdtI56k70"
      },
      "source": [
        "### Dimensionality Reduction using UMAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDwW-wdK6k70"
      },
      "outputs": [],
      "source": [
        "# Reduce 64-dim embeddings to 3-dim using umap\n",
        "umap_embs = [] # dimensionally reduced embeddings\n",
        "epoch_str = [] # what epoch produced the embeddings\n",
        "\n",
        "# Directory contianing the selected model embeddings\n",
        "model_dir = \"model_stats/LGCN_LGC_4_e64_nodes15720__BPR_random.pkl\"\n",
        "\n",
        "for filename in tqdm(sorted(os.listdir(join(os.getcwd(), model_dir)),\n",
        "                       key=lambda name: int(name[:-3].split('_')[-1]))):\n",
        "  embs = torch.load(join(os.getcwd(), model_dir, filename), map_location=torch.device('cpu')).detach().cpu().numpy()\n",
        "  embs = embs[:n_playlists]\n",
        "  u = umap.UMAP(n_components=3).fit_transform(embs) # Do Something\n",
        "  umap_embs.append(u)\n",
        "  epoch_str.append(filename[:-3].split('_')[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SJOA-Dy6k70"
      },
      "source": [
        "### Prep to Plot\n",
        "Now we need to pass these to our plotting library, which needs the dimensions separately.\n",
        "\n",
        "`extract_dim(dim)` will return the values for a single dimension `dim`\n",
        "\n",
        "`compute_min_max(dim,h)` should return the limits of the values on a dimension. Automatic detection by matplotlib may lead to poorly scaled graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUNp4Gjs6k70"
      },
      "outputs": [],
      "source": [
        "# Separate the embeddings dimensions for plotting\n",
        "n = len(umap_embs)\n",
        "n_emb = len(umap_embs[0])\n",
        "\n",
        "def extract_dim(dim):\n",
        "  \"\"\"Given a dim index, return just that dim of umap_embs\"\"\"\n",
        "  return [None]\n",
        "\n",
        "x = extract_dim(0)\n",
        "y = extract_dim(1)\n",
        "z = extract_dim(2)\n",
        "\n",
        "def compute_min_max(dim):\n",
        "  \"\"\"Given a dim index, compute dim limits\"\"\"\n",
        "  return None,None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "040OypJ_6k70"
      },
      "source": [
        "### Time to plot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEY9MvOR6k71"
      },
      "source": [
        "Complete this function that will plot and update the visualisation of the embeddings over time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWoIH15v6k71"
      },
      "outputs": [],
      "source": [
        "def plot_embeddings(select_idx=None, color=None):\n",
        "  \"\"\"\n",
        "  Plot the playlist embeddings.\n",
        "  Optional parameters: select_idx, color are the selected\n",
        "  indices to plot and the associated color mappings\n",
        "  \"\"\"\n",
        "  # Indecies of embeddings - There are other ways to do it\n",
        "  idx = [None]\n",
        "\n",
        "  # Set up plot\n",
        "  fig = plt.figure(figsize = (10, 10))\n",
        "  subplt = plt.axes(projection =\"3d\")\n",
        "\n",
        "  # Initialize the scatter plot\n",
        "  my_sct = subplt.scatter(x[idx[0]], y[idx[0]], z[idx[0]],\n",
        "                          alpha = 0.8, marker ='o')\n",
        "  # Set axis limit\n",
        "  subplt.set_xlim(compute_min_max(x))\n",
        "  subplt.set_xlabel('x')\n",
        "  subplt.set_ylim(compute_min_max(y))\n",
        "  subplt.set_xlabel('y')\n",
        "  subplt.set_zlim(compute_min_max(z))\n",
        "  subplt.set_xlabel('z')\n",
        "\n",
        "  # Define animation update funcion\n",
        "  def update(itr, xa, ya, za, t):\n",
        "    i = idx[itr]\n",
        "    subplt.set_title('Embedding Epoch: ' + epoch_str[i])\n",
        "    xyz = None\n",
        "    my_sct._offsets3d = xyz\n",
        "    return my_sct\n",
        "\n",
        "  # Create animation\n",
        "  ani = animation.FuncAnimation(fig, update,\n",
        "                                fargs=(x, y, z),\n",
        "                                interval=500,\n",
        "                                save_count=len(idx))\n",
        "  video = ani.to_html5_video()\n",
        "  html = display.HTML(video)\n",
        "  display.display(html)\n",
        "  plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYSPWalR6k71"
      },
      "source": [
        "# End\n",
        "\n",
        "And that's it! You could go one step further by poking around this data and plotting artists in different colours, to see if the embeddings are separating out artists whose styles don't match. We aren't grading you for that however.\n",
        "\n",
        "Hopefully you had some fun with this exercise. You could try running your model on newer Spotify data and seeing what happens. Maybe if you built an inference function you could also test it on your own playlists.\n",
        "\n",
        "Traditional ML architechture struggles to handle these kinds of tasks, as the data is related in a graphical structure and the data itself is a complex sequence which would be hard to analyse to generate embeddings for this use case. The upcoming topics will see us build even more on this idea, scaling your models for far larger datasets, poking around the internals of a GNN Model and the power of message passing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVTBGTWu6k73"
      },
      "source": [
        "***\n",
        "***"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}